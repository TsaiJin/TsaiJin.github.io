{"meta":{"version":1,"warehouse":"3.0.2"},"models":{"Asset":[{"_id":"themes/landscape/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","path":"fancybox/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/js/script.js","path":"js/script.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","path":"css/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","path":"css/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","path":"css/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","path":"fancybox/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","path":"fancybox/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","path":"fancybox/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","path":"fancybox/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","path":"fancybox/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","path":"fancybox/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","path":"css/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","path":"css/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/images/banner.jpg","path":"css/images/banner.jpg","modified":0,"renderable":1},{"_id":"themes/Minos/source/css/insight.scss","path":"css/insight.scss","modified":0,"renderable":1},{"_id":"themes/Minos/source/images/check.svg","path":"images/check.svg","modified":0,"renderable":1},{"_id":"themes/Minos/source/css/style.scss","path":"css/style.scss","modified":0,"renderable":1},{"_id":"themes/Minos/source/images/exclamation.svg","path":"images/exclamation.svg","modified":0,"renderable":1},{"_id":"themes/Minos/source/images/info.svg","path":"images/info.svg","modified":0,"renderable":1},{"_id":"themes/Minos/source/images/question.svg","path":"images/question.svg","modified":0,"renderable":1},{"_id":"themes/Minos/source/images/logo.png","path":"images/logo.png","modified":0,"renderable":1},{"_id":"themes/Minos/source/images/quote-left.svg","path":"images/quote-left.svg","modified":0,"renderable":1},{"_id":"themes/Minos/source/js/insight.js","path":"js/insight.js","modified":0,"renderable":1},{"_id":"themes/Minos/source/js/script.js","path":"js/script.js","modified":0,"renderable":1}],"Cache":[{"_id":"themes/landscape/.gitignore","hash":"58d26d4b5f2f94c2d02a4e4a448088e4a2527c77","modified":1589626151237},{"_id":"themes/landscape/Gruntfile.js","hash":"71adaeaac1f3cc56e36c49d549b8d8a72235c9b9","modified":1589626151237},{"_id":"themes/landscape/LICENSE","hash":"c480fce396b23997ee23cc535518ffaaf7f458f8","modified":1589626151237},{"_id":"themes/landscape/README.md","hash":"37fae88639ef60d63bd0de22314d7cc4c5d94b07","modified":1589626151237},{"_id":"themes/landscape/_config.yml","hash":"79ac6b9ed6a4de5a21ea53fc3f5a3de92e2475ff","modified":1589626151237},{"_id":"themes/landscape/package.json","hash":"544f21a0b2c7034998b36ae94dba6e3e0f39f228","modified":1589626151244},{"_id":"source/_posts/hello-world.md","hash":"7d98d6592de80fdcd2949bd7401cec12afd98cdf","modified":1589625131667},{"_id":"themes/landscape/languages/de.yml","hash":"3ebf0775abbee928c8d7bda943c191d166ded0d3","modified":1589626151237},{"_id":"themes/landscape/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1589626151238},{"_id":"themes/landscape/languages/es.yml","hash":"76edb1171b86532ef12cfd15f5f2c1ac3949f061","modified":1589626151238},{"_id":"themes/landscape/languages/ja.yml","hash":"a73e1b9c80fd6e930e2628b393bfe3fb716a21a9","modified":1589626151238},{"_id":"themes/landscape/languages/fr.yml","hash":"415e1c580ced8e4ce20b3b0aeedc3610341c76fb","modified":1589626151238},{"_id":"themes/landscape/languages/ko.yml","hash":"881d6a0a101706e0452af81c580218e0bfddd9cf","modified":1589626151238},{"_id":"themes/landscape/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1589626151238},{"_id":"themes/landscape/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1589626151238},{"_id":"themes/landscape/languages/pt.yml","hash":"57d07b75d434fbfc33b0ddb543021cb5f53318a8","modified":1589626151238},{"_id":"themes/landscape/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1589626151239},{"_id":"themes/landscape/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1589626151239},{"_id":"themes/landscape/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1589626151239},{"_id":"themes/landscape/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1589626151243},{"_id":"themes/landscape/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1589626151243},{"_id":"themes/landscape/layout/index.ejs","hash":"aa1b4456907bdb43e629be3931547e2d29ac58c8","modified":1589626151243},{"_id":"themes/landscape/layout/layout.ejs","hash":"f155824ca6130080bb057fa3e868a743c69c4cf5","modified":1589626151243},{"_id":"themes/landscape/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1589626151243},{"_id":"themes/landscape/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1589626151243},{"_id":"themes/landscape/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1589626151243},{"_id":"themes/landscape/scripts/fancybox.js","hash":"aa411cd072399df1ddc8e2181a3204678a5177d9","modified":1589626151244},{"_id":"themes/landscape/layout/_partial/after-footer.ejs","hash":"d0d753d39038284d52b10e5075979cc97db9cd20","modified":1589626151239},{"_id":"themes/landscape/layout/_partial/archive-post.ejs","hash":"c7a71425a946d05414c069ec91811b5c09a92c47","modified":1589626151239},{"_id":"themes/landscape/layout/_partial/archive.ejs","hash":"950ddd91db8718153b329b96dc14439ab8463ba5","modified":1589626151239},{"_id":"themes/landscape/layout/_partial/article.ejs","hash":"c4c835615d96a950d51fa2c3b5d64d0596534fed","modified":1589626151239},{"_id":"themes/landscape/layout/_partial/footer.ejs","hash":"93518893cf91287e797ebac543c560e2a63b8d0e","modified":1589626151240},{"_id":"themes/landscape/layout/_partial/gauges-analytics.ejs","hash":"aad6312ac197d6c5aaf2104ac863d7eba46b772a","modified":1589626151240},{"_id":"themes/landscape/layout/_partial/google-analytics.ejs","hash":"f921e7f9223d7c95165e0f835f353b2938e40c45","modified":1589626151240},{"_id":"themes/landscape/layout/_partial/head.ejs","hash":"5abf77aec957d9445fc71a8310252f0013c84578","modified":1589626151240},{"_id":"themes/landscape/layout/_partial/header.ejs","hash":"7e749050be126eadbc42decfbea75124ae430413","modified":1589626151240},{"_id":"themes/landscape/layout/_partial/mobile-nav.ejs","hash":"e952a532dfc583930a666b9d4479c32d4a84b44e","modified":1589626151240},{"_id":"themes/landscape/layout/_partial/sidebar.ejs","hash":"930da35cc2d447a92e5ee8f835735e6fd2232469","modified":1589626151241},{"_id":"themes/landscape/layout/_widget/archive.ejs","hash":"beb4a86fcc82a9bdda9289b59db5a1988918bec3","modified":1589626151242},{"_id":"themes/landscape/layout/_widget/category.ejs","hash":"dd1e5af3c6af3f5d6c85dfd5ca1766faed6a0b05","modified":1589626151242},{"_id":"themes/landscape/layout/_widget/recent_posts.ejs","hash":"0d4f064733f8b9e45c0ce131fe4a689d570c883a","modified":1589626151242},{"_id":"themes/landscape/layout/_widget/tag.ejs","hash":"2de380865df9ab5f577f7d3bcadf44261eb5faae","modified":1589626151242},{"_id":"themes/landscape/layout/_widget/tagcloud.ejs","hash":"b4a2079101643f63993dcdb32925c9b071763b46","modified":1589626151243},{"_id":"themes/landscape/source/css/_extend.styl","hash":"222fbe6d222531d61c1ef0f868c90f747b1c2ced","modified":1589626151244},{"_id":"themes/landscape/source/css/_variables.styl","hash":"628e307579ea46b5928424313993f17b8d729e92","modified":1589626151246},{"_id":"themes/landscape/source/css/style.styl","hash":"a70d9c44dac348d742702f6ba87e5bb3084d65db","modified":1589626151253},{"_id":"themes/landscape/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1589626151253},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1589626151253},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1589626151253},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1589626151253},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1589626151253},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1589626151253},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1589626151255},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1589626151255},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1589626151255},{"_id":"themes/landscape/source/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1589626151255},{"_id":"themes/landscape/layout/_partial/post/category.ejs","hash":"c6bcd0e04271ffca81da25bcff5adf3d46f02fc0","modified":1589626151241},{"_id":"themes/landscape/layout/_partial/post/date.ejs","hash":"6197802873157656e3077c5099a7dda3d3b01c29","modified":1589626151241},{"_id":"themes/landscape/layout/_partial/post/gallery.ejs","hash":"3d9d81a3c693ff2378ef06ddb6810254e509de5b","modified":1589626151241},{"_id":"themes/landscape/layout/_partial/post/nav.ejs","hash":"16a904de7bceccbb36b4267565f2215704db2880","modified":1589626151241},{"_id":"themes/landscape/layout/_partial/post/title.ejs","hash":"2f275739b6f1193c123646a5a31f37d48644c667","modified":1589626151241},{"_id":"themes/landscape/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1589626151241},{"_id":"themes/landscape/source/css/_partial/archive.styl","hash":"db15f5677dc68f1730e82190bab69c24611ca292","modified":1589626151244},{"_id":"themes/landscape/source/css/_partial/article.styl","hash":"10685f8787a79f79c9a26c2f943253450c498e3e","modified":1589626151244},{"_id":"themes/landscape/source/css/_partial/comment.styl","hash":"79d280d8d203abb3bd933ca9b8e38c78ec684987","modified":1589626151245},{"_id":"themes/landscape/source/css/_partial/footer.styl","hash":"e35a060b8512031048919709a8e7b1ec0e40bc1b","modified":1589626151245},{"_id":"themes/landscape/source/css/_partial/header.styl","hash":"85ab11e082f4dd86dde72bed653d57ec5381f30c","modified":1589626151245},{"_id":"themes/landscape/source/css/_partial/highlight.styl","hash":"bf4e7be1968dad495b04e83c95eac14c4d0ad7c0","modified":1589626151245},{"_id":"themes/landscape/source/css/_partial/mobile.styl","hash":"a399cf9e1e1cec3e4269066e2948d7ae5854d745","modified":1589626151245},{"_id":"themes/landscape/source/css/_partial/sidebar-aside.styl","hash":"890349df5145abf46ce7712010c89237900b3713","modified":1589626151245},{"_id":"themes/landscape/source/css/_partial/sidebar-bottom.styl","hash":"8fd4f30d319542babfd31f087ddbac550f000a8a","modified":1589626151245},{"_id":"themes/landscape/source/css/_partial/sidebar.styl","hash":"404ec059dc674a48b9ab89cd83f258dec4dcb24d","modified":1589626151246},{"_id":"themes/landscape/source/css/_util/grid.styl","hash":"0bf55ee5d09f193e249083602ac5fcdb1e571aed","modified":1589626151246},{"_id":"themes/landscape/source/css/_util/mixin.styl","hash":"44f32767d9fd3c1c08a60d91f181ee53c8f0dbb3","modified":1589626151246},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1589626151247},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1589626151248},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1589626151250},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1589626151254},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1589626151254},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1589626151254},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1589626151254},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1589626151254},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1589626151254},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1589626151250},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1589626151249},{"_id":"themes/landscape/source/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1589626151252},{"_id":"source/_posts/ls.md","hash":"6dcc1506b9e6e9667f9ceedd46556bae389ba5da","modified":1589626496481},{"_id":"source/_posts/hello.md","hash":"f1c014f3abb2e3203ab4338e8811d1457bd83628","modified":1589631715684},{"_id":"themes/Minos/LICENSE","hash":"ca01a2d52b59346e82f079c593df6cb26dd9a7a5","modified":1589633501129},{"_id":"themes/Minos/.gitignore","hash":"8b02e7219e2dd9b50d198819fd7d8f74ebc9db2a","modified":1589633501129},{"_id":"themes/Minos/README.md","hash":"ba6b4e134d718704cfd030e106bf24d6ef8b496d","modified":1589633501129},{"_id":"themes/Minos/_config.yml.example","hash":"bb4c9460057233749b201382baa799cdbd477d36","modified":1589633501129},{"_id":"themes/Minos/package.json","hash":"f9d450db80149dea6c372990cdf51dfde901e5cc","modified":1589633501137},{"_id":"themes/Minos/package-lock.json","hash":"e1fbecec56fb65379bf651f21fb485376e692b38","modified":1589633501137},{"_id":"themes/Minos/languages/de.yml","hash":"3d6abea8c2990b04fcd54ffd88ab15356b5171e8","modified":1589633501129},{"_id":"themes/Minos/languages/es.yml","hash":"5c35950221411e34e7a9821d0b0671da9a458d8c","modified":1589633501130},{"_id":"themes/Minos/languages/en.yml","hash":"ef98c8674fed78f2350598ee8b15fcd53fbd2ae5","modified":1589633501129},{"_id":"themes/Minos/languages/fr.yml","hash":"cca90260b00842bf73fadb3154c2969102cf80c0","modified":1589633501130},{"_id":"themes/Minos/languages/ko.yml","hash":"1acf3f959f1d2b4f7a77e7e82851821aa8635362","modified":1589633501130},{"_id":"themes/Minos/languages/pl.yml","hash":"0ff13d2d14d1f63341cbe576070f724747238dee","modified":1589633501130},{"_id":"themes/Minos/languages/ru.yml","hash":"8e5a58176bf943432ba6e4f1981d9b98fdea36a4","modified":1589633501130},{"_id":"themes/Minos/lib/i18n.js","hash":"4c90aa27420e0e2ff74f80e976fe146e96354f61","modified":1589633501136},{"_id":"themes/Minos/languages/zh-cn.yml","hash":"9c5a489b11a056d1ea7b9d4a0e127aef9e192ee4","modified":1589633501130},{"_id":"themes/Minos/lib/rfc5646.js","hash":"8ecf38d0ec7145720ea8e888da314131712770e8","modified":1589633501137},{"_id":"themes/Minos/languages/zh-tw.yml","hash":"f61b67b9c454d16bc3973f6cb142b98a4e19f9b0","modified":1589633501130},{"_id":"themes/Minos/layout/categories.ejs","hash":"fff6f911d0f548ee749292bc1942f8fbbb1fbfe7","modified":1589633501131},{"_id":"themes/Minos/layout/archive.ejs","hash":"e3eefe819d61b4d0ee069bb705a9f5707a8bf3da","modified":1589633501131},{"_id":"themes/Minos/layout/category.ejs","hash":"403c646878834964883ac41e63952f7b1595c0ba","modified":1589633501131},{"_id":"themes/Minos/layout/index.ejs","hash":"dff9e199d394f82c5416b814f9e644edbe4090f0","modified":1589633501134},{"_id":"themes/Minos/layout/layout.ejs","hash":"45588aa46857cf9403fa79d738ab37a46ddcf773","modified":1589633501134},{"_id":"themes/Minos/layout/post.ejs","hash":"68b84a717efc5ca59ee9eb6202ccf05c5a8abda5","modified":1589633501135},{"_id":"themes/Minos/layout/tags.ejs","hash":"e4a9909119294f131a45f10b2cb1058af5fb9be1","modified":1589633501136},{"_id":"themes/Minos/layout/tag.ejs","hash":"5593c7cf9618ef5650c779ed9d75424f057aa210","modified":1589633501136},{"_id":"themes/Minos/scripts/01_check.js","hash":"e53508ef82a1518ed7e663ce2d87544c20a50779","modified":1589633501137},{"_id":"themes/Minos/scripts/10_i18n.js","hash":"9c1f9e3ec3c6299fc599769783925d1898a632f0","modified":1589633501137},{"_id":"themes/Minos/scripts/99_config.js","hash":"d41a5df0a442728fbc66514476fe043e416d7438","modified":1589633501138},{"_id":"themes/Minos/scripts/99_content.js","hash":"ff61d3d631b06024509d4fe28b4e31590a02f05b","modified":1589633501138},{"_id":"themes/Minos/scripts/99_tags.js","hash":"f97c2332a7e13fed99672b27c4380a6183d8600e","modified":1589633501138},{"_id":"themes/Minos/layout/comment/changyan.ejs","hash":"9ccc7ec354b968e60bdcfcd1dba451d38de61f12","modified":1589633501131},{"_id":"themes/Minos/layout/comment/disqus.ejs","hash":"a2becdc02214a673c804af93488489807fa2c99c","modified":1589633501131},{"_id":"themes/Minos/layout/comment/facebook.ejs","hash":"e73b6f93d98b27ba9068c1685874ecccfbac737b","modified":1589633501131},{"_id":"themes/Minos/layout/comment/gitment.ejs","hash":"430416210933b7edcbfcc67ede4aa55539da2750","modified":1589633501132},{"_id":"themes/Minos/layout/comment/livere.ejs","hash":"12ff9a345f6bba2f732f592e39508c2afde89b00","modified":1589633501132},{"_id":"themes/Minos/layout/comment/isso.ejs","hash":"cc6a43bd24be764086f88ad7c5c97ff04df87e0b","modified":1589633501132},{"_id":"themes/Minos/layout/comment/valine.ejs","hash":"b0eef3bea0a54b4b66f860ad69889f87e0408f22","modified":1589633501132},{"_id":"themes/Minos/layout/comment/youyan.ejs","hash":"3d6cf9c523a7a5510ec2864bb29f861f9bb78af3","modified":1589633501132},{"_id":"themes/Minos/layout/common/article.ejs","hash":"45d276fb6bfcee6690cfffa7cbdec18709cd8766","modified":1589633501132},{"_id":"themes/Minos/layout/common/footer.ejs","hash":"367c5f2e69c66d4d6fbd8beeade0b60024ce9e6e","modified":1589638230789},{"_id":"themes/Minos/layout/common/head.ejs","hash":"4dd9352ea5f5a59d3af2bedac7d545c08bb08531","modified":1589633501133},{"_id":"themes/Minos/layout/common/languages.ejs","hash":"89665c656a1ffebc9c97f03e7f9c12dd1d90702a","modified":1589633501133},{"_id":"themes/Minos/layout/common/navbar.ejs","hash":"f3aa16f357450651d0454f1ddc06a032f3dc4de3","modified":1589633501133},{"_id":"themes/Minos/layout/common/paginator.ejs","hash":"8f5060e4c8a86a3f4e58455c41c98e831e23e4a4","modified":1589633501133},{"_id":"themes/Minos/layout/common/scripts.ejs","hash":"7a5a5271930423b95046836597e30e31fa708f66","modified":1589633501133},{"_id":"themes/Minos/layout/plugins/clipboard.ejs","hash":"a448757bb8a2c29bd9501c625f7df5087bb18dbe","modified":1589633501134},{"_id":"themes/Minos/layout/plugins/gallery.ejs","hash":"7c2becafdf6b60e677cdd5756b9d55eba2af4944","modified":1589633501134},{"_id":"themes/Minos/layout/plugins/google-analytics.ejs","hash":"2a9d944a60aff7df27def5215bdc071e605c3c42","modified":1589633501134},{"_id":"themes/Minos/layout/plugins/mathjax.ejs","hash":"b92fc2b30040e09145d80ebb9bad6813dda8acf2","modified":1589633501135},{"_id":"themes/Minos/layout/plugins/katex.ejs","hash":"c8a7ecdc5802007f8fec46a299790ce4e0834acd","modified":1589633501135},{"_id":"themes/Minos/layout/search/google-cse.ejs","hash":"a6bf5c30339735126efa7efa684f9eb14dd6136a","modified":1589633501135},{"_id":"themes/Minos/layout/search/insight.ejs","hash":"6fb7d27ef40145d8587b46b44a43516135b5a81a","modified":1589633501135},{"_id":"themes/Minos/layout/share/addthis.ejs","hash":"f1c5f337333009d5f00dfbac4864a16ef8f9cb8d","modified":1589633501136},{"_id":"themes/Minos/layout/share/sharethis.ejs","hash":"4f2c40f790f3be0a4e79db04f02ea41ba2f4d4c0","modified":1589633501136},{"_id":"themes/Minos/source/css/insight.scss","hash":"f785fc6574d2853c660be39b2e3149d4846b577f","modified":1589633501138},{"_id":"themes/Minos/source/images/check.svg","hash":"029b8b3523b7daa4005983b4463cd93408308aab","modified":1589633501139},{"_id":"themes/Minos/source/css/style.scss","hash":"ca4aed6487522a02e0b8f602d2e0542f233ac818","modified":1589633501139},{"_id":"themes/Minos/source/images/exclamation.svg","hash":"b2db56f2cc13fce73dbea46c7b446d9bcb3bf0fd","modified":1589633501139},{"_id":"themes/Minos/source/images/info.svg","hash":"c8aa387e935ba9a7fa72c5dd000b7d46f2e030c4","modified":1589633501139},{"_id":"themes/Minos/source/images/question.svg","hash":"7153fa2a0c21e32da6a1f96a333d8b66a178569d","modified":1589633501139},{"_id":"themes/Minos/source/images/logo.png","hash":"4e012d9ba58cb8f87ee775262ef871c158ac5948","modified":1589633501139},{"_id":"themes/Minos/source/images/quote-left.svg","hash":"d2561fa8d13e63ff196b71232a5968415ec6e372","modified":1589633501140},{"_id":"themes/Minos/source/js/insight.js","hash":"eb23c31141784eef7300f1d1c548950e77883f56","modified":1589633501140},{"_id":"themes/Minos/source/js/script.js","hash":"6b670ec4f90fb43b21a0bbd750a217af5d8aab6b","modified":1589633501140},{"_id":"source/_posts/The-mechanism-behind-WriteBatch-in-leveldb.md","hash":"a290eeafcaddc5016cb0b984a2b19b67c532bb89","modified":1589638594005},{"_id":"source/_posts/The-mechanism-behind-WriteBatch-in-leveldb/rep_format.png","hash":"a6092fa1672562a55d6b2f8e7c1fbf175e5ae5e6","modified":1589634314135},{"_id":"source/_posts/Redo-log-in-InnoDB.md","hash":"22ba11f9872a3436fb83b471f9aa6941a81f2fb2","modified":1589638514110},{"_id":"source/_posts/Redo-log-in-InnoDB/redo-log-block.jpg","hash":"bc4cc2268ef18f396963a27433a46de92d757d04","modified":1589635263857},{"_id":"source/_posts/Compile-your-own-linux-kernel.md","hash":"ecda67ac809f91ede21f812b3319e5d9114ea4ef","modified":1589638550082},{"_id":"source/_posts/Compile-your-own-linux-kernel/kernel-booting.png","hash":"c893fdae2c89ce5425baa0de6fa14927748bd871","modified":1589636093486},{"_id":"source/_posts/Compile-your-own-linux-kernel/kernel-directory.png","hash":"5912734b56fbb305f241128a69043bc3c1adea6e","modified":1589636028529},{"_id":"source/_posts/Compile-your-own-linux-kernel/kernel-configuration.png","hash":"ca26873142863858ea26deeed08516965e5025bc","modified":1589636053175},{"_id":"source/_posts/Compile-your-own-linux-kernel/kernel-grub.png","hash":"1d32e766359978deb091128bc690f360c83cb646","modified":1589636081674},{"_id":"source/_posts/Compile-your-own-linux-kernel/kernel-menuconfig.png","hash":"7ed118cc58f05e7ffe961290bcd2337f406ba72a","modified":1589636066610},{"_id":"source/_posts/Read-and-Write-functions-in-linux.md","hash":"5d1450869c115e3d0729091a0c0154b4713a18df","modified":1589638566995},{"_id":"public/content.json","hash":"0d62967787e37022e7a819a288f3ec4955ce167b","modified":1589821083584},{"_id":"public/2016/07/09/Redo-log-in-InnoDB/index.html","hash":"fc89c5b4de734006c680b61ad358ba36d8a84248","modified":1589821083584},{"_id":"public/2016/04/12/Read-and-Write-functions-in-linux/index.html","hash":"06f2c2a44e43d34f3cfb9df3e0e2be97d8e3d88a","modified":1589821083584},{"_id":"public/archives/index.html","hash":"e78e574bee53ad2a55041389bcb5c292ba690ff0","modified":1589821083584},{"_id":"public/archives/2016/index.html","hash":"8733c423513d32ae66d21c4da483afcd1386d708","modified":1589821083584},{"_id":"public/archives/2016/03/index.html","hash":"00fc38fd54a5cc6cf22e3d06d733ac9078eebed2","modified":1589821083584},{"_id":"public/archives/2016/04/index.html","hash":"9c1b7d0a99c6899f90d562f3a3951c79e65930fd","modified":1589821083584},{"_id":"public/archives/2016/07/index.html","hash":"2ca6722f6ec02dc021890652a040bda1dcca0257","modified":1589821083584},{"_id":"public/archives/2020/index.html","hash":"494935d7a89af68819c6a01aef234b1f6911c3d4","modified":1589821083584},{"_id":"public/archives/2020/05/index.html","hash":"4b22144191a8d8e33b92576d95dfb5196ecf2bc2","modified":1589821083584},{"_id":"public/index.html","hash":"16dadaeff104edf63949cea6275ea20393bf7aa6","modified":1589821083584},{"_id":"public/tags/database/index.html","hash":"a890ef6c43b9036554113c48337a6c577d86c704","modified":1589821083584},{"_id":"public/tags/kernel/index.html","hash":"c19e35b6cdb27b0215730f256cf8f57d2b6967d9","modified":1589821083584},{"_id":"public/tags/linux/index.html","hash":"b273b01cc4b53a51d3600d60c2a302c34a2660ad","modified":1589821083584},{"_id":"public/categories/Technology/index.html","hash":"a281371569042b0de7713480bb32d1ca904fe83a","modified":1589821083584},{"_id":"public/categories/index.html","hash":"c42201661e5c83d1dee63c0425d009599f37171f","modified":1589821083584},{"_id":"public/tags/index.html","hash":"2749cefedcf6e4550446b78d4f506b427777b010","modified":1589821083584},{"_id":"public/2020/05/16/The-mechanism-behind-WriteBatch-in-leveldb/index.html","hash":"98dbd7c2a2ac3e2959b6b7381b5bed38884b781f","modified":1589821083584},{"_id":"public/2016/03/23/Compile-your-own-linux-kernel/index.html","hash":"da26ecf0c73af8c81bbf44df968f951614c102e0","modified":1589821083584},{"_id":"public/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1589638305277},{"_id":"public/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1589638305277},{"_id":"public/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1589638305277},{"_id":"public/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1589638305277},{"_id":"public/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1589638305277},{"_id":"public/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1589638305277},{"_id":"public/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1589638305277},{"_id":"public/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1589638305277},{"_id":"public/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1589638305277},{"_id":"public/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1589638305277},{"_id":"public/images/check.svg","hash":"029b8b3523b7daa4005983b4463cd93408308aab","modified":1589638305277},{"_id":"public/images/logo.png","hash":"4e012d9ba58cb8f87ee775262ef871c158ac5948","modified":1589638305277},{"_id":"public/images/exclamation.svg","hash":"b2db56f2cc13fce73dbea46c7b446d9bcb3bf0fd","modified":1589638305277},{"_id":"public/images/info.svg","hash":"c8aa387e935ba9a7fa72c5dd000b7d46f2e030c4","modified":1589638305277},{"_id":"public/images/question.svg","hash":"7153fa2a0c21e32da6a1f96a333d8b66a178569d","modified":1589638305277},{"_id":"public/images/quote-left.svg","hash":"d2561fa8d13e63ff196b71232a5968415ec6e372","modified":1589638305277},{"_id":"public/2016/07/09/Redo-log-in-InnoDB/redo-log-block.jpg","hash":"bc4cc2268ef18f396963a27433a46de92d757d04","modified":1589638305277},{"_id":"public/2016/03/23/Compile-your-own-linux-kernel/kernel-booting.png","hash":"c893fdae2c89ce5425baa0de6fa14927748bd871","modified":1589638305277},{"_id":"public/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1589638305277},{"_id":"public/2020/05/16/The-mechanism-behind-WriteBatch-in-leveldb/rep_format.png","hash":"a6092fa1672562a55d6b2f8e7c1fbf175e5ae5e6","modified":1589638305277},{"_id":"public/2016/03/23/Compile-your-own-linux-kernel/kernel-grub.png","hash":"1d32e766359978deb091128bc690f360c83cb646","modified":1589638305277},{"_id":"public/2016/03/23/Compile-your-own-linux-kernel/kernel-directory.png","hash":"5912734b56fbb305f241128a69043bc3c1adea6e","modified":1589638305277},{"_id":"public/css/style.css","hash":"4e7a159e292129b1abbdcabc69aad37c4eb5fc8e","modified":1589638305277},{"_id":"public/js/script.js","hash":"6b670ec4f90fb43b21a0bbd750a217af5d8aab6b","modified":1589638305277},{"_id":"public/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1589638305277},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1589638305277},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1589638305277},{"_id":"public/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1589638305277},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1589638305277},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1589638305277},{"_id":"public/css/insight.css","hash":"f376dcda6bb50b708f3206c15a49f7530b3c534d","modified":1589638305277},{"_id":"public/js/insight.js","hash":"eb23c31141784eef7300f1d1c548950e77883f56","modified":1589638305277},{"_id":"public/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1589638305277},{"_id":"public/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1589638305277},{"_id":"public/2016/03/23/Compile-your-own-linux-kernel/kernel-menuconfig.png","hash":"7ed118cc58f05e7ffe961290bcd2337f406ba72a","modified":1589638305277},{"_id":"public/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1589638305277},{"_id":"public/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1589638305277},{"_id":"public/2016/03/23/Compile-your-own-linux-kernel/kernel-configuration.png","hash":"ca26873142863858ea26deeed08516965e5025bc","modified":1589638305277},{"_id":"themes/Minos/_config.yml","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1589819172601},{"_id":"public/tags/C/index.html","hash":"e347568de3a58fe3d5200a1b95eb399741cdb46a","modified":1589821083584},{"_id":"public/tags/mysql/index.html","hash":"57a615476a53b531949d7b769b7661871159ffe7","modified":1589821083584},{"_id":"public/tags/leveldb/index.html","hash":"b0fa2f00dc7592372f29a78ad294cc28d0abc3b5","modified":1589821083584},{"_id":"source/_posts/How-leveldb-log-works.md","hash":"041538456f48b5e424808981097bd35e28e9f344","modified":1590933956554},{"_id":"source/_posts/How-leveldb-log-works/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1590933712151},{"_id":"source/_posts/How-leveldb-log-works/record_within_one_block.png","hash":"6c38916b25093f039ccc3cc32d2a39dcb6a2abb1","modified":1590927526090},{"_id":"source/_posts/How-leveldb-log-works/record_format.png","hash":"81966ac5f76b774a06c2f7c0abdc9f564879686e","modified":1590916274423},{"_id":"source/_posts/How-leveldb-log-works/record_not_for_header.png","hash":"b8fa63e4ffe0e197126f48fc2a02be0cb6196520","modified":1590926910332},{"_id":"source/_posts/How-leveldb-log-works/record_across_multipe_blocks.png","hash":"acd21f734a596d38ca44f279641587e541575683","modified":1590927961830},{"_id":"source/_posts/How-leveldb-log-works/record_across_two_blocks.png","hash":"6b004d87beabec9b7bbcfd1b9c116b8c5f75bc30","modified":1590927283814}],"Category":[{"name":"Technology","_id":"cka9pgaji0000rppkhq8jbvzn"}],"Data":[],"Page":[],"Post":[{"title":"The mechanism behind WriteBatch in leveldb","date":"2020-05-16T12:59:09.000Z","_content":"\n### Introduction\n\nAs a well-known key-value database,[ leveldb](https://github.com/google/leveldb) provides general key-values interfaces like Put, Get and Delete. Besides those interfaces, leveldb provides a batch operation called WriteBatch as well. A batch operation means we can group multiple operations into one and submit this one to leveldb, the atomicity guarantees that either all of those operations are applied or none of them is applied.\n\nFor example, we can use batch operation in leveldb in the following way:\n\n```C++\nWriteBatch b;\nbatch.Put(\"key\", \"v1\");\nbatch.Delete(\"key\");\nbatch.Put(\"key\", \"v2\");\nbatch.Put(\"key\", \"v3\");\ndb->Write(WriteOptions(), &b);\n```\n\nThe updates are applied in the order in which they are added to the WriteBatch. And the value of \"key\" in the above code sample will be \"v3\" after the batch is written.\n\n### Implementaion of WriteBatch\n\nWell, how does leveldb implement this simple but powerful interface? Let's figure it out by digging the source code step by step.\n\n<!-- more -->\n\n\n\n#### WriteBatch Class\n\nThe `WriteBatch` is a class containing member function `WriteBatch::Put`, `WriteBatch::Delete`,`WriteBatch::Clear` and `WriteBatch::Iterate`. A private member variable of string type called `rep_` is also owned by it.\n\n\n\nIn the constructor of `WriteBatch`, `rep_` is cleared firstly and resize to length `kHeader`:\n\n```c++\nstatic const size_t kHeader = 12;\n\nWriteBatch::WriteBatch() {\n  Clear();\n}\n\nvoid WriteBatch::Clear() {\n  rep_.clear();\n  rep_.resize(kHeader);\n}\n```\n\nThat's because the first `kHeader` bytes in `rep_` is used to maintain meta information: 8-byte sequence number and followed by a 4-byte count.\n\n\n\nEach time when the user calls `WriteBatch::Put`, the `rep_` variable gets updated by the following behaviors :\n\n```C++\nvoid WriteBatch::Put(const Slice& key, const Slice& value) {\n  WriteBatchInternal::SetCount(this, WriteBatchInternal::Count(this) + 1);\n  rep_.push_back(static_cast<char>(kTypeValue));\n  PutLengthPrefixedSlice(&rep_, key);\n  PutLengthPrefixedSlice(&rep_, value);\n}\n```\n\nConsequently, the 4-byte count in `rep_` gets incremented, and an enum char value `kTypeValue`(means this is a put operation), the key and the value are appended into the end of `rep_`.\n\n\n\nSimilarly, when the user calls `WriteBatch::Delete`, the `rep_` variable is updated in a similar way:\n\n```C++\nvoid WriteBatch::Delete(const Slice& key) {\n  WriteBatchInternal::SetCount(this, WriteBatchInternal::Count(this) + 1);\n  rep_.push_back(static_cast<char>(kTypeDeletion));\n  PutLengthPrefixedSlice(&rep_, key);\n}\n```\n\nExcepting that an enum char value `kTypeDeletion`(means this is a put operation) and the key information are appended into the `rep_`.\n\n\n\nLet's be more specific about the function `PutLengthPrefixedSlice` called by both `WriteBatch::Delete`and `WriteBatch::Put`:\n\n```C++\nvoid PutLengthPrefixedSlice(std::string* dst, const Slice& value) {\n  PutVarint32(dst, value.size());\n  dst->append(value.data(), value.size());\n}\n```\n\nIt appends the value size into dst and then the value itself into dst.\n\nBased on all the information provided above, we know that the `Put` and `Delete` member function of `WriteBatch` only update its member variable `rep_`. \n\nAnd we can also know the data layout format of `rep_`: \n\n```\nWriteBatch::rep_ :=\n      sequence: fixed64\n      count: fixed32\n      data: record[count]\nrecord :=\n      kTypeValue varstring varstring         |\n      kTypeDeletion varstring\nvarstring :=\n      len: varint32\n      data: uint8[len]\n```\n\n{% asset_img rep_format.png image of rep_ format %}\n\n\n\n#### DBImpl::Write\n\nAfter a batch operation is encapsulated, it's time to call `DBImpl::Write` to apply the batch operation into the underlying layer to persist it.\n\nInside `DBImpl::Write`, an instance of `Writer` called `w` , which represents the batch operation context, is created and added to the end of the double-ended queue `writers_`.  Because write operation is in strict order, so the write thread must wait until `w` is the first element of `writers_`.\n\n```c++\nWriter w(&mutex_);\n  w.batch = my_batch;\n  w.sync = options.sync;\n  w.done = false;\n\n  MutexLock l(&mutex_);\n  writers_.push_back(&w);\n  while (!w.done && &w != writers_.front()) {\n    w.cv.Wait();\n  }\n  if (w.done) {\n    return w.status;\n  }\n```\n\n\n\nWhen the condition is fulfilled, leveldb will make sure there is enough room for this batch operation. Enough room mainly means memtable is prepared well to handle this write (I will write another new article to talk about that). Because `w` becomes the first element in `writers_`, the write thread will try to group other writers behind `w` into a so-called `BatchGroup` to speed up the io efficiency by the function `DBImpl::BuildBatchGroup`.\n\n```c++\nWriteBatch* DBImpl::BuildBatchGroup(Writer** last_writer) {\n  assert(!writers_.empty());\n  Writer* first = writers_.front();\n  WriteBatch* result = first->batch;\n  assert(result != NULL);\n\n  size_t size = WriteBatchInternal::ByteSize(first->batch);\n\n  // Allow the group to grow up to a maximum size, but if the\n  // original write is small, limit the growth so we do not slow\n  // down the small write too much.\n  size_t max_size = 1 << 20; // 1M\n  if (size <= (128<<10)) { // 128K\n    max_size = size + (128<<10);\n  }\n\n  *last_writer = first;\n  std::deque<Writer*>::iterator iter = writers_.begin();\n  ++iter;  // Advance past \"first\"\n  for (; iter != writers_.end(); ++iter) {\n    Writer* w = *iter;\n    if (w->sync && !first->sync) {\n      // Do not include a sync write into a batch handled by a non-sync write.\n      break;\n    }\n\n    if (w->batch != NULL) {\n      size += WriteBatchInternal::ByteSize(w->batch);\n      if (size > max_size) {\n        // Do not make batch too big\n        break;\n      }\n\n      // Append to *result\n      if (result == first->batch) {\n        // Switch to temporary batch instead of disturbing caller's batch\n        result = tmp_batch_;\n        assert(WriteBatchInternal::Count(result) == 0);\n        WriteBatchInternal::Append(result, first->batch);\n      }\n      WriteBatchInternal::Append(result, w->batch);\n    }\n    *last_writer = w;\n  }\n  return result;\n}\n```\n\nInside `DBImpl::BuildBatchGroup`, it will traverse the whole `writers_` from beginning to end and try to group all  those batches into one batch. For convenience to clarify, we call this procedure \"batch mergence\"\n\nHowever, there are some limitations in batch mergence. First, the maximum size of `rep_` in the merged batch group is **1M** and if the first writer is a small writer(`rep_.size() <= 128K` ) it will limit the maximum size to `rep_.size() + 128K`. The idea behind this is not slowing down the small write too much. Second, it will not include a sync write into the batch handled by a non-sync write.\n\nIn the batch mergence, with the help of `WriteBatchInternal::Append`, the data field of `rep_` in second batch is appended to the end of the `rep_` in first batch:\n\n```c++\nvoid WriteBatchInternal::Append(WriteBatch* dst, const WriteBatch* src) {\n  SetCount(dst, Count(dst) + Count(src));\n  assert(src->rep_.size() >= kHeader);\n  dst->rep_.append(src->rep_.data() + kHeader, src->rep_.size() - kHeader);\n}\n```\n\nAfter batch mergence is finished, a new batch called `updates` is generated. And the sequence number in `rep_` of `updates` is set to `last_sequence + 1`, `last_sequnce` gets adjusted to a new one(`last_sequnce = last_sequnce + count field rep_ of updates`) as well.\n\n\n\nNow, we come to the IO critical path. The WAL in leveldb will persist the contents of the `update` in an append-only way. After that success, memtable will replay the operations in the batch `update` and do corresponding behavior in memtable:\n\n\n\nApply to WAL and insert into memtable:\n\n```c++\n{\n      mutex_.Unlock();\n      status = log_->AddRecord(WriteBatchInternal::Contents(updates));\n      bool sync_error = false;\n      if (status.ok() && options.sync) {\n        status = logfile_->Sync();\n        if (!status.ok()) {\n          sync_error = true;\n        }\n      }\n      if (status.ok()) {\n        status = WriteBatchInternal::InsertInto(updates, mem_);\n      }\n      mutex_.Lock();\n      if (sync_error) {\n        // The state of the log file is indeterminate: the log record we\n        // just added may or may not show up when the DB is re-opened.\n        // So we force the DB into a mode where all future writes fail.\n        RecordBackgroundError(status);\n      }\n    }\n```\n\n\n\n`WriteBatch` Parse the whole `rep_` string to replay original operations specified by users and call handler(MemTableInserter) to handle these operations.\n\n```\nStatus WriteBatch::Iterate(Handler* handler) const {\n  Slice input(rep_);\n  if (input.size() < kHeader) {\n    return Status::Corruption(\"malformed WriteBatch (too small)\");\n  }\n\n  input.remove_prefix(kHeader);\n  Slice key, value;\n  int found = 0;\n  while (!input.empty()) {\n    found++;\n    char tag = input[0];\n    input.remove_prefix(1);\n    switch (tag) {\n      case kTypeValue:\n        if (GetLengthPrefixedSlice(&input, &key) &&\n            GetLengthPrefixedSlice(&input, &value)) {\n          handler->Put(key, value);\n        } else {\n          return Status::Corruption(\"bad WriteBatch Put\");\n        }\n        break;\n      case kTypeDeletion:\n        if (GetLengthPrefixedSlice(&input, &key)) {\n          handler->Delete(key);\n        } else {\n          return Status::Corruption(\"bad WriteBatch Delete\");\n        }\n        break;\n      default:\n        return Status::Corruption(\"unknown WriteBatch tag\");\n    }\n  }\n  if (found != WriteBatchInternal::Count(this)) {\n    return Status::Corruption(\"WriteBatch has wrong count\");\n  } else {\n    return Status::OK();\n  }\n}\n```\n\n\n\n### Summary\n\nHoo, finally get here! This article mainly talks about how batch operation is handled in leveldb. Due to space limitation, other import parts like WAL and Memtable in leveldb cannot be presented here.\n\nHave fun~~~\n\n","source":"_posts/The-mechanism-behind-WriteBatch-in-leveldb.md","raw":"---\ntitle: The mechanism behind WriteBatch in leveldb\ndate: 2020-05-16 20:59:09\ntags: [database, leveldb]\ncategories: Technology\n---\n\n### Introduction\n\nAs a well-known key-value database,[ leveldb](https://github.com/google/leveldb) provides general key-values interfaces like Put, Get and Delete. Besides those interfaces, leveldb provides a batch operation called WriteBatch as well. A batch operation means we can group multiple operations into one and submit this one to leveldb, the atomicity guarantees that either all of those operations are applied or none of them is applied.\n\nFor example, we can use batch operation in leveldb in the following way:\n\n```C++\nWriteBatch b;\nbatch.Put(\"key\", \"v1\");\nbatch.Delete(\"key\");\nbatch.Put(\"key\", \"v2\");\nbatch.Put(\"key\", \"v3\");\ndb->Write(WriteOptions(), &b);\n```\n\nThe updates are applied in the order in which they are added to the WriteBatch. And the value of \"key\" in the above code sample will be \"v3\" after the batch is written.\n\n### Implementaion of WriteBatch\n\nWell, how does leveldb implement this simple but powerful interface? Let's figure it out by digging the source code step by step.\n\n<!-- more -->\n\n\n\n#### WriteBatch Class\n\nThe `WriteBatch` is a class containing member function `WriteBatch::Put`, `WriteBatch::Delete`,`WriteBatch::Clear` and `WriteBatch::Iterate`. A private member variable of string type called `rep_` is also owned by it.\n\n\n\nIn the constructor of `WriteBatch`, `rep_` is cleared firstly and resize to length `kHeader`:\n\n```c++\nstatic const size_t kHeader = 12;\n\nWriteBatch::WriteBatch() {\n  Clear();\n}\n\nvoid WriteBatch::Clear() {\n  rep_.clear();\n  rep_.resize(kHeader);\n}\n```\n\nThat's because the first `kHeader` bytes in `rep_` is used to maintain meta information: 8-byte sequence number and followed by a 4-byte count.\n\n\n\nEach time when the user calls `WriteBatch::Put`, the `rep_` variable gets updated by the following behaviors :\n\n```C++\nvoid WriteBatch::Put(const Slice& key, const Slice& value) {\n  WriteBatchInternal::SetCount(this, WriteBatchInternal::Count(this) + 1);\n  rep_.push_back(static_cast<char>(kTypeValue));\n  PutLengthPrefixedSlice(&rep_, key);\n  PutLengthPrefixedSlice(&rep_, value);\n}\n```\n\nConsequently, the 4-byte count in `rep_` gets incremented, and an enum char value `kTypeValue`(means this is a put operation), the key and the value are appended into the end of `rep_`.\n\n\n\nSimilarly, when the user calls `WriteBatch::Delete`, the `rep_` variable is updated in a similar way:\n\n```C++\nvoid WriteBatch::Delete(const Slice& key) {\n  WriteBatchInternal::SetCount(this, WriteBatchInternal::Count(this) + 1);\n  rep_.push_back(static_cast<char>(kTypeDeletion));\n  PutLengthPrefixedSlice(&rep_, key);\n}\n```\n\nExcepting that an enum char value `kTypeDeletion`(means this is a put operation) and the key information are appended into the `rep_`.\n\n\n\nLet's be more specific about the function `PutLengthPrefixedSlice` called by both `WriteBatch::Delete`and `WriteBatch::Put`:\n\n```C++\nvoid PutLengthPrefixedSlice(std::string* dst, const Slice& value) {\n  PutVarint32(dst, value.size());\n  dst->append(value.data(), value.size());\n}\n```\n\nIt appends the value size into dst and then the value itself into dst.\n\nBased on all the information provided above, we know that the `Put` and `Delete` member function of `WriteBatch` only update its member variable `rep_`. \n\nAnd we can also know the data layout format of `rep_`: \n\n```\nWriteBatch::rep_ :=\n      sequence: fixed64\n      count: fixed32\n      data: record[count]\nrecord :=\n      kTypeValue varstring varstring         |\n      kTypeDeletion varstring\nvarstring :=\n      len: varint32\n      data: uint8[len]\n```\n\n{% asset_img rep_format.png image of rep_ format %}\n\n\n\n#### DBImpl::Write\n\nAfter a batch operation is encapsulated, it's time to call `DBImpl::Write` to apply the batch operation into the underlying layer to persist it.\n\nInside `DBImpl::Write`, an instance of `Writer` called `w` , which represents the batch operation context, is created and added to the end of the double-ended queue `writers_`.  Because write operation is in strict order, so the write thread must wait until `w` is the first element of `writers_`.\n\n```c++\nWriter w(&mutex_);\n  w.batch = my_batch;\n  w.sync = options.sync;\n  w.done = false;\n\n  MutexLock l(&mutex_);\n  writers_.push_back(&w);\n  while (!w.done && &w != writers_.front()) {\n    w.cv.Wait();\n  }\n  if (w.done) {\n    return w.status;\n  }\n```\n\n\n\nWhen the condition is fulfilled, leveldb will make sure there is enough room for this batch operation. Enough room mainly means memtable is prepared well to handle this write (I will write another new article to talk about that). Because `w` becomes the first element in `writers_`, the write thread will try to group other writers behind `w` into a so-called `BatchGroup` to speed up the io efficiency by the function `DBImpl::BuildBatchGroup`.\n\n```c++\nWriteBatch* DBImpl::BuildBatchGroup(Writer** last_writer) {\n  assert(!writers_.empty());\n  Writer* first = writers_.front();\n  WriteBatch* result = first->batch;\n  assert(result != NULL);\n\n  size_t size = WriteBatchInternal::ByteSize(first->batch);\n\n  // Allow the group to grow up to a maximum size, but if the\n  // original write is small, limit the growth so we do not slow\n  // down the small write too much.\n  size_t max_size = 1 << 20; // 1M\n  if (size <= (128<<10)) { // 128K\n    max_size = size + (128<<10);\n  }\n\n  *last_writer = first;\n  std::deque<Writer*>::iterator iter = writers_.begin();\n  ++iter;  // Advance past \"first\"\n  for (; iter != writers_.end(); ++iter) {\n    Writer* w = *iter;\n    if (w->sync && !first->sync) {\n      // Do not include a sync write into a batch handled by a non-sync write.\n      break;\n    }\n\n    if (w->batch != NULL) {\n      size += WriteBatchInternal::ByteSize(w->batch);\n      if (size > max_size) {\n        // Do not make batch too big\n        break;\n      }\n\n      // Append to *result\n      if (result == first->batch) {\n        // Switch to temporary batch instead of disturbing caller's batch\n        result = tmp_batch_;\n        assert(WriteBatchInternal::Count(result) == 0);\n        WriteBatchInternal::Append(result, first->batch);\n      }\n      WriteBatchInternal::Append(result, w->batch);\n    }\n    *last_writer = w;\n  }\n  return result;\n}\n```\n\nInside `DBImpl::BuildBatchGroup`, it will traverse the whole `writers_` from beginning to end and try to group all  those batches into one batch. For convenience to clarify, we call this procedure \"batch mergence\"\n\nHowever, there are some limitations in batch mergence. First, the maximum size of `rep_` in the merged batch group is **1M** and if the first writer is a small writer(`rep_.size() <= 128K` ) it will limit the maximum size to `rep_.size() + 128K`. The idea behind this is not slowing down the small write too much. Second, it will not include a sync write into the batch handled by a non-sync write.\n\nIn the batch mergence, with the help of `WriteBatchInternal::Append`, the data field of `rep_` in second batch is appended to the end of the `rep_` in first batch:\n\n```c++\nvoid WriteBatchInternal::Append(WriteBatch* dst, const WriteBatch* src) {\n  SetCount(dst, Count(dst) + Count(src));\n  assert(src->rep_.size() >= kHeader);\n  dst->rep_.append(src->rep_.data() + kHeader, src->rep_.size() - kHeader);\n}\n```\n\nAfter batch mergence is finished, a new batch called `updates` is generated. And the sequence number in `rep_` of `updates` is set to `last_sequence + 1`, `last_sequnce` gets adjusted to a new one(`last_sequnce = last_sequnce + count field rep_ of updates`) as well.\n\n\n\nNow, we come to the IO critical path. The WAL in leveldb will persist the contents of the `update` in an append-only way. After that success, memtable will replay the operations in the batch `update` and do corresponding behavior in memtable:\n\n\n\nApply to WAL and insert into memtable:\n\n```c++\n{\n      mutex_.Unlock();\n      status = log_->AddRecord(WriteBatchInternal::Contents(updates));\n      bool sync_error = false;\n      if (status.ok() && options.sync) {\n        status = logfile_->Sync();\n        if (!status.ok()) {\n          sync_error = true;\n        }\n      }\n      if (status.ok()) {\n        status = WriteBatchInternal::InsertInto(updates, mem_);\n      }\n      mutex_.Lock();\n      if (sync_error) {\n        // The state of the log file is indeterminate: the log record we\n        // just added may or may not show up when the DB is re-opened.\n        // So we force the DB into a mode where all future writes fail.\n        RecordBackgroundError(status);\n      }\n    }\n```\n\n\n\n`WriteBatch` Parse the whole `rep_` string to replay original operations specified by users and call handler(MemTableInserter) to handle these operations.\n\n```\nStatus WriteBatch::Iterate(Handler* handler) const {\n  Slice input(rep_);\n  if (input.size() < kHeader) {\n    return Status::Corruption(\"malformed WriteBatch (too small)\");\n  }\n\n  input.remove_prefix(kHeader);\n  Slice key, value;\n  int found = 0;\n  while (!input.empty()) {\n    found++;\n    char tag = input[0];\n    input.remove_prefix(1);\n    switch (tag) {\n      case kTypeValue:\n        if (GetLengthPrefixedSlice(&input, &key) &&\n            GetLengthPrefixedSlice(&input, &value)) {\n          handler->Put(key, value);\n        } else {\n          return Status::Corruption(\"bad WriteBatch Put\");\n        }\n        break;\n      case kTypeDeletion:\n        if (GetLengthPrefixedSlice(&input, &key)) {\n          handler->Delete(key);\n        } else {\n          return Status::Corruption(\"bad WriteBatch Delete\");\n        }\n        break;\n      default:\n        return Status::Corruption(\"unknown WriteBatch tag\");\n    }\n  }\n  if (found != WriteBatchInternal::Count(this)) {\n    return Status::Corruption(\"WriteBatch has wrong count\");\n  } else {\n    return Status::OK();\n  }\n}\n```\n\n\n\n### Summary\n\nHoo, finally get here! This article mainly talks about how batch operation is handled in leveldb. Due to space limitation, other import parts like WAL and Memtable in leveldb cannot be presented here.\n\nHave fun~~~\n\n","slug":"The-mechanism-behind-WriteBatch-in-leveldb","published":1,"updated":"2020-05-16T14:16:34.005Z","_id":"cka9n1t1y0000grpkegovf6kg","comments":1,"layout":"post","photos":[],"link":"","content":"<html><head></head><body><h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>As a well-known key-value database,<a href=\"https://github.com/google/leveldb\" target=\"_blank\" rel=\"noopener\"> leveldb</a> provides general key-values interfaces like Put, Get and Delete. Besides those interfaces, leveldb provides a batch operation called WriteBatch as well. A batch operation means we can group multiple operations into one and submit this one to leveldb, the atomicity guarantees that either all of those operations are applied or none of them is applied.</p>\n<p>For example, we can use batch operation in leveldb in the following way:</p>\n<p></p><figure class=\"highlight c++ hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">WriteBatch b;</span><br><span class=\"line\">batch.Put(<span class=\"hljs-string\">&quot;key&quot;</span>, <span class=\"hljs-string\">&quot;v1&quot;</span>);</span><br><span class=\"line\">batch.Delete(<span class=\"hljs-string\">&quot;key&quot;</span>);</span><br><span class=\"line\">batch.Put(<span class=\"hljs-string\">&quot;key&quot;</span>, <span class=\"hljs-string\">&quot;v2&quot;</span>);</span><br><span class=\"line\">batch.Put(<span class=\"hljs-string\">&quot;key&quot;</span>, <span class=\"hljs-string\">&quot;v3&quot;</span>);</span><br><span class=\"line\">db-&gt;Write(WriteOptions(), &amp;b);</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>The updates are applied in the order in which they are added to the WriteBatch. And the value of &#x201C;key&#x201D; in the above code sample will be &#x201C;v3&#x201D; after the batch is written.</p>\n<h3 id=\"Implementaion-of-WriteBatch\"><a href=\"#Implementaion-of-WriteBatch\" class=\"headerlink\" title=\"Implementaion of WriteBatch\"></a>Implementaion of WriteBatch</h3><p>Well, how does leveldb implement this simple but powerful interface? Let&#x2019;s figure it out by digging the source code step by step.</p>\n<a id=\"more\"></a>\n\n\n\n<h4 id=\"WriteBatch-Class\"><a href=\"#WriteBatch-Class\" class=\"headerlink\" title=\"WriteBatch Class\"></a>WriteBatch Class</h4><p>The <code>WriteBatch</code> is a class containing member function <code>WriteBatch::Put</code>, <code>WriteBatch::Delete</code>,<code>WriteBatch::Clear</code> and <code>WriteBatch::Iterate</code>. A private member variable of string type called <code>rep_</code> is also owned by it.</p>\n<p>In the constructor of <code>WriteBatch</code>, <code>rep_</code> is cleared firstly and resize to length <code>kHeader</code>:</p>\n<p></p><figure class=\"highlight c++ hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-keyword\">static</span> <span class=\"hljs-keyword\">const</span> <span class=\"hljs-keyword\">size_t</span> kHeader = <span class=\"hljs-number\">12</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">WriteBatch::WriteBatch() {</span><br><span class=\"line\">  Clear();</span><br><span class=\"line\">}</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">void</span> <span class=\"hljs-title\">WriteBatch::Clear</span><span class=\"hljs-params\">()</span> </span>{</span><br><span class=\"line\">  rep_.<span class=\"hljs-built_in\">clear</span>();</span><br><span class=\"line\">  rep_.resize(kHeader);</span><br><span class=\"line\">}</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>That&#x2019;s because the first <code>kHeader</code> bytes in <code>rep_</code> is used to maintain meta information: 8-byte sequence number and followed by a 4-byte count.</p>\n<p>Each time when the user calls <code>WriteBatch::Put</code>, the <code>rep_</code> variable gets updated by the following behaviors :</p>\n<p></p><figure class=\"highlight c++ hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">void</span> <span class=\"hljs-title\">WriteBatch::Put</span><span class=\"hljs-params\">(<span class=\"hljs-keyword\">const</span> Slice&amp; key, <span class=\"hljs-keyword\">const</span> Slice&amp; value)</span> </span>{</span><br><span class=\"line\">  WriteBatchInternal::SetCount(<span class=\"hljs-keyword\">this</span>, WriteBatchInternal::Count(<span class=\"hljs-keyword\">this</span>) + <span class=\"hljs-number\">1</span>);</span><br><span class=\"line\">  rep_.push_back(<span class=\"hljs-keyword\">static_cast</span>&lt;<span class=\"hljs-keyword\">char</span>&gt;(kTypeValue));</span><br><span class=\"line\">  PutLengthPrefixedSlice(&amp;rep_, key);</span><br><span class=\"line\">  PutLengthPrefixedSlice(&amp;rep_, value);</span><br><span class=\"line\">}</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>Consequently, the 4-byte count in <code>rep_</code> gets incremented, and an enum char value <code>kTypeValue</code>(means this is a put operation), the key and the value are appended into the end of <code>rep_</code>.</p>\n<p>Similarly, when the user calls <code>WriteBatch::Delete</code>, the <code>rep_</code> variable is updated in a similar way:</p>\n<p></p><figure class=\"highlight c++ hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">void</span> <span class=\"hljs-title\">WriteBatch::Delete</span><span class=\"hljs-params\">(<span class=\"hljs-keyword\">const</span> Slice&amp; key)</span> </span>{</span><br><span class=\"line\">  WriteBatchInternal::SetCount(<span class=\"hljs-keyword\">this</span>, WriteBatchInternal::Count(<span class=\"hljs-keyword\">this</span>) + <span class=\"hljs-number\">1</span>);</span><br><span class=\"line\">  rep_.push_back(<span class=\"hljs-keyword\">static_cast</span>&lt;<span class=\"hljs-keyword\">char</span>&gt;(kTypeDeletion));</span><br><span class=\"line\">  PutLengthPrefixedSlice(&amp;rep_, key);</span><br><span class=\"line\">}</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>Excepting that an enum char value <code>kTypeDeletion</code>(means this is a put operation) and the key information are appended into the <code>rep_</code>.</p>\n<p>Let&#x2019;s be more specific about the function <code>PutLengthPrefixedSlice</code> called by both <code>WriteBatch::Delete</code>and <code>WriteBatch::Put</code>:</p>\n<p></p><figure class=\"highlight c++ hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">void</span> <span class=\"hljs-title\">PutLengthPrefixedSlice</span><span class=\"hljs-params\">(<span class=\"hljs-built_in\">std</span>::<span class=\"hljs-built_in\">string</span>* dst, <span class=\"hljs-keyword\">const</span> Slice&amp; value)</span> </span>{</span><br><span class=\"line\">  PutVarint32(dst, value.<span class=\"hljs-built_in\">size</span>());</span><br><span class=\"line\">  dst-&gt;append(value.data(), value.<span class=\"hljs-built_in\">size</span>());</span><br><span class=\"line\">}</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>It appends the value size into dst and then the value itself into dst.</p>\n<p>Based on all the information provided above, we know that the <code>Put</code> and <code>Delete</code> member function of <code>WriteBatch</code> only update its member variable <code>rep_</code>. </p>\n<p>And we can also know the data layout format of <code>rep_</code>: </p>\n<p></p><figure class=\"highlight plain hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">WriteBatch::rep_ :=</span><br><span class=\"line\">      sequence: fixed64</span><br><span class=\"line\">      count: fixed32</span><br><span class=\"line\">      data: record[count]</span><br><span class=\"line\">record :=</span><br><span class=\"line\">      kTypeValue varstring varstring         |</span><br><span class=\"line\">      kTypeDeletion varstring</span><br><span class=\"line\">varstring :=</span><br><span class=\"line\">      len: varint32</span><br><span class=\"line\">      data: uint8[len]</span><br></pre></td></tr></tbody></table></figure><p></p>\n<img src=\"/2020/05/16/The-mechanism-behind-WriteBatch-in-leveldb/rep_format.png\" class title=\"image of rep_ format\">\n\n\n\n<h4 id=\"DBImpl-Write\"><a href=\"#DBImpl-Write\" class=\"headerlink\" title=\"DBImpl::Write\"></a>DBImpl::Write</h4><p>After a batch operation is encapsulated, it&#x2019;s time to call <code>DBImpl::Write</code> to apply the batch operation into the underlying layer to persist it.</p>\n<p>Inside <code>DBImpl::Write</code>, an instance of <code>Writer</code> called <code>w</code> , which represents the batch operation context, is created and added to the end of the double-ended queue <code>writers_</code>.  Because write operation is in strict order, so the write thread must wait until <code>w</code> is the first element of <code>writers_</code>.</p>\n<p></p><figure class=\"highlight c++ hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-function\">Writer <span class=\"hljs-title\">w</span><span class=\"hljs-params\">(&amp;mutex_)</span></span>;</span><br><span class=\"line\">  w.batch = my_batch;</span><br><span class=\"line\">  w.sync = options.sync;</span><br><span class=\"line\">  w.done = <span class=\"hljs-literal\">false</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-function\">MutexLock <span class=\"hljs-title\">l</span><span class=\"hljs-params\">(&amp;mutex_)</span></span>;</span><br><span class=\"line\">  writers_.push_back(&amp;w);</span><br><span class=\"line\">  <span class=\"hljs-keyword\">while</span> (!w.done &amp;&amp; &amp;w != writers_.front()) {</span><br><span class=\"line\">    w.cv.Wait();</span><br><span class=\"line\">  }</span><br><span class=\"line\">  <span class=\"hljs-keyword\">if</span> (w.done) {</span><br><span class=\"line\">    <span class=\"hljs-keyword\">return</span> w.status;</span><br><span class=\"line\">  }</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>When the condition is fulfilled, leveldb will make sure there is enough room for this batch operation. Enough room mainly means memtable is prepared well to handle this write (I will write another new article to talk about that). Because <code>w</code> becomes the first element in <code>writers_</code>, the write thread will try to group other writers behind <code>w</code> into a so-called <code>BatchGroup</code> to speed up the io efficiency by the function <code>DBImpl::BuildBatchGroup</code>.</p>\n<p></p><figure class=\"highlight c++ hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-function\">WriteBatch* <span class=\"hljs-title\">DBImpl::BuildBatchGroup</span><span class=\"hljs-params\">(Writer** last_writer)</span> </span>{</span><br><span class=\"line\">  assert(!writers_.empty());</span><br><span class=\"line\">  Writer* first = writers_.front();</span><br><span class=\"line\">  WriteBatch* result = first-&gt;batch;</span><br><span class=\"line\">  assert(result != <span class=\"hljs-literal\">NULL</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-keyword\">size_t</span> <span class=\"hljs-built_in\">size</span> = WriteBatchInternal::ByteSize(first-&gt;batch);</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-comment\">// Allow the group to grow up to a maximum size, but if the</span></span><br><span class=\"line\">  <span class=\"hljs-comment\">// original write is small, limit the growth so we do not slow</span></span><br><span class=\"line\">  <span class=\"hljs-comment\">// down the small write too much.</span></span><br><span class=\"line\">  <span class=\"hljs-keyword\">size_t</span> max_size = <span class=\"hljs-number\">1</span> &lt;&lt; <span class=\"hljs-number\">20</span>; <span class=\"hljs-comment\">// 1M</span></span><br><span class=\"line\">  <span class=\"hljs-keyword\">if</span> (<span class=\"hljs-built_in\">size</span> &lt;= (<span class=\"hljs-number\">128</span>&lt;&lt;<span class=\"hljs-number\">10</span>)) { <span class=\"hljs-comment\">// 128K</span></span><br><span class=\"line\">    max_size = <span class=\"hljs-built_in\">size</span> + (<span class=\"hljs-number\">128</span>&lt;&lt;<span class=\"hljs-number\">10</span>);</span><br><span class=\"line\">  }</span><br><span class=\"line\"></span><br><span class=\"line\">  *last_writer = first;</span><br><span class=\"line\">  <span class=\"hljs-built_in\">std</span>::<span class=\"hljs-built_in\">deque</span>&lt;Writer*&gt;::iterator iter = writers_.<span class=\"hljs-built_in\">begin</span>();</span><br><span class=\"line\">  ++iter;  <span class=\"hljs-comment\">// Advance past &quot;first&quot;</span></span><br><span class=\"line\">  <span class=\"hljs-keyword\">for</span> (; iter != writers_.<span class=\"hljs-built_in\">end</span>(); ++iter) {</span><br><span class=\"line\">    Writer* w = *iter;</span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (w-&gt;sync &amp;&amp; !first-&gt;sync) {</span><br><span class=\"line\">      <span class=\"hljs-comment\">// Do not include a sync write into a batch handled by a non-sync write.</span></span><br><span class=\"line\">      <span class=\"hljs-keyword\">break</span>;</span><br><span class=\"line\">    }</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (w-&gt;batch != <span class=\"hljs-literal\">NULL</span>) {</span><br><span class=\"line\">      <span class=\"hljs-built_in\">size</span> += WriteBatchInternal::ByteSize(w-&gt;batch);</span><br><span class=\"line\">      <span class=\"hljs-keyword\">if</span> (<span class=\"hljs-built_in\">size</span> &gt; max_size) {</span><br><span class=\"line\">        <span class=\"hljs-comment\">// Do not make batch too big</span></span><br><span class=\"line\">        <span class=\"hljs-keyword\">break</span>;</span><br><span class=\"line\">      }</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"hljs-comment\">// Append to *result</span></span><br><span class=\"line\">      <span class=\"hljs-keyword\">if</span> (result == first-&gt;batch) {</span><br><span class=\"line\">        <span class=\"hljs-comment\">// Switch to temporary batch instead of disturbing caller&apos;s batch</span></span><br><span class=\"line\">        result = tmp_batch_;</span><br><span class=\"line\">        assert(WriteBatchInternal::Count(result) == <span class=\"hljs-number\">0</span>);</span><br><span class=\"line\">        WriteBatchInternal::Append(result, first-&gt;batch);</span><br><span class=\"line\">      }</span><br><span class=\"line\">      WriteBatchInternal::Append(result, w-&gt;batch);</span><br><span class=\"line\">    }</span><br><span class=\"line\">    *last_writer = w;</span><br><span class=\"line\">  }</span><br><span class=\"line\">  <span class=\"hljs-keyword\">return</span> result;</span><br><span class=\"line\">}</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>Inside <code>DBImpl::BuildBatchGroup</code>, it will traverse the whole <code>writers_</code> from beginning to end and try to group all  those batches into one batch. For convenience to clarify, we call this procedure &#x201C;batch mergence&#x201D;</p>\n<p>However, there are some limitations in batch mergence. First, the maximum size of <code>rep_</code> in the merged batch group is <strong>1M</strong> and if the first writer is a small writer(<code>rep_.size() &lt;= 128K</code> ) it will limit the maximum size to <code>rep_.size() + 128K</code>. The idea behind this is not slowing down the small write too much. Second, it will not include a sync write into the batch handled by a non-sync write.</p>\n<p>In the batch mergence, with the help of <code>WriteBatchInternal::Append</code>, the data field of <code>rep_</code> in second batch is appended to the end of the <code>rep_</code> in first batch:</p>\n<p></p><figure class=\"highlight c++ hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">void</span> <span class=\"hljs-title\">WriteBatchInternal::Append</span><span class=\"hljs-params\">(WriteBatch* dst, <span class=\"hljs-keyword\">const</span> WriteBatch* src)</span> </span>{</span><br><span class=\"line\">  SetCount(dst, Count(dst) + Count(src));</span><br><span class=\"line\">  assert(src-&gt;rep_.<span class=\"hljs-built_in\">size</span>() &gt;= kHeader);</span><br><span class=\"line\">  dst-&gt;rep_.append(src-&gt;rep_.data() + kHeader, src-&gt;rep_.<span class=\"hljs-built_in\">size</span>() - kHeader);</span><br><span class=\"line\">}</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>After batch mergence is finished, a new batch called <code>updates</code> is generated. And the sequence number in <code>rep_</code> of <code>updates</code> is set to <code>last_sequence + 1</code>, <code>last_sequnce</code> gets adjusted to a new one(<code>last_sequnce = last_sequnce + count field rep_ of updates</code>) as well.</p>\n<p>Now, we come to the IO critical path. The WAL in leveldb will persist the contents of the <code>update</code> in an append-only way. After that success, memtable will replay the operations in the batch <code>update</code> and do corresponding behavior in memtable:</p>\n<p>Apply to WAL and insert into memtable:</p>\n<p></p><figure class=\"highlight c++ hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">{</span><br><span class=\"line\">      mutex_.Unlock();</span><br><span class=\"line\">      status = log_-&gt;AddRecord(WriteBatchInternal::Contents(updates));</span><br><span class=\"line\">      <span class=\"hljs-keyword\">bool</span> sync_error = <span class=\"hljs-literal\">false</span>;</span><br><span class=\"line\">      <span class=\"hljs-keyword\">if</span> (status.ok() &amp;&amp; options.sync) {</span><br><span class=\"line\">        status = logfile_-&gt;Sync();</span><br><span class=\"line\">        <span class=\"hljs-keyword\">if</span> (!status.ok()) {</span><br><span class=\"line\">          sync_error = <span class=\"hljs-literal\">true</span>;</span><br><span class=\"line\">        }</span><br><span class=\"line\">      }</span><br><span class=\"line\">      <span class=\"hljs-keyword\">if</span> (status.ok()) {</span><br><span class=\"line\">        status = WriteBatchInternal::InsertInto(updates, mem_);</span><br><span class=\"line\">      }</span><br><span class=\"line\">      mutex_.Lock();</span><br><span class=\"line\">      <span class=\"hljs-keyword\">if</span> (sync_error) {</span><br><span class=\"line\">        <span class=\"hljs-comment\">// The state of the log file is indeterminate: the log record we</span></span><br><span class=\"line\">        <span class=\"hljs-comment\">// just added may or may not show up when the DB is re-opened.</span></span><br><span class=\"line\">        <span class=\"hljs-comment\">// So we force the DB into a mode where all future writes fail.</span></span><br><span class=\"line\">        RecordBackgroundError(status);</span><br><span class=\"line\">      }</span><br><span class=\"line\">    }</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p><code>WriteBatch</code> Parse the whole <code>rep_</code> string to replay original operations specified by users and call handler(MemTableInserter) to handle these operations.</p>\n<p></p><figure class=\"highlight plain hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Status WriteBatch::Iterate(Handler* handler) const {</span><br><span class=\"line\">  Slice input(rep_);</span><br><span class=\"line\">  if (input.size() &lt; kHeader) {</span><br><span class=\"line\">    return Status::Corruption(&quot;malformed WriteBatch (too small)&quot;);</span><br><span class=\"line\">  }</span><br><span class=\"line\"></span><br><span class=\"line\">  input.remove_prefix(kHeader);</span><br><span class=\"line\">  Slice key, value;</span><br><span class=\"line\">  int found = 0;</span><br><span class=\"line\">  while (!input.empty()) {</span><br><span class=\"line\">    found++;</span><br><span class=\"line\">    char tag = input[0];</span><br><span class=\"line\">    input.remove_prefix(1);</span><br><span class=\"line\">    switch (tag) {</span><br><span class=\"line\">      case kTypeValue:</span><br><span class=\"line\">        if (GetLengthPrefixedSlice(&amp;input, &amp;key) &amp;&amp;</span><br><span class=\"line\">            GetLengthPrefixedSlice(&amp;input, &amp;value)) {</span><br><span class=\"line\">          handler-&gt;Put(key, value);</span><br><span class=\"line\">        } else {</span><br><span class=\"line\">          return Status::Corruption(&quot;bad WriteBatch Put&quot;);</span><br><span class=\"line\">        }</span><br><span class=\"line\">        break;</span><br><span class=\"line\">      case kTypeDeletion:</span><br><span class=\"line\">        if (GetLengthPrefixedSlice(&amp;input, &amp;key)) {</span><br><span class=\"line\">          handler-&gt;Delete(key);</span><br><span class=\"line\">        } else {</span><br><span class=\"line\">          return Status::Corruption(&quot;bad WriteBatch Delete&quot;);</span><br><span class=\"line\">        }</span><br><span class=\"line\">        break;</span><br><span class=\"line\">      default:</span><br><span class=\"line\">        return Status::Corruption(&quot;unknown WriteBatch tag&quot;);</span><br><span class=\"line\">    }</span><br><span class=\"line\">  }</span><br><span class=\"line\">  if (found != WriteBatchInternal::Count(this)) {</span><br><span class=\"line\">    return Status::Corruption(&quot;WriteBatch has wrong count&quot;);</span><br><span class=\"line\">  } else {</span><br><span class=\"line\">    return Status::OK();</span><br><span class=\"line\">  }</span><br><span class=\"line\">}</span><br></pre></td></tr></tbody></table></figure><p></p>\n<h3 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h3><p>Hoo, finally get here! This article mainly talks about how batch operation is handled in leveldb. Due to space limitation, other import parts like WAL and Memtable in leveldb cannot be presented here.</p>\n<p>Have fun<del>~</del></p>\n</body></html>","site":{"data":{}},"_categories":[{"name":"Technology","path":"categories/Technology/"}],"_tags":[{"name":"database","path":"tags/database/"},{"name":"leveldb","path":"tags/leveldb/"}],"excerpt":"<html><head></head><body><h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>As a well-known key-value database,<a href=\"https://github.com/google/leveldb\" target=\"_blank\" rel=\"noopener\"> leveldb</a> provides general key-values interfaces like Put, Get and Delete. Besides those interfaces, leveldb provides a batch operation called WriteBatch as well. A batch operation means we can group multiple operations into one and submit this one to leveldb, the atomicity guarantees that either all of those operations are applied or none of them is applied.</p>\n<p>For example, we can use batch operation in leveldb in the following way:</p>\n<p><epacse hidden>0</epacse></p>\n<p>The updates are applied in the order in which they are added to the WriteBatch. And the value of &#x201C;key&#x201D; in the above code sample will be &#x201C;v3&#x201D; after the batch is written.</p>\n<h3 id=\"Implementaion-of-WriteBatch\"><a href=\"#Implementaion-of-WriteBatch\" class=\"headerlink\" title=\"Implementaion of WriteBatch\"></a>Implementaion of WriteBatch</h3><p>Well, how does leveldb implement this simple but powerful interface? Let&#x2019;s figure it out by digging the source code step by step.</p></body></html>","more":"<h4 id=\"WriteBatch-Class\"><a href=\"#WriteBatch-Class\" class=\"headerlink\" title=\"WriteBatch Class\"></a>WriteBatch Class</h4><p>The <code>WriteBatch</code> is a class containing member function <code>WriteBatch::Put</code>, <code>WriteBatch::Delete</code>,<code>WriteBatch::Clear</code> and <code>WriteBatch::Iterate</code>. A private member variable of string type called <code>rep_</code> is also owned by it.</p>\n<p>In the constructor of <code>WriteBatch</code>, <code>rep_</code> is cleared firstly and resize to length <code>kHeader</code>:</p>\n<p><epacse hidden>1</epacse></p>\n<p>Thats because the first <code>kHeader</code> bytes in <code>rep_</code> is used to maintain meta information: 8-byte sequence number and followed by a 4-byte count.</p>\n<p>Each time when the user calls <code>WriteBatch::Put</code>, the <code>rep_</code> variable gets updated by the following behaviors :</p>\n<p><epacse hidden>2</epacse></p>\n<p>Consequently, the 4-byte count in <code>rep_</code> gets incremented, and an enum char value <code>kTypeValue</code>(means this is a put operation), the key and the value are appended into the end of <code>rep_</code>.</p>\n<p>Similarly, when the user calls <code>WriteBatch::Delete</code>, the <code>rep_</code> variable is updated in a similar way:</p>\n<p><epacse hidden>3</epacse></p>\n<p>Excepting that an enum char value <code>kTypeDeletion</code>(means this is a put operation) and the key information are appended into the <code>rep_</code>.</p>\n<p>Lets be more specific about the function <code>PutLengthPrefixedSlice</code> called by both <code>WriteBatch::Delete</code>and <code>WriteBatch::Put</code>:</p>\n<p><epacse hidden>4</epacse></p>\n<p>It appends the value size into dst and then the value itself into dst.</p>\n<p>Based on all the information provided above, we know that the <code>Put</code> and <code>Delete</code> member function of <code>WriteBatch</code> only update its member variable <code>rep_</code>. </p>\n<p>And we can also know the data layout format of <code>rep_</code>: </p>\n<p><epacse hidden>5</epacse></p>\n<img src=\"/2020/05/16/The-mechanism-behind-WriteBatch-in-leveldb/rep_format.png\" class=\"\" title=\"image of rep_ format\">\n\n\n\n<h4 id=\"DBImpl-Write\"><a href=\"#DBImpl-Write\" class=\"headerlink\" title=\"DBImpl::Write\"></a>DBImpl::Write</h4><p>After a batch operation is encapsulated, its time to call <code>DBImpl::Write</code> to apply the batch operation into the underlying layer to persist it.</p>\n<p>Inside <code>DBImpl::Write</code>, an instance of <code>Writer</code> called <code>w</code> , which represents the batch operation context, is created and added to the end of the double-ended queue <code>writers_</code>.  Because write operation is in strict order, so the write thread must wait until <code>w</code> is the first element of <code>writers_</code>.</p>\n<p><epacse hidden>6</epacse></p>\n<p>When the condition is fulfilled, leveldb will make sure there is enough room for this batch operation. Enough room mainly means memtable is prepared well to handle this write (I will write another new article to talk about that). Because <code>w</code> becomes the first element in <code>writers_</code>, the write thread will try to group other writers behind <code>w</code> into a so-called <code>BatchGroup</code> to speed up the io efficiency by the function <code>DBImpl::BuildBatchGroup</code>.</p>\n<p><epacse hidden>7</epacse></p>\n<p>Inside <code>DBImpl::BuildBatchGroup</code>, it will traverse the whole <code>writers_</code> from beginning to end and try to group all  those batches into one batch. For convenience to clarify, we call this procedure batch mergence</p>\n<p>However, there are some limitations in batch mergence. First, the maximum size of <code>rep_</code> in the merged batch group is <strong>1M</strong> and if the first writer is a small writer(<code>rep_.size() &lt;= 128K</code> ) it will limit the maximum size to <code>rep_.size() + 128K</code>. The idea behind this is not slowing down the small write too much. Second, it will not include a sync write into the batch handled by a non-sync write.</p>\n<p>In the batch mergence, with the help of <code>WriteBatchInternal::Append</code>, the data field of <code>rep_</code> in second batch is appended to the end of the <code>rep_</code> in first batch:</p>\n<p><epacse hidden>8</epacse></p>\n<p>After batch mergence is finished, a new batch called <code>updates</code> is generated. And the sequence number in <code>rep_</code> of <code>updates</code> is set to <code>last_sequence + 1</code>, <code>last_sequnce</code> gets adjusted to a new one(<code>last_sequnce = last_sequnce + count field rep_ of updates</code>) as well.</p>\n<p>Now, we come to the IO critical path. The WAL in leveldb will persist the contents of the <code>update</code> in an append-only way. After that success, memtable will replay the operations in the batch <code>update</code> and do corresponding behavior in memtable:</p>\n<p>Apply to WAL and insert into memtable:</p>\n<p><epacse hidden>9</epacse></p>\n<p><code>WriteBatch</code> Parse the whole <code>rep_</code> string to replay original operations specified by users and call handler(MemTableInserter) to handle these operations.</p>\n<p><epacse hidden>10</epacse></p>\n<h3 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h3><p>Hoo, finally get here! This article mainly talks about how batch operation is handled in leveldb. Due to space limitation, other import parts like WAL and Memtable in leveldb cannot be presented here.</p>\n<p>Have fun<del>~</del></p>"},{"title":"Redo log in InnoDB","date":"2016-07-09T09:33:15.000Z","_content":"\n## What is redo log\n\nFor a relational database, ACID is a set of properties that it must support for a transaction. That is to say, a transaction should be atomic, consistent, isolated and durable under the management of such database.\nInnoDB, the default storage engine since MySQL 5.5, use a method called **redo log** to implement the durability of a transaction. Redo log consists of redo log buffer and redo log file.\nThe redo log buffer resides in memory and is volatile while the redo log file resides in disks and is durable. Redo log records the information about a transaction. As the literal meaning of words redo log denotes, you can redo your operations after the system crashes by redo log.\n\n<!-- more -->\n\n## How does it work\n\nAs a transaction-based storage engine, InnoDB uses force log at commit mechanism to achieve durability.\nSo before a transaction is committed, all logs of that transaction must be flushed to redo log files. Even though the whole system crashes during the process of transaction commit, this transaction can be recovered by the redo log file after the system boots up again.\nThere exist a redo log memory buffer where the redo log is written to boost system performance. To ensure that redo log is written to redo log file successfully each time, the InnoDB storage engine need to call the `fsync` because `O_DIRECT` flag is not used when open redo log so that redo log is written to file system buffer firstly. Nevertheless, the time consumed by `fsync` call is up to the performance of disk. Consequently, the performance of disk determines the performance of transaction commit.[Reference1](http://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html)\n\nTaking account of the performance problem mentioned previously, the InnoDB storage engine allows user to set up the frequency of calling `fsync`. Specifically, parameter `innodb_flush_log_at_trx_commit` is used for frequency management. `innodb_flush_log_at_trx_commit` controls the balance between strict ACID compliance for commit operations, and higher performance that is possible when commit-related I/O operations are rearranged and done in batches. You can achieve better performance by changing the default value, but then you can lose up to a second of transactions in a crash.\n\n- The default value of 1 is required for full ACID compliance. With this value, the contents of the InnoDB log buffer are written out to the log file at each transaction commit and the log file is flushed to disk.\n- With a value of 0, the contents of the InnoDB log buffer are written to the log file approximately once per second and the log file is flushed to disk. No writes from the log buffer to the log file are performed at transaction commit. Once-per-second flushing is not 100% guaranteed to happen every second, due to process scheduling issues. Because the flush to disk operation only occurs approximately once per second, you can lose up to a second of transactions with any mysqld process crash\n- With a value of 2, the contents of the InnoDB log buffer are written to the log file after each transaction commit and the log file is flushed to disk approximately once per second. Once-per-second flushing is not 100% guaranteed to happen every second, due to process scheduling issues. Because the flush to disk operation only occurs approximately once per second, you can lose up to a second of transactions in an operating system crash or a power outage.[Reference2](https://book.douban.com/subject/25872763/)\n\n## The format of redo log file\n\nIn InnoDB storage engine, the redo logs are stored in 512-byte format. This means that redo log cache, redo log files are both kept in blocks and each bock has a size of 512 bytes. Besides the log itself in block, log block header and lock block tailer are also stored in each block. In a redo log block, 12 bytes and 8 bytes are occupied by redo log header and redo log tailer respectively(So real information stored in each block is 492 bytes).\n{% asset_img redo-log-block.jpg image of redo log file %}\n","source":"_posts/Redo-log-in-InnoDB.md","raw":"---\ntitle: Redo log in InnoDB\ndate: 2016-07-09 17:33:15\ntags: [database, mysql]\ncategories: Technology\n---\n\n## What is redo log\n\nFor a relational database, ACID is a set of properties that it must support for a transaction. That is to say, a transaction should be atomic, consistent, isolated and durable under the management of such database.\nInnoDB, the default storage engine since MySQL 5.5, use a method called **redo log** to implement the durability of a transaction. Redo log consists of redo log buffer and redo log file.\nThe redo log buffer resides in memory and is volatile while the redo log file resides in disks and is durable. Redo log records the information about a transaction. As the literal meaning of words redo log denotes, you can redo your operations after the system crashes by redo log.\n\n<!-- more -->\n\n## How does it work\n\nAs a transaction-based storage engine, InnoDB uses force log at commit mechanism to achieve durability.\nSo before a transaction is committed, all logs of that transaction must be flushed to redo log files. Even though the whole system crashes during the process of transaction commit, this transaction can be recovered by the redo log file after the system boots up again.\nThere exist a redo log memory buffer where the redo log is written to boost system performance. To ensure that redo log is written to redo log file successfully each time, the InnoDB storage engine need to call the `fsync` because `O_DIRECT` flag is not used when open redo log so that redo log is written to file system buffer firstly. Nevertheless, the time consumed by `fsync` call is up to the performance of disk. Consequently, the performance of disk determines the performance of transaction commit.[Reference1](http://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html)\n\nTaking account of the performance problem mentioned previously, the InnoDB storage engine allows user to set up the frequency of calling `fsync`. Specifically, parameter `innodb_flush_log_at_trx_commit` is used for frequency management. `innodb_flush_log_at_trx_commit` controls the balance between strict ACID compliance for commit operations, and higher performance that is possible when commit-related I/O operations are rearranged and done in batches. You can achieve better performance by changing the default value, but then you can lose up to a second of transactions in a crash.\n\n- The default value of 1 is required for full ACID compliance. With this value, the contents of the InnoDB log buffer are written out to the log file at each transaction commit and the log file is flushed to disk.\n- With a value of 0, the contents of the InnoDB log buffer are written to the log file approximately once per second and the log file is flushed to disk. No writes from the log buffer to the log file are performed at transaction commit. Once-per-second flushing is not 100% guaranteed to happen every second, due to process scheduling issues. Because the flush to disk operation only occurs approximately once per second, you can lose up to a second of transactions with any mysqld process crash\n- With a value of 2, the contents of the InnoDB log buffer are written to the log file after each transaction commit and the log file is flushed to disk approximately once per second. Once-per-second flushing is not 100% guaranteed to happen every second, due to process scheduling issues. Because the flush to disk operation only occurs approximately once per second, you can lose up to a second of transactions in an operating system crash or a power outage.[Reference2](https://book.douban.com/subject/25872763/)\n\n## The format of redo log file\n\nIn InnoDB storage engine, the redo logs are stored in 512-byte format. This means that redo log cache, redo log files are both kept in blocks and each bock has a size of 512 bytes. Besides the log itself in block, log block header and lock block tailer are also stored in each block. In a redo log block, 12 bytes and 8 bytes are occupied by redo log header and redo log tailer respectively(So real information stored in each block is 492 bytes).\n{% asset_img redo-log-block.jpg image of redo log file %}\n","slug":"Redo-log-in-InnoDB","published":1,"updated":"2020-05-16T14:15:14.110Z","_id":"cka9nungs0000acpk3popcxt3","comments":1,"layout":"post","photos":[],"link":"","content":"<html><head></head><body><h2 id=\"What-is-redo-log\"><a href=\"#What-is-redo-log\" class=\"headerlink\" title=\"What is redo log\"></a>What is redo log</h2><p>For a relational database, ACID is a set of properties that it must support for a transaction. That is to say, a transaction should be atomic, consistent, isolated and durable under the management of such database.<br>InnoDB, the default storage engine since MySQL 5.5, use a method called <strong>redo log</strong> to implement the durability of a transaction. Redo log consists of redo log buffer and redo log file.<br>The redo log buffer resides in memory and is volatile while the redo log file resides in disks and is durable. Redo log records the information about a transaction. As the literal meaning of words redo log denotes, you can redo your operations after the system crashes by redo log.</p>\n<a id=\"more\"></a>\n\n<h2 id=\"How-does-it-work\"><a href=\"#How-does-it-work\" class=\"headerlink\" title=\"How does it work\"></a>How does it work</h2><p>As a transaction-based storage engine, InnoDB uses &#x201C;force log at commit&#x201D; mechanism to achieve durability.<br>So before a transaction is committed, all logs of that transaction must be flushed to redo log files. Even though the whole system crashes during the process of transaction commit, this transaction can be recovered by the redo log file after the system boots up again.<br>There exist a redo log memory buffer where the redo log is written to boost system performance. To ensure that redo log is written to redo log file successfully each time, the InnoDB storage engine need to call the <code>fsync</code> because <code>O_DIRECT</code> flag is not used when open redo log so that redo log is written to file system buffer firstly. Nevertheless, the time consumed by <code>fsync</code> call is up to the performance of disk. Consequently, the performance of disk determines the performance of transaction commit.<a href=\"http://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html\" target=\"_blank\" rel=\"noopener\">Reference1</a></p>\n<p>Taking account of the performance problem mentioned previously, the InnoDB storage engine allows user to set up the frequency of calling <code>fsync</code>. Specifically, parameter <code>innodb_flush_log_at_trx_commit</code> is used for frequency management. <code>innodb_flush_log_at_trx_commit</code> controls the balance between strict ACID compliance for commit operations, and higher performance that is possible when commit-related I/O operations are rearranged and done in batches. You can achieve better performance by changing the default value, but then you can lose up to a second of transactions in a crash.</p>\n<ul>\n<li>The default value of 1 is required for full ACID compliance. With this value, the contents of the InnoDB log buffer are written out to the log file at each transaction commit and the log file is flushed to disk.</li>\n<li>With a value of 0, the contents of the InnoDB log buffer are written to the log file approximately once per second and the log file is flushed to disk. No writes from the log buffer to the log file are performed at transaction commit. Once-per-second flushing is not 100% guaranteed to happen every second, due to process scheduling issues. Because the flush to disk operation only occurs approximately once per second, you can lose up to a second of transactions with any mysqld process crash</li>\n<li>With a value of 2, the contents of the InnoDB log buffer are written to the log file after each transaction commit and the log file is flushed to disk approximately once per second. Once-per-second flushing is not 100% guaranteed to happen every second, due to process scheduling issues. Because the flush to disk operation only occurs approximately once per second, you can lose up to a second of transactions in an operating system crash or a power outage.<a href=\"https://book.douban.com/subject/25872763/\" target=\"_blank\" rel=\"noopener\">Reference2</a></li>\n</ul>\n<h2 id=\"The-format-of-redo-log-file\"><a href=\"#The-format-of-redo-log-file\" class=\"headerlink\" title=\"The format of redo log file\"></a>The format of redo log file</h2><p>In InnoDB storage engine, the redo logs are stored in 512-byte format. This means that redo log cache, redo log files are both kept in blocks and each bock has a size of 512 bytes. Besides the log itself in block, log block header and lock block tailer are also stored in each block. In a redo log block, 12 bytes and 8 bytes are occupied by redo log header and redo log tailer respectively(So real information stored in each block is 492 bytes).</p>\n<img src=\"/2016/07/09/Redo-log-in-InnoDB/redo-log-block.jpg\" class title=\"image of redo log file\">\n</body></html>","site":{"data":{}},"_categories":[{"name":"Technology","path":"categories/Technology/"}],"_tags":[{"name":"database","path":"tags/database/"},{"name":"mysql","path":"tags/mysql/"}],"excerpt":"<html><head></head><body><h2 id=\"What-is-redo-log\"><a href=\"#What-is-redo-log\" class=\"headerlink\" title=\"What is redo log\"></a>What is redo log</h2><p>For a relational database, ACID is a set of properties that it must support for a transaction. That is to say, a transaction should be atomic, consistent, isolated and durable under the management of such database.<br>InnoDB, the default storage engine since MySQL 5.5, use a method called <strong>redo log</strong> to implement the durability of a transaction. Redo log consists of redo log buffer and redo log file.<br>The redo log buffer resides in memory and is volatile while the redo log file resides in disks and is durable. Redo log records the information about a transaction. As the literal meaning of words redo log denotes, you can redo your operations after the system crashes by redo log.</p></body></html>","more":"<h2 id=\"How-does-it-work\"><a href=\"#How-does-it-work\" class=\"headerlink\" title=\"How does it work\"></a>How does it work</h2><p>As a transaction-based storage engine, InnoDB uses force log at commit mechanism to achieve durability.<br>So before a transaction is committed, all logs of that transaction must be flushed to redo log files. Even though the whole system crashes during the process of transaction commit, this transaction can be recovered by the redo log file after the system boots up again.<br>There exist a redo log memory buffer where the redo log is written to boost system performance. To ensure that redo log is written to redo log file successfully each time, the InnoDB storage engine need to call the <code>fsync</code> because <code>O_DIRECT</code> flag is not used when open redo log so that redo log is written to file system buffer firstly. Nevertheless, the time consumed by <code>fsync</code> call is up to the performance of disk. Consequently, the performance of disk determines the performance of transaction commit.<a href=\"http://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html\" target=\"_blank\" rel=\"noopener\">Reference1</a></p>\n<p>Taking account of the performance problem mentioned previously, the InnoDB storage engine allows user to set up the frequency of calling <code>fsync</code>. Specifically, parameter <code>innodb_flush_log_at_trx_commit</code> is used for frequency management. <code>innodb_flush_log_at_trx_commit</code> controls the balance between strict ACID compliance for commit operations, and higher performance that is possible when commit-related I/O operations are rearranged and done in batches. You can achieve better performance by changing the default value, but then you can lose up to a second of transactions in a crash.</p>\n<ul>\n<li>The default value of 1 is required for full ACID compliance. With this value, the contents of the InnoDB log buffer are written out to the log file at each transaction commit and the log file is flushed to disk.</li>\n<li>With a value of 0, the contents of the InnoDB log buffer are written to the log file approximately once per second and the log file is flushed to disk. No writes from the log buffer to the log file are performed at transaction commit. Once-per-second flushing is not 100% guaranteed to happen every second, due to process scheduling issues. Because the flush to disk operation only occurs approximately once per second, you can lose up to a second of transactions with any mysqld process crash</li>\n<li>With a value of 2, the contents of the InnoDB log buffer are written to the log file after each transaction commit and the log file is flushed to disk approximately once per second. Once-per-second flushing is not 100% guaranteed to happen every second, due to process scheduling issues. Because the flush to disk operation only occurs approximately once per second, you can lose up to a second of transactions in an operating system crash or a power outage.<a href=\"https://book.douban.com/subject/25872763/\" target=\"_blank\" rel=\"noopener\">Reference2</a></li>\n</ul>\n<h2 id=\"The-format-of-redo-log-file\"><a href=\"#The-format-of-redo-log-file\" class=\"headerlink\" title=\"The format of redo log file\"></a>The format of redo log file</h2><p>In InnoDB storage engine, the redo logs are stored in 512-byte format. This means that redo log cache, redo log files are both kept in blocks and each bock has a size of 512 bytes. Besides the log itself in block, log block header and lock block tailer are also stored in each block. In a redo log block, 12 bytes and 8 bytes are occupied by redo log header and redo log tailer respectively(So real information stored in each block is 492 bytes).</p>\n<img src=\"/2016/07/09/Redo-log-in-InnoDB/redo-log-block.jpg\" class=\"\" title=\"image of redo log file\">"},{"title":"Compile your own linux kernel","date":"2016-03-23T06:33:15.000Z","_content":"\nAs we know, linux is one of the greatest open source projects in the world and serves millions of enterprises. An open source project means that you can define your own features catering to different application scenarios. All big Internet firms such as Google, Facebook and Aamazon recompile the linux kernel so that features can be added to or removed from the official kernel release version.\nCompiling the kernel for linux kernel developers is also unavoidable. In the rest part of this post, attention will focus on tutorials on compiling a linux kernel.\n\n\n\n### 1. Getting the kernel source of official release\n\nNothing comes from nothing. So the first thing before compiling a customized kernel is getting source code.\nI strongly recommend using `Git` to download and manage the linux kernel source:\n\n```\ngit clone source_git_link\n```\n\n<!-- more -->\n\n\nSurely, you can also download the compressed package of source code and then uncompress it.\nGo to the source code root directory, there exists a number of directories under it.\n\n{% asset_img kernel-directory.png kernel src file dir %}\n\nThe following table[1] illustrates explanation about these directories.\n\n|   Directory   |                  Description                  |\n| :-----------: | :-------------------------------------------: |\n|     arch      |         Architecture-specific source          |\n|     block     |                Block I/O layer                |\n|     certs     |             SSL/TLS certification             |\n|    crypto     |                  Crypto API                   |\n| Documentation |          Kernel source documentation          |\n|    drivers    |                drivers Device                 |\n|   firmware    | Device firmware needed to use certain drivers |\n|      fs       |    The VFS and the individual filesystems     |\n|    include    |                Kernel headers                 |\n|     init      |        Kernel boot and initialization         |\n|      ipc      |        Interprocess communication code        |\n|    kernel     |    Core subsystems, such as the scheduler     |\n|      lib      |                Helper routines                |\n|      mm       |    Memory management subsystem and the VM     |\n|      net      |             Networking subsystem              |\n|    samples    |          Sample, demonstrative code           |\n|    scripts    |       Scripts used to build the kernel        |\n|   security    |             Linux Security Module             |\n|     sound     |                Sound subsystem                |\n|      usr      |   Early user-space code (called initramfs)    |\n|     tools     |      Tools helpful for developing Linux       |\n|     virt      |         Virtualization infrastructure         |\n\n### 2. Building the kernel source code\n\nAfter the first step, you come here. Now what you should do is configuring the kernel before compiling. As mentioned previously, it is possible to compile support into your kernel for only the specific features and drivers you want. Configuring the kernel is a required process before building it. By default, the kernel of official release version provides myriad features and supports a varied basket of hardware.\n\n#### (1). Configuration\n\nwhen you change your current working directory to the root directory of linux kernel source code, you will find there is a file named **.config**. Using command such as `cat .config | more` you can take a glimpse of its content.\n\n```\ncat .config | more\n```\n\n{% asset_img kernel-configuration.png kernel config file %}\n\nAs shown in the picture, kernel configuration is controlled by configuration options, which are prefixed by **CONFIG** in the form **CONFIG_FEATURE**. That is to say, asynchronous IO is controlled by the configuration option **CONFIG_AIO**. This option enables POSIX asynchronous I/O which may by used by some high performance threaded applications[Reference][2]. When this option is set, AIO is enabled; if unset, AIO is disabled.\nConfiguration options that control the build process are either Booleans or tristates. A Boolean option is either yes or no. Kernel features, such as CONFIG_PREEMPT, are usually Booleans. A tristate option is one of yes, no, or module.The module setting represents a configuration option that is set but is to be compiled as a module (that is, a separate dynamically loadable object). In the case of tristates, a yes option explicitly means to compile the code into the main kernel image and not as a module. Drivers are usually represented by tristates[Reference][3].\nConfiguration options can also be strings or integers.These options do not control the build process but instead specify values that kernel source can access as a preprocessor macro. For example, a configuration option can specify the size of a statically allocated array[Reference][4].\nKernel provides multiple choices for you to facilitate configurations. A straightfoward way is using a graphical interactive interface: `make menuconfig`.\n\n```\nmake menuconfig\n```\n\nAfter typing this command, a graphical interactive interface will appears in your screen like this:\n\n{% asset_img kernel-menuconfig.png kernel menuconfig file %}\n\nAnd you can move the cursor to different options to set them. Because of space, how to configure these options correctly can not be presented. For more detailed knowledge, you can find them in the linux orgnization.\n\n#### (2). Compile and build\n\nNow, it is time to get into the marrow of the second part: Compile && Build. Please make sure that command `make` and `gcc` is installed on your machine firstly.\nJust type `make` and all related source code about kernel will be compiled and built, the default Makefile rule will handle everything.\n\n```\nmake\n```\n\nIn general, one flaw about the `make` method is that this action spawns only a single job because Makefiles all too often have incorrect dependency information. With incorrect dependencies, multiple jobs can step on each others toes, resulting in errors in the build process. However, The kernels Makefile have correct dependency information, so spawning multiple jobs does not result in failures. To build the kernel with multiple make jobs, use\n\n```\nmake -jn\n```\n\nThe n here is number of jobs to spawn. Usual practice is to spawn one or two jobs per processor. If you have 16 processors in you machine, then you might do\n\n```\nmake -j32\n```\n\nThe resulting kernel file is arch/x86/boot/bzImage (in x86 platform).\n\n### 3. Installation\n\nAfter the kernel is built, you can install it. It is possible that the kernel you install cannot boot successfully, so in case of that, you should have at least two kernel installed on you machine so that you can choose the another one to boot.\n\n#### (1). Install modules\n\nInstalling modules, thankfully, is automated and architecture-independent. As root, simply run\n\n```\nmake modules_install\n```\n\nAfter this, you will find a module file under **/lib/modules/a.b.c** where a.b.c is the kernel version.\n\n#### (2). Install kernel\n\nAs root user, simply run\n\n```\nmake install\n```\n\nAfter this, a new kernel file and a new boot image will appear in the **/boot** directory.\n\n#### (3). Set booting order\n\nIf you execute all the steps normally, new content about the new installed kernel has been added to **/boot/grub/grub.conf** file. And you can edit the **grub.conf** file to choose to use which kernel when booting.\n\n{% asset_img kernel-grub.png kernel grub file %}\n\nReboot the machine, and then you will find the new installed kernel in the booting screen.\n\n{% asset_img kernel-booting.png booting file %}\n\n[1]: Love, Robert Love. (2003). Linux Kernel Development, 3, 40-42\n\n[2]: http://cateee.net/lkddb/web-lkddb/AIO.html\n\n[3]: Love, Robert Love. (2003). Linux Kernel Development, 3, 42-43\n[4]: Love, Robert Love. (2003). Linux Kernel Development, 3, 43-45\n","source":"_posts/Compile-your-own-linux-kernel.md","raw":"---\ntitle: Compile your own linux kernel\ndate: 2016-03-23 14:33:15\ntags: [kernel, linux]\ncategories: Technology\n---\n\nAs we know, linux is one of the greatest open source projects in the world and serves millions of enterprises. An open source project means that you can define your own features catering to different application scenarios. All big Internet firms such as Google, Facebook and Aamazon recompile the linux kernel so that features can be added to or removed from the official kernel release version.\nCompiling the kernel for linux kernel developers is also unavoidable. In the rest part of this post, attention will focus on tutorials on compiling a linux kernel.\n\n\n\n### 1. Getting the kernel source of official release\n\nNothing comes from nothing. So the first thing before compiling a customized kernel is getting source code.\nI strongly recommend using `Git` to download and manage the linux kernel source:\n\n```\ngit clone source_git_link\n```\n\n<!-- more -->\n\n\nSurely, you can also download the compressed package of source code and then uncompress it.\nGo to the source code root directory, there exists a number of directories under it.\n\n{% asset_img kernel-directory.png kernel src file dir %}\n\nThe following table[1] illustrates explanation about these directories.\n\n|   Directory   |                  Description                  |\n| :-----------: | :-------------------------------------------: |\n|     arch      |         Architecture-specific source          |\n|     block     |                Block I/O layer                |\n|     certs     |             SSL/TLS certification             |\n|    crypto     |                  Crypto API                   |\n| Documentation |          Kernel source documentation          |\n|    drivers    |                drivers Device                 |\n|   firmware    | Device firmware needed to use certain drivers |\n|      fs       |    The VFS and the individual filesystems     |\n|    include    |                Kernel headers                 |\n|     init      |        Kernel boot and initialization         |\n|      ipc      |        Interprocess communication code        |\n|    kernel     |    Core subsystems, such as the scheduler     |\n|      lib      |                Helper routines                |\n|      mm       |    Memory management subsystem and the VM     |\n|      net      |             Networking subsystem              |\n|    samples    |          Sample, demonstrative code           |\n|    scripts    |       Scripts used to build the kernel        |\n|   security    |             Linux Security Module             |\n|     sound     |                Sound subsystem                |\n|      usr      |   Early user-space code (called initramfs)    |\n|     tools     |      Tools helpful for developing Linux       |\n|     virt      |         Virtualization infrastructure         |\n\n### 2. Building the kernel source code\n\nAfter the first step, you come here. Now what you should do is configuring the kernel before compiling. As mentioned previously, it is possible to compile support into your kernel for only the specific features and drivers you want. Configuring the kernel is a required process before building it. By default, the kernel of official release version provides myriad features and supports a varied basket of hardware.\n\n#### (1). Configuration\n\nwhen you change your current working directory to the root directory of linux kernel source code, you will find there is a file named **.config**. Using command such as `cat .config | more` you can take a glimpse of its content.\n\n```\ncat .config | more\n```\n\n{% asset_img kernel-configuration.png kernel config file %}\n\nAs shown in the picture, kernel configuration is controlled by configuration options, which are prefixed by **CONFIG** in the form **CONFIG_FEATURE**. That is to say, asynchronous IO is controlled by the configuration option **CONFIG_AIO**. This option enables POSIX asynchronous I/O which may by used by some high performance threaded applications[Reference][2]. When this option is set, AIO is enabled; if unset, AIO is disabled.\nConfiguration options that control the build process are either Booleans or tristates. A Boolean option is either yes or no. Kernel features, such as CONFIG_PREEMPT, are usually Booleans. A tristate option is one of yes, no, or module.The module setting represents a configuration option that is set but is to be compiled as a module (that is, a separate dynamically loadable object). In the case of tristates, a yes option explicitly means to compile the code into the main kernel image and not as a module. Drivers are usually represented by tristates[Reference][3].\nConfiguration options can also be strings or integers.These options do not control the build process but instead specify values that kernel source can access as a preprocessor macro. For example, a configuration option can specify the size of a statically allocated array[Reference][4].\nKernel provides multiple choices for you to facilitate configurations. A straightfoward way is using a graphical interactive interface: `make menuconfig`.\n\n```\nmake menuconfig\n```\n\nAfter typing this command, a graphical interactive interface will appears in your screen like this:\n\n{% asset_img kernel-menuconfig.png kernel menuconfig file %}\n\nAnd you can move the cursor to different options to set them. Because of space, how to configure these options correctly can not be presented. For more detailed knowledge, you can find them in the linux orgnization.\n\n#### (2). Compile and build\n\nNow, it is time to get into the marrow of the second part: Compile && Build. Please make sure that command `make` and `gcc` is installed on your machine firstly.\nJust type `make` and all related source code about kernel will be compiled and built, the default Makefile rule will handle everything.\n\n```\nmake\n```\n\nIn general, one flaw about the `make` method is that this action spawns only a single job because Makefiles all too often have incorrect dependency information. With incorrect dependencies, multiple jobs can step on each others toes, resulting in errors in the build process. However, The kernels Makefile have correct dependency information, so spawning multiple jobs does not result in failures. To build the kernel with multiple make jobs, use\n\n```\nmake -jn\n```\n\nThe n here is number of jobs to spawn. Usual practice is to spawn one or two jobs per processor. If you have 16 processors in you machine, then you might do\n\n```\nmake -j32\n```\n\nThe resulting kernel file is arch/x86/boot/bzImage (in x86 platform).\n\n### 3. Installation\n\nAfter the kernel is built, you can install it. It is possible that the kernel you install cannot boot successfully, so in case of that, you should have at least two kernel installed on you machine so that you can choose the another one to boot.\n\n#### (1). Install modules\n\nInstalling modules, thankfully, is automated and architecture-independent. As root, simply run\n\n```\nmake modules_install\n```\n\nAfter this, you will find a module file under **/lib/modules/a.b.c** where a.b.c is the kernel version.\n\n#### (2). Install kernel\n\nAs root user, simply run\n\n```\nmake install\n```\n\nAfter this, a new kernel file and a new boot image will appear in the **/boot** directory.\n\n#### (3). Set booting order\n\nIf you execute all the steps normally, new content about the new installed kernel has been added to **/boot/grub/grub.conf** file. And you can edit the **grub.conf** file to choose to use which kernel when booting.\n\n{% asset_img kernel-grub.png kernel grub file %}\n\nReboot the machine, and then you will find the new installed kernel in the booting screen.\n\n{% asset_img kernel-booting.png booting file %}\n\n[1]: Love, Robert Love. (2003). Linux Kernel Development, 3, 40-42\n\n[2]: http://cateee.net/lkddb/web-lkddb/AIO.html\n\n[3]: Love, Robert Love. (2003). Linux Kernel Development, 3, 42-43\n[4]: Love, Robert Love. (2003). Linux Kernel Development, 3, 43-45\n","slug":"Compile-your-own-linux-kernel","published":1,"updated":"2020-05-16T14:15:50.082Z","_id":"cka9oizps0000r6pkf8gta8e5","comments":1,"layout":"post","photos":[],"link":"","content":"<html><head></head><body><p>As we know, linux is one of the greatest open source projects in the world and serves millions of enterprises. An open source project means that you can define your own features catering to different application scenarios. All big Internet firms such as Google, Facebook and Aamazon recompile the linux kernel so that features can be added to or removed from the official kernel release version.<br>Compiling the kernel for linux kernel developers is also unavoidable. In the rest part of this post, attention will focus on tutorials on compiling a linux kernel.</p>\n<h3 id=\"1-Getting-the-kernel-source-of-official-release\"><a href=\"#1-Getting-the-kernel-source-of-official-release\" class=\"headerlink\" title=\"1. Getting the kernel source of official release\"></a>1. Getting the kernel source of official release</h3><p>Nothing comes from nothing. So the first thing before compiling a customized kernel is getting source code.<br>I strongly recommend using <code>Git</code> to download and manage the linux kernel source:</p>\n<p></p><figure class=\"highlight plain hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone source_git_link</span><br></pre></td></tr></tbody></table></figure><p></p>\n<a id=\"more\"></a>\n\n\n<p>Surely, you can also download the compressed package of source code and then uncompress it.<br>Go to the source code root directory, there exists a number of directories under it.</p>\n<img src=\"/2016/03/23/Compile-your-own-linux-kernel/kernel-directory.png\" class title=\"kernel src file dir\">\n\n<p>The following table[1] illustrates explanation about these directories.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Directory</th>\n<th align=\"center\">Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">arch</td>\n<td align=\"center\">Architecture-specific source</td>\n</tr>\n<tr>\n<td align=\"center\">block</td>\n<td align=\"center\">Block I/O layer</td>\n</tr>\n<tr>\n<td align=\"center\">certs</td>\n<td align=\"center\">SSL/TLS certification</td>\n</tr>\n<tr>\n<td align=\"center\">crypto</td>\n<td align=\"center\">Crypto API</td>\n</tr>\n<tr>\n<td align=\"center\">Documentation</td>\n<td align=\"center\">Kernel source documentation</td>\n</tr>\n<tr>\n<td align=\"center\">drivers</td>\n<td align=\"center\">drivers Device</td>\n</tr>\n<tr>\n<td align=\"center\">firmware</td>\n<td align=\"center\">Device firmware needed to use certain drivers</td>\n</tr>\n<tr>\n<td align=\"center\">fs</td>\n<td align=\"center\">The VFS and the individual filesystems</td>\n</tr>\n<tr>\n<td align=\"center\">include</td>\n<td align=\"center\">Kernel headers</td>\n</tr>\n<tr>\n<td align=\"center\">init</td>\n<td align=\"center\">Kernel boot and initialization</td>\n</tr>\n<tr>\n<td align=\"center\">ipc</td>\n<td align=\"center\">Interprocess communication code</td>\n</tr>\n<tr>\n<td align=\"center\">kernel</td>\n<td align=\"center\">Core subsystems, such as the scheduler</td>\n</tr>\n<tr>\n<td align=\"center\">lib</td>\n<td align=\"center\">Helper routines</td>\n</tr>\n<tr>\n<td align=\"center\">mm</td>\n<td align=\"center\">Memory management subsystem and the VM</td>\n</tr>\n<tr>\n<td align=\"center\">net</td>\n<td align=\"center\">Networking subsystem</td>\n</tr>\n<tr>\n<td align=\"center\">samples</td>\n<td align=\"center\">Sample, demonstrative code</td>\n</tr>\n<tr>\n<td align=\"center\">scripts</td>\n<td align=\"center\">Scripts used to build the kernel</td>\n</tr>\n<tr>\n<td align=\"center\">security</td>\n<td align=\"center\">Linux Security Module</td>\n</tr>\n<tr>\n<td align=\"center\">sound</td>\n<td align=\"center\">Sound subsystem</td>\n</tr>\n<tr>\n<td align=\"center\">usr</td>\n<td align=\"center\">Early user-space code (called initramfs)</td>\n</tr>\n<tr>\n<td align=\"center\">tools</td>\n<td align=\"center\">Tools helpful for developing Linux</td>\n</tr>\n<tr>\n<td align=\"center\">virt</td>\n<td align=\"center\">Virtualization infrastructure</td>\n</tr>\n</tbody></table>\n<h3 id=\"2-Building-the-kernel-source-code\"><a href=\"#2-Building-the-kernel-source-code\" class=\"headerlink\" title=\"2. Building the kernel source code\"></a>2. Building the kernel source code</h3><p>After the first step, you come here. Now what you should do is configuring the kernel before compiling. As mentioned previously, it is possible to compile support into your kernel for only the specific features and drivers you want. Configuring the kernel is a required process before building it. By default, the kernel of official release version provides myriad features and supports a varied basket of hardware.</p>\n<h4 id=\"1-Configuration\"><a href=\"#1-Configuration\" class=\"headerlink\" title=\"(1). Configuration\"></a>(1). Configuration</h4><p>when you change your current working directory to the root directory of linux kernel source code, you will find there is a file named <strong>.config</strong>. Using command such as <code>cat .config | more</code> you can take a glimpse of its content.</p>\n<p></p><figure class=\"highlight plain hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat .config | more</span><br></pre></td></tr></tbody></table></figure><p></p>\n<img src=\"/2016/03/23/Compile-your-own-linux-kernel/kernel-configuration.png\" class title=\"kernel config file\">\n\n<p>As shown in the picture, kernel configuration is controlled by configuration options, which are prefixed by <strong>CONFIG</strong> in the form <strong>CONFIG_FEATURE</strong>. That is to say, asynchronous IO is controlled by the configuration option <strong>CONFIG_AIO</strong>. This option enables POSIX asynchronous I/O which may by used by some high performance threaded applications<a href=\"http://cateee.net/lkddb/web-lkddb/AIO.html\" target=\"_blank\" rel=\"noopener\">Reference</a>. When this option is set, AIO is enabled; if unset, AIO is disabled.<br>Configuration options that control the build process are either Booleans or tristates. A Boolean option is either yes or no. Kernel features, such as CONFIG_PREEMPT, are usually Booleans. A tristate option is one of yes, no, or module.The module setting represents a configuration option that is set but is to be compiled as a module (that is, a separate dynamically loadable object). In the case of tristates, a yes option explicitly means to compile the code into the main kernel image and not as a module. Drivers are usually represented by tristates[Reference][3].<br>Configuration options can also be strings or integers.These options do not control the build process but instead specify values that kernel source can access as a preprocessor macro. For example, a configuration option can specify the size of a statically allocated array[Reference][4].<br>Kernel provides multiple choices for you to facilitate configurations. A straightfoward way is using a graphical interactive interface: <code>make menuconfig</code>.</p>\n<p></p><figure class=\"highlight plain hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">make menuconfig</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>After typing this command, a graphical interactive interface will appears in your screen like this:</p>\n<img src=\"/2016/03/23/Compile-your-own-linux-kernel/kernel-menuconfig.png\" class title=\"kernel menuconfig file\">\n\n<p>And you can move the cursor to different options to set them. Because of space, how to configure these options correctly can not be presented. For more detailed knowledge, you can find them in the linux orgnization.</p>\n<h4 id=\"2-Compile-and-build\"><a href=\"#2-Compile-and-build\" class=\"headerlink\" title=\"(2). Compile and build\"></a>(2). Compile and build</h4><p>Now, it is time to get into the marrow of the second part: Compile &amp;&amp; Build. Please make sure that command <code>make</code> and <code>gcc</code> is installed on your machine firstly.<br>Just type <code>make</code> and all related source code about kernel will be compiled and built, the default Makefile rule will handle everything.</p>\n<p></p><figure class=\"highlight plain hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">make</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>In general, one flaw about the <code>make</code> method is that this action spawns only a single job because Makefiles all too often have incorrect dependency information. With incorrect dependencies, multiple jobs can step on each other&#x2019;s toes, resulting in errors in the build process. However, The kernel&#x2019;s Makefile have correct dependency information, so spawning multiple jobs does not result in failures. To build the kernel with multiple make jobs, use</p>\n<p></p><figure class=\"highlight plain hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">make -jn</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>The n here is number of jobs to spawn. Usual practice is to spawn one or two jobs per processor. If you have 16 processors in you machine, then you might do</p>\n<p></p><figure class=\"highlight plain hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">make -j32</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>The resulting kernel file is &#x201C;arch/x86/boot/bzImage&#x201D; (in x86 platform).</p>\n<h3 id=\"3-Installation\"><a href=\"#3-Installation\" class=\"headerlink\" title=\"3. Installation\"></a>3. Installation</h3><p>After the kernel is built, you can install it. It is possible that the kernel you install cannot boot successfully, so in case of that, you should have at least two kernel installed on you machine so that you can choose the another one to boot.</p>\n<h4 id=\"1-Install-modules\"><a href=\"#1-Install-modules\" class=\"headerlink\" title=\"(1). Install modules\"></a>(1). Install modules</h4><p>Installing modules, thankfully, is automated and architecture-independent. As root, simply run</p>\n<p></p><figure class=\"highlight plain hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">make modules_install</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>After this, you will find a module file under <strong>/lib/modules/a.b.c</strong> where a.b.c is the kernel version.</p>\n<h4 id=\"2-Install-kernel\"><a href=\"#2-Install-kernel\" class=\"headerlink\" title=\"(2). Install kernel\"></a>(2). Install kernel</h4><p>As root user, simply run</p>\n<p></p><figure class=\"highlight plain hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">make install</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>After this, a new kernel file and a new boot image will appear in the <strong>/boot</strong> directory.</p>\n<h4 id=\"3-Set-booting-order\"><a href=\"#3-Set-booting-order\" class=\"headerlink\" title=\"(3). Set booting order\"></a>(3). Set booting order</h4><p>If you execute all the steps normally, new content about the new installed kernel has been added to <strong>/boot/grub/grub.conf</strong> file. And you can edit the <strong>grub.conf</strong> file to choose to use which kernel when booting.</p>\n<img src=\"/2016/03/23/Compile-your-own-linux-kernel/kernel-grub.png\" class title=\"kernel grub file\">\n\n<p>Reboot the machine, and then you will find the new installed kernel in the booting screen.</p>\n<img src=\"/2016/03/23/Compile-your-own-linux-kernel/kernel-booting.png\" class title=\"booting file\">\n\n<p>[1]: Love, Robert Love. (2003). Linux Kernel Development, 3, 40-42</p>\n<p>[3]: Love, Robert Love. (2003). Linux Kernel Development, 3, 42-43<br>[4]: Love, Robert Love. (2003). Linux Kernel Development, 3, 43-45</p>\n</body></html>","site":{"data":{}},"_categories":[{"name":"Technology","path":"categories/Technology/"}],"_tags":[{"name":"kernel","path":"tags/kernel/"},{"name":"linux","path":"tags/linux/"}],"excerpt":"<html><head></head><body><p>As we know, linux is one of the greatest open source projects in the world and serves millions of enterprises. An open source project means that you can define your own features catering to different application scenarios. All big Internet firms such as Google, Facebook and Aamazon recompile the linux kernel so that features can be added to or removed from the official kernel release version.<br>Compiling the kernel for linux kernel developers is also unavoidable. In the rest part of this post, attention will focus on tutorials on compiling a linux kernel.</p>\n<h3 id=\"1-Getting-the-kernel-source-of-official-release\"><a href=\"#1-Getting-the-kernel-source-of-official-release\" class=\"headerlink\" title=\"1. Getting the kernel source of official release\"></a>1. Getting the kernel source of official release</h3><p>Nothing comes from nothing. So the first thing before compiling a customized kernel is getting source code.<br>I strongly recommend using <code>Git</code> to download and manage the linux kernel source:</p>\n<p><epacse hidden>11</epacse></p></body></html>","more":"<p>Surely, you can also download the compressed package of source code and then uncompress it.<br>Go to the source code root directory, there exists a number of directories under it.</p>\n<img src=\"/2016/03/23/Compile-your-own-linux-kernel/kernel-directory.png\" class=\"\" title=\"kernel src file dir\">\n\n<p>The following table[1] illustrates explanation about these directories.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Directory</th>\n<th align=\"center\">Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">arch</td>\n<td align=\"center\">Architecture-specific source</td>\n</tr>\n<tr>\n<td align=\"center\">block</td>\n<td align=\"center\">Block I/O layer</td>\n</tr>\n<tr>\n<td align=\"center\">certs</td>\n<td align=\"center\">SSL/TLS certification</td>\n</tr>\n<tr>\n<td align=\"center\">crypto</td>\n<td align=\"center\">Crypto API</td>\n</tr>\n<tr>\n<td align=\"center\">Documentation</td>\n<td align=\"center\">Kernel source documentation</td>\n</tr>\n<tr>\n<td align=\"center\">drivers</td>\n<td align=\"center\">drivers Device</td>\n</tr>\n<tr>\n<td align=\"center\">firmware</td>\n<td align=\"center\">Device firmware needed to use certain drivers</td>\n</tr>\n<tr>\n<td align=\"center\">fs</td>\n<td align=\"center\">The VFS and the individual filesystems</td>\n</tr>\n<tr>\n<td align=\"center\">include</td>\n<td align=\"center\">Kernel headers</td>\n</tr>\n<tr>\n<td align=\"center\">init</td>\n<td align=\"center\">Kernel boot and initialization</td>\n</tr>\n<tr>\n<td align=\"center\">ipc</td>\n<td align=\"center\">Interprocess communication code</td>\n</tr>\n<tr>\n<td align=\"center\">kernel</td>\n<td align=\"center\">Core subsystems, such as the scheduler</td>\n</tr>\n<tr>\n<td align=\"center\">lib</td>\n<td align=\"center\">Helper routines</td>\n</tr>\n<tr>\n<td align=\"center\">mm</td>\n<td align=\"center\">Memory management subsystem and the VM</td>\n</tr>\n<tr>\n<td align=\"center\">net</td>\n<td align=\"center\">Networking subsystem</td>\n</tr>\n<tr>\n<td align=\"center\">samples</td>\n<td align=\"center\">Sample, demonstrative code</td>\n</tr>\n<tr>\n<td align=\"center\">scripts</td>\n<td align=\"center\">Scripts used to build the kernel</td>\n</tr>\n<tr>\n<td align=\"center\">security</td>\n<td align=\"center\">Linux Security Module</td>\n</tr>\n<tr>\n<td align=\"center\">sound</td>\n<td align=\"center\">Sound subsystem</td>\n</tr>\n<tr>\n<td align=\"center\">usr</td>\n<td align=\"center\">Early user-space code (called initramfs)</td>\n</tr>\n<tr>\n<td align=\"center\">tools</td>\n<td align=\"center\">Tools helpful for developing Linux</td>\n</tr>\n<tr>\n<td align=\"center\">virt</td>\n<td align=\"center\">Virtualization infrastructure</td>\n</tr>\n</tbody></table>\n<h3 id=\"2-Building-the-kernel-source-code\"><a href=\"#2-Building-the-kernel-source-code\" class=\"headerlink\" title=\"2. Building the kernel source code\"></a>2. Building the kernel source code</h3><p>After the first step, you come here. Now what you should do is configuring the kernel before compiling. As mentioned previously, it is possible to compile support into your kernel for only the specific features and drivers you want. Configuring the kernel is a required process before building it. By default, the kernel of official release version provides myriad features and supports a varied basket of hardware.</p>\n<h4 id=\"1-Configuration\"><a href=\"#1-Configuration\" class=\"headerlink\" title=\"(1). Configuration\"></a>(1). Configuration</h4><p>when you change your current working directory to the root directory of linux kernel source code, you will find there is a file named <strong>.config</strong>. Using command such as <code>cat .config | more</code> you can take a glimpse of its content.</p>\n<p><epacse hidden>12</epacse></p>\n<img src=\"/2016/03/23/Compile-your-own-linux-kernel/kernel-configuration.png\" class=\"\" title=\"kernel config file\">\n\n<p>As shown in the picture, kernel configuration is controlled by configuration options, which are prefixed by <strong>CONFIG</strong> in the form <strong>CONFIG_FEATURE</strong>. That is to say, asynchronous IO is controlled by the configuration option <strong>CONFIG_AIO</strong>. This option enables POSIX asynchronous I/O which may by used by some high performance threaded applications<a href=\"http://cateee.net/lkddb/web-lkddb/AIO.html\" target=\"_blank\" rel=\"noopener\">Reference</a>. When this option is set, AIO is enabled; if unset, AIO is disabled.<br>Configuration options that control the build process are either Booleans or tristates. A Boolean option is either yes or no. Kernel features, such as CONFIG_PREEMPT, are usually Booleans. A tristate option is one of yes, no, or module.The module setting represents a configuration option that is set but is to be compiled as a module (that is, a separate dynamically loadable object). In the case of tristates, a yes option explicitly means to compile the code into the main kernel image and not as a module. Drivers are usually represented by tristates[Reference][3].<br>Configuration options can also be strings or integers.These options do not control the build process but instead specify values that kernel source can access as a preprocessor macro. For example, a configuration option can specify the size of a statically allocated array[Reference][4].<br>Kernel provides multiple choices for you to facilitate configurations. A straightfoward way is using a graphical interactive interface: <code>make menuconfig</code>.</p>\n<p><epacse hidden>13</epacse></p>\n<p>After typing this command, a graphical interactive interface will appears in your screen like this:</p>\n<img src=\"/2016/03/23/Compile-your-own-linux-kernel/kernel-menuconfig.png\" class=\"\" title=\"kernel menuconfig file\">\n\n<p>And you can move the cursor to different options to set them. Because of space, how to configure these options correctly can not be presented. For more detailed knowledge, you can find them in the linux orgnization.</p>\n<h4 id=\"2-Compile-and-build\"><a href=\"#2-Compile-and-build\" class=\"headerlink\" title=\"(2). Compile and build\"></a>(2). Compile and build</h4><p>Now, it is time to get into the marrow of the second part: Compile &amp;&amp; Build. Please make sure that command <code>make</code> and <code>gcc</code> is installed on your machine firstly.<br>Just type <code>make</code> and all related source code about kernel will be compiled and built, the default Makefile rule will handle everything.</p>\n<p><epacse hidden>14</epacse></p>\n<p>In general, one flaw about the <code>make</code> method is that this action spawns only a single job because Makefiles all too often have incorrect dependency information. With incorrect dependencies, multiple jobs can step on each others toes, resulting in errors in the build process. However, The kernels Makefile have correct dependency information, so spawning multiple jobs does not result in failures. To build the kernel with multiple make jobs, use</p>\n<p><epacse hidden>15</epacse></p>\n<p>The n here is number of jobs to spawn. Usual practice is to spawn one or two jobs per processor. If you have 16 processors in you machine, then you might do</p>\n<p><epacse hidden>16</epacse></p>\n<p>The resulting kernel file is arch/x86/boot/bzImage (in x86 platform).</p>\n<h3 id=\"3-Installation\"><a href=\"#3-Installation\" class=\"headerlink\" title=\"3. Installation\"></a>3. Installation</h3><p>After the kernel is built, you can install it. It is possible that the kernel you install cannot boot successfully, so in case of that, you should have at least two kernel installed on you machine so that you can choose the another one to boot.</p>\n<h4 id=\"1-Install-modules\"><a href=\"#1-Install-modules\" class=\"headerlink\" title=\"(1). Install modules\"></a>(1). Install modules</h4><p>Installing modules, thankfully, is automated and architecture-independent. As root, simply run</p>\n<p><epacse hidden>17</epacse></p>\n<p>After this, you will find a module file under <strong>/lib/modules/a.b.c</strong> where a.b.c is the kernel version.</p>\n<h4 id=\"2-Install-kernel\"><a href=\"#2-Install-kernel\" class=\"headerlink\" title=\"(2). Install kernel\"></a>(2). Install kernel</h4><p>As root user, simply run</p>\n<p><epacse hidden>18</epacse></p>\n<p>After this, a new kernel file and a new boot image will appear in the <strong>/boot</strong> directory.</p>\n<h4 id=\"3-Set-booting-order\"><a href=\"#3-Set-booting-order\" class=\"headerlink\" title=\"(3). Set booting order\"></a>(3). Set booting order</h4><p>If you execute all the steps normally, new content about the new installed kernel has been added to <strong>/boot/grub/grub.conf</strong> file. And you can edit the <strong>grub.conf</strong> file to choose to use which kernel when booting.</p>\n<img src=\"/2016/03/23/Compile-your-own-linux-kernel/kernel-grub.png\" class=\"\" title=\"kernel grub file\">\n\n<p>Reboot the machine, and then you will find the new installed kernel in the booting screen.</p>\n<img src=\"/2016/03/23/Compile-your-own-linux-kernel/kernel-booting.png\" class=\"\" title=\"booting file\">\n\n<p>[1]: Love, Robert Love. (2003). Linux Kernel Development, 3, 40-42</p>\n<p>[3]: Love, Robert Love. (2003). Linux Kernel Development, 3, 42-43<br>[4]: Love, Robert Love. (2003). Linux Kernel Development, 3, 43-45</p>"},{"title":"Read and Write functions in linux","date":"2016-04-12T13:43:21.000Z","_content":"\nResulting from work, I have learned I/O models of the linux operating system during these days. In linux operating system, various read and write APIs are provided to user space for use. Comparasions between them are illustraed below.\n\n\n\n### read()\n\n```\n#include <unistd.h>\nssize_t read(int fd, void *buf, size_t count);\n```\n\n`read()` is the basic read function in linux environment. It attempts to read up to count bytes from file descriptor fd into the buffer starting at buf.\nIt will start from current file offset. And the current file offset will be increased by the number of bytes read. However, if current file offset is at or past the end of operating file, no bytes will be read into buffer.\nOn success, the number of bytes read is returned (zero indicates end of file), and the file position is advanced by this number. It is not an error if this number is smaller than the number of bytes requested; this may happen for example because fewer bytes are actually available right now (maybe because we were close to end-of-file, or because we are reading from a pipe, or from a terminal), or because `read()` was interrupted by a signal. On error, -1 is returned, and errno is set appropriately. In this case it is left unspecified whether the file position (if any) changes.[Reference](http://linux.die.net/man/2/read)\n`read()` is thread safe in the sense that your program will not have undefined behavior (crash or hung) if multiple threads perform IO on the same open file using at once. But the order and atomicity of these operations could vary greatly depending on the type of the file and the implementation of program.\n\n\n<!-- more -->\n\n### lseek()\n\n```\n#include <sys/types.h>\n#include <unistd.h>\noff_t lseek(int fd, off_t offset, int whence);\n```\n\nThe `lseek()` function repositions the offset of the open file associated with the file descriptor fd to the argument offset according to the directive whence\nThe directive whence can be as follows:\n**SEEK_SET** The offset is set to offset bytes.\n**SEEK_CUR** The offset is set to its current location plus offset bytes.\n**SEEK_END** The offset is set to the size of the file plus offset bytes.\nWhen whence is as the last one, `lseek()` function allows the file offset to be set beyond the size of file while the file size still keeps the same. If data is latter write at this point, subsequent reads of the data in the gap (as a hole) return null bytes until data is actually written to this gap. [Reference](http://linux.die.net/man/2/lseek)\nThere are some special usage methods about `lseek()`:\n\n1. `lseek(int fildes, 0, SEEK_SET)`:\n   move the read or write position to the start of the file\n2. `lseek(int fildes, 0, SEEK_END)`:\n   move the read or write position to the end of the file\n3. `lseek(int fildes, 0, SEEK_CUR)`:\n   get the current read or write position of the file\n\nWith `lseek()`, you can implement the random I/O models of read and write easily.\n\n### pread()\n\n```\n#include <unistd.h>\nssize_t pread(int fd, void *buf, size_t count, off_t offset);\n```\n\nSimilar to `read()`, `pread()` attempts to read count bytes from file descriptor fd at offset into buffer starting at buf. Unlike `read()`, the offset here will be not changed after the call of `pread`\nIn many cases `pread()` is the only option when youre dealing with threads reading from a database or such.\nCompared with `read()`, `pread()` does more than `read()` on account of the time to positioning offset. From the work mechanism of `pread()`, we can find that it is like the combination of `read()` and `lseek()`. Nevertheless, performance of `pread()` is quite higher than the combination of `read()` and `lseek()`.\nAs mentioned above, `read()` function will be in mess when multiple threads or processes perform IO operations on the same open file because it will increase the current file offset. On the flip side, `pread()` do not change the position in the open file so it is more convenient to using in the scenario of multiple threads and processes.\n\n### pwrite()\n\n```\n#include <unistd.h>\nssize_t pwrite(int fd, const void *buf, size_t nbytes, off_t offset);\n\t\t\t\tReturns: number of bytes written if OK, 1 on error\n```\n\nCalling `pwrite()` is equivalent to calling `lseek()` followed by a call to `write()`. Instead of calling `lseesk()` and `write()` separately, the combination of `lseek()` and `write()` is atomic operation in `pwrite()`.\n","source":"_posts/Read-and-Write-functions-in-linux.md","raw":"---\ntitle: Read and Write functions in linux\ndate: 2016-04-12 21:43:21\ntags: [linux, C]\ncategories: Technology\n---\n\nResulting from work, I have learned I/O models of the linux operating system during these days. In linux operating system, various read and write APIs are provided to user space for use. Comparasions between them are illustraed below.\n\n\n\n### read()\n\n```\n#include <unistd.h>\nssize_t read(int fd, void *buf, size_t count);\n```\n\n`read()` is the basic read function in linux environment. It attempts to read up to count bytes from file descriptor fd into the buffer starting at buf.\nIt will start from current file offset. And the current file offset will be increased by the number of bytes read. However, if current file offset is at or past the end of operating file, no bytes will be read into buffer.\nOn success, the number of bytes read is returned (zero indicates end of file), and the file position is advanced by this number. It is not an error if this number is smaller than the number of bytes requested; this may happen for example because fewer bytes are actually available right now (maybe because we were close to end-of-file, or because we are reading from a pipe, or from a terminal), or because `read()` was interrupted by a signal. On error, -1 is returned, and errno is set appropriately. In this case it is left unspecified whether the file position (if any) changes.[Reference](http://linux.die.net/man/2/read)\n`read()` is thread safe in the sense that your program will not have undefined behavior (crash or hung) if multiple threads perform IO on the same open file using at once. But the order and atomicity of these operations could vary greatly depending on the type of the file and the implementation of program.\n\n\n<!-- more -->\n\n### lseek()\n\n```\n#include <sys/types.h>\n#include <unistd.h>\noff_t lseek(int fd, off_t offset, int whence);\n```\n\nThe `lseek()` function repositions the offset of the open file associated with the file descriptor fd to the argument offset according to the directive whence\nThe directive whence can be as follows:\n**SEEK_SET** The offset is set to offset bytes.\n**SEEK_CUR** The offset is set to its current location plus offset bytes.\n**SEEK_END** The offset is set to the size of the file plus offset bytes.\nWhen whence is as the last one, `lseek()` function allows the file offset to be set beyond the size of file while the file size still keeps the same. If data is latter write at this point, subsequent reads of the data in the gap (as a hole) return null bytes until data is actually written to this gap. [Reference](http://linux.die.net/man/2/lseek)\nThere are some special usage methods about `lseek()`:\n\n1. `lseek(int fildes, 0, SEEK_SET)`:\n   move the read or write position to the start of the file\n2. `lseek(int fildes, 0, SEEK_END)`:\n   move the read or write position to the end of the file\n3. `lseek(int fildes, 0, SEEK_CUR)`:\n   get the current read or write position of the file\n\nWith `lseek()`, you can implement the random I/O models of read and write easily.\n\n### pread()\n\n```\n#include <unistd.h>\nssize_t pread(int fd, void *buf, size_t count, off_t offset);\n```\n\nSimilar to `read()`, `pread()` attempts to read count bytes from file descriptor fd at offset into buffer starting at buf. Unlike `read()`, the offset here will be not changed after the call of `pread`\nIn many cases `pread()` is the only option when youre dealing with threads reading from a database or such.\nCompared with `read()`, `pread()` does more than `read()` on account of the time to positioning offset. From the work mechanism of `pread()`, we can find that it is like the combination of `read()` and `lseek()`. Nevertheless, performance of `pread()` is quite higher than the combination of `read()` and `lseek()`.\nAs mentioned above, `read()` function will be in mess when multiple threads or processes perform IO operations on the same open file because it will increase the current file offset. On the flip side, `pread()` do not change the position in the open file so it is more convenient to using in the scenario of multiple threads and processes.\n\n### pwrite()\n\n```\n#include <unistd.h>\nssize_t pwrite(int fd, const void *buf, size_t nbytes, off_t offset);\n\t\t\t\tReturns: number of bytes written if OK, 1 on error\n```\n\nCalling `pwrite()` is equivalent to calling `lseek()` followed by a call to `write()`. Instead of calling `lseesk()` and `write()` separately, the combination of `lseek()` and `write()` is atomic operation in `pwrite()`.\n","slug":"Read-and-Write-functions-in-linux","published":1,"updated":"2020-05-16T14:16:06.995Z","_id":"cka9on2a8000011pk5h00g8jk","comments":1,"layout":"post","photos":[],"link":"","content":"<html><head></head><body><p>Resulting from work, I have learned I/O models of the linux operating system during these days. In linux operating system, various read and write APIs are provided to user space for use. Comparasions between them are illustraed below.</p>\n<h3 id=\"read\"><a href=\"#read\" class=\"headerlink\" title=\"read()\"></a>read()</h3><p></p><figure class=\"highlight plain hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;unistd.h&gt;</span><br><span class=\"line\">ssize_t read(int fd, void *buf, size_t count);</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p><code>read()</code> is the basic read function in linux environment. It attempts to read up to count bytes from file descriptor fd into the buffer starting at buf.<br>It will start from current file offset. And the current file offset will be increased by the number of bytes read. However, if current file offset is at or past the end of operating file, no bytes will be read into buffer.<br>On success, the number of bytes read is returned (zero indicates end of file), and the file position is advanced by this number. It is not an error if this number is smaller than the number of bytes requested; this may happen for example because fewer bytes are actually available right now (maybe because we were close to end-of-file, or because we are reading from a pipe, or from a terminal), or because <code>read()</code> was interrupted by a signal. On error, -1 is returned, and errno is set appropriately. In this case it is left unspecified whether the file position (if any) changes.<a href=\"http://linux.die.net/man/2/read\" target=\"_blank\" rel=\"noopener\">Reference</a><br><code>read()</code> is thread safe in the sense that your program will not have undefined behavior (crash or hung) if multiple threads perform IO on the same open file using at once. But the order and atomicity of these operations could vary greatly depending on the type of the file and the implementation of program.</p>\n<a id=\"more\"></a>\n\n<h3 id=\"lseek\"><a href=\"#lseek\" class=\"headerlink\" title=\"lseek()\"></a>lseek()</h3><p></p><figure class=\"highlight plain hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;sys/types.h&gt;</span><br><span class=\"line\">#include &lt;unistd.h&gt;</span><br><span class=\"line\">off_t lseek(int fd, off_t offset, int whence);</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>The <code>lseek()</code> function repositions the offset of the open file associated with the file descriptor fd to the argument offset according to the directive whence<br>The directive whence can be as follows:<br><strong>SEEK_SET</strong> The offset is set to offset bytes.<br><strong>SEEK_CUR</strong> The offset is set to its current location plus offset bytes.<br><strong>SEEK_END</strong> The offset is set to the size of the file plus offset bytes.<br>When whence is as the last one, <code>lseek()</code> function allows the file offset to be set beyond the size of file while the file size still keeps the same. If data is latter write at this point, subsequent reads of the data in the gap (as a &#x201C;hole&#x201D;) return null bytes until data is actually written to this gap. <a href=\"http://linux.die.net/man/2/lseek\" target=\"_blank\" rel=\"noopener\">Reference</a><br>There are some special usage methods about <code>lseek()</code>:</p>\n<ol>\n<li><code>lseek(int fildes, 0, SEEK_SET)</code>:<br>move the read or write position to the start of the file</li>\n<li><code>lseek(int fildes, 0, SEEK_END)</code>:<br>move the read or write position to the end of the file</li>\n<li><code>lseek(int fildes, 0, SEEK_CUR)</code>:<br>get the current read or write position of the file</li>\n</ol>\n<p>With <code>lseek()</code>, you can implement the random I/O models of read and write easily.</p>\n<h3 id=\"pread\"><a href=\"#pread\" class=\"headerlink\" title=\"pread()\"></a>pread()</h3><p></p><figure class=\"highlight plain hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;unistd.h&gt;</span><br><span class=\"line\">ssize_t pread(int fd, void *buf, size_t count, off_t offset);</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>Similar to <code>read()</code>, <code>pread()</code> attempts to read count bytes from file descriptor fd at offset into buffer starting at buf. Unlike <code>read()</code>, the offset here will be not changed after the call of <code>pread</code><br>In many cases <code>pread()</code> is the only option when you&#x2019;re dealing with threads reading from a database or such.<br>Compared with <code>read()</code>, <code>pread()</code> does more than <code>read()</code> on account of the time to positioning offset. From the work mechanism of <code>pread()</code>, we can find that it is like the combination of <code>read()</code> and <code>lseek()</code>. Nevertheless, performance of <code>pread()</code> is quite higher than the combination of <code>read()</code> and <code>lseek()</code>.<br>As mentioned above, <code>read()</code> function will be in mess when multiple threads or processes perform IO operations on the same open file because it will increase the current file offset. On the flip side, <code>pread()</code> do not change the position in the open file so it is more convenient to using in the scenario of multiple threads and processes.</p>\n<h3 id=\"pwrite\"><a href=\"#pwrite\" class=\"headerlink\" title=\"pwrite()\"></a>pwrite()</h3><p></p><figure class=\"highlight plain hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;unistd.h&gt;</span><br><span class=\"line\">ssize_t pwrite(int fd, const void *buf, size_t nbytes, off_t offset);</span><br><span class=\"line\">\t\t\t\tReturns: number of bytes written if OK, &#x2212;1 on error</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>Calling <code>pwrite()</code> is equivalent to calling <code>lseek()</code> followed by a call to <code>write()</code>. Instead of calling <code>lseesk()</code> and <code>write()</code> separately, the combination of <code>lseek()</code> and <code>write()</code> is atomic operation in <code>pwrite()</code>.</p>\n</body></html>","site":{"data":{}},"_categories":[{"name":"Technology","path":"categories/Technology/"}],"_tags":[{"name":"linux","path":"tags/linux/"},{"name":"C","path":"tags/C/"}],"excerpt":"<html><head></head><body><p>Resulting from work, I have learned I/O models of the linux operating system during these days. In linux operating system, various read and write APIs are provided to user space for use. Comparasions between them are illustraed below.</p>\n<h3 id=\"read\"><a href=\"#read\" class=\"headerlink\" title=\"read()\"></a>read()</h3><p><epacse hidden>19</epacse></p>\n<p><code>read()</code> is the basic read function in linux environment. It attempts to read up to count bytes from file descriptor fd into the buffer starting at buf.<br>It will start from current file offset. And the current file offset will be increased by the number of bytes read. However, if current file offset is at or past the end of operating file, no bytes will be read into buffer.<br>On success, the number of bytes read is returned (zero indicates end of file), and the file position is advanced by this number. It is not an error if this number is smaller than the number of bytes requested; this may happen for example because fewer bytes are actually available right now (maybe because we were close to end-of-file, or because we are reading from a pipe, or from a terminal), or because <code>read()</code> was interrupted by a signal. On error, -1 is returned, and errno is set appropriately. In this case it is left unspecified whether the file position (if any) changes.<a href=\"http://linux.die.net/man/2/read\" target=\"_blank\" rel=\"noopener\">Reference</a><br><code>read()</code> is thread safe in the sense that your program will not have undefined behavior (crash or hung) if multiple threads perform IO on the same open file using at once. But the order and atomicity of these operations could vary greatly depending on the type of the file and the implementation of program.</p></body></html>","more":"<h3 id=\"lseek\"><a href=\"#lseek\" class=\"headerlink\" title=\"lseek()\"></a>lseek()</h3><p><epacse hidden>20</epacse></p>\n<p>The <code>lseek()</code> function repositions the offset of the open file associated with the file descriptor fd to the argument offset according to the directive whence<br>The directive whence can be as follows:<br><strong>SEEK_SET</strong> The offset is set to offset bytes.<br><strong>SEEK_CUR</strong> The offset is set to its current location plus offset bytes.<br><strong>SEEK_END</strong> The offset is set to the size of the file plus offset bytes.<br>When whence is as the last one, <code>lseek()</code> function allows the file offset to be set beyond the size of file while the file size still keeps the same. If data is latter write at this point, subsequent reads of the data in the gap (as a hole) return null bytes until data is actually written to this gap. <a href=\"http://linux.die.net/man/2/lseek\" target=\"_blank\" rel=\"noopener\">Reference</a><br>There are some special usage methods about <code>lseek()</code>:</p>\n<ol>\n<li><code>lseek(int fildes, 0, SEEK_SET)</code>:<br>move the read or write position to the start of the file</li>\n<li><code>lseek(int fildes, 0, SEEK_END)</code>:<br>move the read or write position to the end of the file</li>\n<li><code>lseek(int fildes, 0, SEEK_CUR)</code>:<br>get the current read or write position of the file</li>\n</ol>\n<p>With <code>lseek()</code>, you can implement the random I/O models of read and write easily.</p>\n<h3 id=\"pread\"><a href=\"#pread\" class=\"headerlink\" title=\"pread()\"></a>pread()</h3><p><epacse hidden>21</epacse></p>\n<p>Similar to <code>read()</code>, <code>pread()</code> attempts to read count bytes from file descriptor fd at offset into buffer starting at buf. Unlike <code>read()</code>, the offset here will be not changed after the call of <code>pread</code><br>In many cases <code>pread()</code> is the only option when youre dealing with threads reading from a database or such.<br>Compared with <code>read()</code>, <code>pread()</code> does more than <code>read()</code> on account of the time to positioning offset. From the work mechanism of <code>pread()</code>, we can find that it is like the combination of <code>read()</code> and <code>lseek()</code>. Nevertheless, performance of <code>pread()</code> is quite higher than the combination of <code>read()</code> and <code>lseek()</code>.<br>As mentioned above, <code>read()</code> function will be in mess when multiple threads or processes perform IO operations on the same open file because it will increase the current file offset. On the flip side, <code>pread()</code> do not change the position in the open file so it is more convenient to using in the scenario of multiple threads and processes.</p>\n<h3 id=\"pwrite\"><a href=\"#pwrite\" class=\"headerlink\" title=\"pwrite()\"></a>pwrite()</h3><p><epacse hidden>22</epacse></p>\n<p>Calling <code>pwrite()</code> is equivalent to calling <code>lseek()</code> followed by a call to <code>write()</code>. Instead of calling <code>lseesk()</code> and <code>write()</code> separately, the combination of <code>lseek()</code> and <code>write()</code> is atomic operation in <code>pwrite()</code>.</p>"},{"title":"How leveldb log works","date":"2020-05-31T13:56:02.000Z","_content":"\n### Introduction\n\nIn leveldb, a log file (*.log) stores a sequence of recent updates. Each update is appended to the current log file. \n\nThere are several benefits of log file for leveldb:\n\n- Boosting performance by converting random write into sequential write automatically in the underlying hard drives\n- Satisfying the atomicity and durability requirements of database properties\n\nWhen the user issues an update operation(put or delete), a corresponding record will be appended to the log file firstly. Only when the record is persisted successfully will a successful status be returned to the user.\n\n<!-- more -->\n\n\nfunction `DBImpl::Write` is the critical path for all update operations. Inside this function, statement `status = log_->AddRecord(WriteBatchInternal::Contents(updates))` appends update information to the current log file. And if it fails, it will return corresponding error code to the upper layer.\n\n```c++\nStatus DBImpl::Write(const WriteOptions& options, WriteBatch* my_batch) {\n  Writer w(&mutex_);\n  w.batch = my_batch;\n  w.sync = options.sync;\n  w.done = false;\n\n  MutexLock l(&mutex_);\n  writers_.push_back(&w);\n  while (!w.done && &w != writers_.front()) {\n    w.cv.Wait();\n  }\n  if (w.done) {\n    return w.status;\n  }\n\n  // May temporarily unlock and wait.\n  Status status = MakeRoomForWrite(my_batch == NULL);\n  uint64_t last_sequence = versions_->LastSequence();\n  Writer* last_writer = &w;\n  if (status.ok() && my_batch != NULL) {  // NULL batch is for compactions\n    WriteBatch* updates = BuildBatchGroup(&last_writer);\n    WriteBatchInternal::SetSequence(updates, last_sequence + 1);\n    last_sequence += WriteBatchInternal::Count(updates);\n\n    // Add to log and apply to memtable.  We can release the lock\n    // during this phase since &w is currently responsible for logging\n    // and protects against concurrent loggers and concurrent writes\n    // into mem_.\n    {\n      mutex_.Unlock();\n      status = log_->AddRecord(WriteBatchInternal::Contents(updates));\n      bool sync_error = false;\n      if (status.ok() && options.sync) {\n        status = logfile_->Sync();\n        if (!status.ok()) {\n          sync_error = true;\n        }\n      }\n      if (status.ok()) {\n        status = WriteBatchInternal::InsertInto(updates, mem_);\n      }\n      mutex_.Lock();\n      if (sync_error) {\n        // The state of the log file is indeterminate: the log record we\n        // just added may or may not show up when the DB is re-opened.\n        // So we force the DB into a mode where all future writes fail.\n        RecordBackgroundError(status);\n      }\n    }\n    if (updates == tmp_batch_) tmp_batch_->Clear();\n\n    versions_->SetLastSequence(last_sequence);\n  }\n\n  while (true) {\n    Writer* ready = writers_.front();\n    writers_.pop_front();\n    if (ready != &w) {\n      ready->status = status;\n      ready->done = true;\n      ready->cv.Signal();\n    }\n    if (ready == last_writer) break;\n  }\n\n  // Notify new head of write queue\n  if (!writers_.empty()) {\n    writers_.front()->cv.Signal();\n  }\n\n  return status;\n}\n```\n\n### Layout of Log File\n\nIn a log file, information is stored and retrieved with the unit of **record**. The record consists of two parts: header and payload. The header part is filled with CRC(4 bytes), Length(2 bytes), Type(1 byte). While the payload type is just all the content encapsulated by [WriteBatch](http://tsaijin.github.io/2020/05/16/The-mechanism-behind-WriteBatch-in-leveldb) class. \n\n{% asset_img record_format.png image of record format %}\n\n```c++\n// Header is checksum (4 bytes), length (2 bytes), type (1 byte).\nstatic const int kHeaderSize = 4 + 2 + 1\n```\n\nA more detailed description about the header:\n\n- CRC: the crc sum of the record type and the payload\n- Len: The total length of the payload\n- Type: The record type, candidates are `kFullType`, `kFirstType`, `kMiddleType` and `kLastType`\n\n```c++\nStatus Writer::EmitPhysicalRecord(RecordType t, const char* ptr, size_t n) {\n  assert(n <= 0xffff);  // Must fit in two bytes\n  assert(block_offset_ + kHeaderSize + n <= kBlockSize);\n\n  // Format the header\n  char buf[kHeaderSize];\n  buf[4] = static_cast<char>(n & 0xff);\n  buf[5] = static_cast<char>(n >> 8);\n  buf[6] = static_cast<char>(t);\n\n  // Compute the crc of the record type and the payload.\n  uint32_t crc = crc32c::Extend(type_crc_[t], ptr, n);\n  crc = crc32c::Mask(crc);                 // Adjust for storage\n  EncodeFixed32(buf, crc);\n\n  // Write the header and the payload\n  Status s = dest_->Append(Slice(buf, kHeaderSize));\n  if (s.ok()) {\n    s = dest_->Append(Slice(ptr, n));\n    if (s.ok()) {\n      s = dest_->Flush();\n    }\n  }\n  block_offset_ += kHeaderSize + n;\n  return s;\n}\n```\n\n\n\nThe block size of log file is designed  to 32KB intentionally. When leveldb tries to append a record to the tail of the log file, it must take block size into consideration.\n\n```c++\nstatic const int kBlockSize = 32768;\n```\n\nThere are several cases to handle when appending a record:\n\n- The remaining space inside a block cannot even accommodate the header(kHeaderSize = 7 bytes)\n\n- The remainning space inside a block can accommodate  the whole record\n- The whole record consumes space across two continuous blocks\n- The whole record consumes space across multiple(at least three) continuous blocks\n\nNow, let's talk about the first case: **The remaining space inside a block cannot even accommodate the header(kHeaderSize = 7 bytes)**.\n\nBecause each record must begin with a header, so when the remaning space inside a block is not enough to handle the header, the remaning space will be just padding with dummy data and the next continuous block will be chosen as an operating block.\n\n```c++\nconst int leftover = kBlockSize - block_offset_;\nassert(leftover >= 0);\nif (leftover < kHeaderSize) {\n\t// Switch to a new block\n  if (leftover > 0) {\n  // Fill the trailer (literal below relies on kHeaderSize being 7)\n  assert(kHeaderSize == 7);\n  dest_->Append(Slice(\"\\x00\\x00\\x00\\x00\\x00\\x00\", leftover));\n  }\n  block_offset_ = 0;\n}\n```\n\n{% asset_img record_not_for_header.png image of dummy data %}\n\n\n\nObviously, the new block will begin to handle the record.\n\nSecond case: **The remaining space inside a block can accommodate  the whole record**. In this case, a record with type `kFullType` will be appended to this block and the payload field contains all the data.\n\n{% asset_img record_within_one_block.png image of full record %}\n\n\n\nThird case: **The whole record consumes space across two continuous blocks**. The record will be splitted into two subrecords, the first subrecord is `kFirstType` type and the second subrecord is `kLastType` type.\n\n{% asset_img record_across_two_blocks.png  image of across two blocks %}\n\n\n\nFourth case: **The whole record consumes space across multiple(at least three) continuous blocks**. The record will be splitted into multiple subrecords. The first subrecord is `kFirstType` type and the last subrecord is `kLastType` type, all the other subrecords are `kMiddleType` type.\n\n{% asset_img record_across_multipe_blocks.png image of across multiple blocks %}\n\n```c++\nStatus Writer::AddRecord(const Slice& slice) {\n  const char* ptr = slice.data();\n  size_t left = slice.size();\n\n  // Fragment the record if necessary and emit it.  Note that if slice\n  // is empty, we still want to iterate once to emit a single\n  // zero-length record\n  Status s;\n  bool begin = true;\n  do {\n    const int leftover = kBlockSize - block_offset_;\n    assert(leftover >= 0);\n    if (leftover < kHeaderSize) {\n      // Switch to a new block\n      if (leftover > 0) {\n        // Fill the trailer (literal below relies on kHeaderSize being 7)\n        assert(kHeaderSize == 7);\n        dest_->Append(Slice(\"\\x00\\x00\\x00\\x00\\x00\\x00\", leftover));\n      }\n      block_offset_ = 0;\n    }\n\n    // Invariant: we never leave < kHeaderSize bytes in a block.\n    assert(kBlockSize - block_offset_ - kHeaderSize >= 0);\n\n    const size_t avail = kBlockSize - block_offset_ - kHeaderSize;\n    const size_t fragment_length = (left < avail) ? left : avail;\n\n    RecordType type;\n    const bool end = (left == fragment_length);\n    if (begin && end) {\n      type = kFullType;\n    } else if (begin) {\n      type = kFirstType;\n    } else if (end) {\n      type = kLastType;\n    } else {\n      type = kMiddleType;\n    }\n\n    s = EmitPhysicalRecord(type, ptr, fragment_length);\n    ptr += fragment_length;\n    left -= fragment_length;\n    begin = false;\n  } while (s.ok() && left > 0);\n  return s;\n}\n```\n\n\n\nIn summary, **no physical record on the disk is allowed to persist across blocks.**\n\n\n\n### Log Recovery\n\nLeveldb will call `DBImpl::Recover` to try to recover data from the log file when a database is opened each time. Function `DBImpl::Recover` replays all existing log files with the help of function `DBImpl::RecoverLogFile` in chronological order.\n\n```c++\n// Recover from all newer log files than the ones named in the\n  // descriptor (new log files may have been added by the previous\n  // incarnation without registering them in the descriptor).\n  //\n  // Note that PrevLogNumber() is no longer used, but we pay\n  // attention to it in case we are recovering a database\n  // produced by an older version of leveldb.\n  const uint64_t min_log = versions_->LogNumber();\n  const uint64_t prev_log = versions_->PrevLogNumber();\n  std::vector<std::string> filenames;\n  s = env_->GetChildren(dbname_, &filenames);\n  if (!s.ok()) {\n    return s;\n  }\n  std::set<uint64_t> expected;\n  versions_->AddLiveFiles(&expected);\n  uint64_t number;\n  FileType type;\n  std::vector<uint64_t> logs;\n  for (size_t i = 0; i < filenames.size(); i++) {\n    if (ParseFileName(filenames[i], &number, &type)) {\n      expected.erase(number);\n      if (type == kLogFile && ((number >= min_log) || (number == prev_log)))\n        logs.push_back(number);\n    }\n  }\n  if (!expected.empty()) {\n    char buf[50];\n    snprintf(buf, sizeof(buf), \"%d missing files; e.g.\",\n             static_cast<int>(expected.size()));\n    return Status::Corruption(buf, TableFileName(dbname_, *(expected.begin())));\n  }\n\n  // Recover in the order in which the logs were generated\n  std::sort(logs.begin(), logs.end());\n  for (size_t i = 0; i < logs.size(); i++) {\n    s = RecoverLogFile(logs[i], (i == logs.size() - 1), save_manifest, edit,\n                       &max_sequence);\n    if (!s.ok()) {\n      return s;\n    }\n\n    // The previous incarnation may not have written any MANIFEST\n    // records after allocating this log number.  So we manually\n    // update the file number allocation counter in VersionSet.\n    versions_->MarkFileNumberUsed(logs[i]);\n  }\n```\n\n\n\nIn function `DBImpl::RecoverLogFile`, it will read the log file sequentially from beginning to end.\n\n```c++\nStatus DBImpl::RecoverLogFile(uint64_t log_number, bool last_log,\n                              bool* save_manifest, VersionEdit* edit,\n                              SequenceNumber* max_sequence) {\n  struct LogReporter : public log::Reader::Reporter {\n    Env* env;\n    Logger* info_log;\n    const char* fname;\n    Status* status;  // NULL if options_.paranoid_checks==false\n    virtual void Corruption(size_t bytes, const Status& s) {\n      Log(info_log, \"%s%s: dropping %d bytes; %s\",\n          (this->status == NULL ? \"(ignoring error) \" : \"\"),\n          fname, static_cast<int>(bytes), s.ToString().c_str());\n      if (this->status != NULL && this->status->ok()) *this->status = s;\n    }\n  };\n\n  mutex_.AssertHeld();\n\n  // Open the log file\n  std::string fname = LogFileName(dbname_, log_number);\n  SequentialFile* file;\n  Status status = env_->NewSequentialFile(fname, &file);\n  if (!status.ok()) {\n    MaybeIgnoreError(&status);\n    return status;\n  }\n\n  // Create the log reader.\n  LogReporter reporter;\n  reporter.env = env_;\n  reporter.info_log = options_.info_log;\n  reporter.fname = fname.c_str();\n  reporter.status = (options_.paranoid_checks ? &status : NULL);\n  // We intentionally make log::Reader do checksumming even if\n  // paranoid_checks==false so that corruptions cause entire commits\n  // to be skipped instead of propagating bad information (like overly\n  // large sequence numbers).\n  log::Reader reader(file, &reporter, true/*checksum*/,\n                     0/*initial_offset*/);\n  Log(options_.info_log, \"Recovering log #%llu\",\n      (unsigned long long) log_number);\n\n  // Read all the records and add to a memtable\n  std::string scratch;\n  Slice record;\n  WriteBatch batch;\n  int compactions = 0;\n  MemTable* mem = NULL;\n  while (reader.ReadRecord(&record, &scratch) &&\n         status.ok()) {\n    if (record.size() < 12) {\n      reporter.Corruption(\n          record.size(), Status::Corruption(\"log record too small\"));\n      continue;\n    }\n    WriteBatchInternal::SetContents(&batch, record);\n\n    if (mem == NULL) {\n      mem = new MemTable(internal_comparator_);\n      mem->Ref();\n    }\n    status = WriteBatchInternal::InsertInto(&batch, mem);\n    MaybeIgnoreError(&status);\n    if (!status.ok()) {\n      break;\n    }\n    const SequenceNumber last_seq =\n        WriteBatchInternal::Sequence(&batch) +\n        WriteBatchInternal::Count(&batch) - 1;\n    if (last_seq > *max_sequence) {\n      *max_sequence = last_seq;\n    }\n\n    if (mem->ApproximateMemoryUsage() > options_.write_buffer_size) {\n      compactions++;\n      *save_manifest = true;\n      status = WriteLevel0Table(mem, edit, NULL);\n      mem->Unref();\n      mem = NULL;\n      if (!status.ok()) {\n        // Reflect errors immediately so that conditions like full\n        // file-systems cause the DB::Open() to fail.\n        break;\n      }\n    }\n  }\n\n  delete file;\n\n  // See if we should keep reusing the last log file.\n  if (status.ok() && options_.reuse_logs && last_log && compactions == 0) {\n    assert(logfile_ == NULL);\n    assert(log_ == NULL);\n    assert(mem_ == NULL);\n    uint64_t lfile_size;\n    if (env_->GetFileSize(fname, &lfile_size).ok() &&\n        env_->NewAppendableFile(fname, &logfile_).ok()) {\n      Log(options_.info_log, \"Reusing old log %s \\n\", fname.c_str());\n      log_ = new log::Writer(logfile_, lfile_size);\n      logfile_number_ = log_number;\n      if (mem != NULL) {\n        mem_ = mem;\n        mem = NULL;\n      } else {\n        // mem can be NULL if lognum exists but was empty.\n        mem_ = new MemTable(internal_comparator_);\n        mem_->Ref();\n      }\n    }\n  }\n\n  if (mem != NULL) {\n    // mem did not get reused; compact it.\n    if (status.ok()) {\n      *save_manifest = true;\n      status = WriteLevel0Table(mem, edit, NULL);\n    }\n    mem->Unref();\n  }\n\n  return status;\n}\n```\n\n\n\nIn function `Reader::ReadRecord`, there are several important variables:\n\n- `initial_offset_`: Offset at which to start looking for the first record to return. It is initialized in the Reader constructor function\n- `last_record_offset_`: Offset of the last record returned by ReadRecord\n\n\n\nIf `last_record_offset_` is less than `initial_offset_`, it will call `Reader::ReadRecord` to go to the correct block position to read the next record:\n\n```c++\nbool Reader::SkipToInitialBlock() {\n  size_t offset_in_block = initial_offset_ % kBlockSize;\n  uint64_t block_start_location = initial_offset_ - offset_in_block;\n\n  // Don't search a block if we'd be in the trailer\n  if (offset_in_block > kBlockSize - 6) {\n    offset_in_block = 0;\n    block_start_location += kBlockSize;\n  }\n\n  end_of_buffer_offset_ = block_start_location;\n\n  // Skip to start of first block that can contain the initial record\n  if (block_start_location > 0) {\n    Status skip_status = file_->Skip(block_start_location);\n    if (!skip_status.ok()) {\n      ReportDrop(block_start_location, skip_status);\n      return false;\n    }\n  }\n\n  return true;\n}\n```\n\n\n\nAs we see, a logical record maybe splitted into multiple subrecords. So there is another function named `Reader::ReadPhysicalRecord` to read the physical record on disk. \n\n```c++\nunsigned int Reader::ReadPhysicalRecord(Slice* result) {\n  while (true) {\n    if (buffer_.size() < kHeaderSize) {\n      if (!eof_) {\n        // Last read was a full read, so this is a trailer to skip\n        buffer_.clear();\n        Status status = file_->Read(kBlockSize, &buffer_, backing_store_);\n        end_of_buffer_offset_ += buffer_.size();\n        if (!status.ok()) {\n          buffer_.clear();\n          ReportDrop(kBlockSize, status);\n          eof_ = true;\n          return kEof;\n        } else if (buffer_.size() < kBlockSize) {\n          eof_ = true;\n        }\n        continue;\n      } else {\n        // Note that if buffer_ is non-empty, we have a truncated header at the\n        // end of the file, which can be caused by the writer crashing in the\n        // middle of writing the header. Instead of considering this an error,\n        // just report EOF.\n        buffer_.clear();\n        return kEof;\n      }\n    }\n\n    // Parse the header\n    const char* header = buffer_.data();\n    const uint32_t a = static_cast<uint32_t>(header[4]) & 0xff;\n    const uint32_t b = static_cast<uint32_t>(header[5]) & 0xff;\n    const unsigned int type = header[6];\n    const uint32_t length = a | (b << 8);\n    if (kHeaderSize + length > buffer_.size()) {\n      size_t drop_size = buffer_.size();\n      buffer_.clear();\n      if (!eof_) {\n        ReportCorruption(drop_size, \"bad record length\");\n        return kBadRecord;\n      }\n      // If the end of the file has been reached without reading |length| bytes\n      // of payload, assume the writer died in the middle of writing the record.\n      // Don't report a corruption.\n      return kEof;\n    }\n\n    if (type == kZeroType && length == 0) {\n      // Skip zero length record without reporting any drops since\n      // such records are produced by the mmap based writing code in\n      // env_posix.cc that preallocates file regions.\n      buffer_.clear();\n      return kBadRecord;\n    }\n\n    // Check crc\n    if (checksum_) {\n      uint32_t expected_crc = crc32c::Unmask(DecodeFixed32(header));\n      uint32_t actual_crc = crc32c::Value(header + 6, 1 + length);\n      if (actual_crc != expected_crc) {\n        // Drop the rest of the buffer since \"length\" itself may have\n        // been corrupted and if we trust it, we could find some\n        // fragment of a real log record that just happens to look\n        // like a valid log record.\n        size_t drop_size = buffer_.size();\n        buffer_.clear();\n        ReportCorruption(drop_size, \"checksum mismatch\");\n        return kBadRecord;\n      }\n    }\n\n    buffer_.remove_prefix(kHeaderSize + length);\n\n    // Skip physical record that started before initial_offset_\n    if (end_of_buffer_offset_ - buffer_.size() - kHeaderSize - length <\n        initial_offset_) {\n      result->clear();\n      return kBadRecord;\n    }\n\n    *result = Slice(header + kHeaderSize, length);\n    return type;\n  }\n}\n```\n\nIn `Reader::ReadPhysicalRecord`, it will parse the record header to get crc value and length value. With this information, it can check whether the data is intact or not.\n\nAnd if multiple physical records belong to one logical record, the `Reader::ReadRecord` will combine them together and return to the upper layer:\n\n```c++\nwhile (true) {\n    const unsigned int record_type = ReadPhysicalRecord(&fragment);\n\n    // ReadPhysicalRecord may have only had an empty trailer remaining in its\n    // internal buffer. Calculate the offset of the next physical record now\n    // that it has returned, properly accounting for its header size.\n    uint64_t physical_record_offset =\n        end_of_buffer_offset_ - buffer_.size() - kHeaderSize - fragment.size();\n\n    if (resyncing_) {\n      if (record_type == kMiddleType) {\n        continue;\n      } else if (record_type == kLastType) {\n        resyncing_ = false;\n        continue;\n      } else {\n        resyncing_ = false;\n      }\n    }\n\n    switch (record_type) {\n      case kFullType:\n        if (in_fragmented_record) {\n          // Handle bug in earlier versions of log::Writer where\n          // it could emit an empty kFirstType record at the tail end\n          // of a block followed by a kFullType or kFirstType record\n          // at the beginning of the next block.\n          if (scratch->empty()) {\n            in_fragmented_record = false;\n          } else {\n            ReportCorruption(scratch->size(), \"partial record without end(1)\");\n          }\n        }\n        prospective_record_offset = physical_record_offset;\n        scratch->clear();\n        *record = fragment;\n        last_record_offset_ = prospective_record_offset;\n        return true;\n\n      case kFirstType:\n        if (in_fragmented_record) {\n          // Handle bug in earlier versions of log::Writer where\n          // it could emit an empty kFirstType record at the tail end\n          // of a block followed by a kFullType or kFirstType record\n          // at the beginning of the next block.\n          if (scratch->empty()) {\n            in_fragmented_record = false;\n          } else {\n            ReportCorruption(scratch->size(), \"partial record without end(2)\");\n          }\n        }\n        prospective_record_offset = physical_record_offset;\n        scratch->assign(fragment.data(), fragment.size());\n        in_fragmented_record = true;\n        break;\n\n      case kMiddleType:\n        if (!in_fragmented_record) {\n          ReportCorruption(fragment.size(),\n                           \"missing start of fragmented record(1)\");\n        } else {\n          scratch->append(fragment.data(), fragment.size());\n        }\n        break;\n\n      case kLastType:\n        if (!in_fragmented_record) {\n          ReportCorruption(fragment.size(),\n                           \"missing start of fragmented record(2)\");\n        } else {\n          scratch->append(fragment.data(), fragment.size());\n          *record = Slice(*scratch);\n          last_record_offset_ = prospective_record_offset;\n          return true;\n        }\n        break;\n\n      case kEof:\n        if (in_fragmented_record) {\n          // This can be caused by the writer dying immediately after\n          // writing a physical record but before completing the next; don't\n          // treat it as a corruption, just ignore the entire logical record.\n          scratch->clear();\n        }\n        return false;\n\n      case kBadRecord:\n        if (in_fragmented_record) {\n          ReportCorruption(scratch->size(), \"error in middle of record\");\n          in_fragmented_record = false;\n          scratch->clear();\n        }\n        break;\n\n      default: {\n        char buf[40];\n        snprintf(buf, sizeof(buf), \"unknown record type %u\", record_type);\n        ReportCorruption(\n            (fragment.size() + (in_fragmented_record ? scratch->size() : 0)),\n            buf);\n        in_fragmented_record = false;\n        scratch->clear();\n        break;\n      }\n    }\n  }\n```\n\n\n\nFrom the code above, we know that in normal cases only when a record type of `kFullType` or `kLastType` is met can the function return to the upper layer.\n\nAnd the upper layer replays the whole operation based on the logical record information returned by `Reader::ReadRecord`. \n\n\n\n### Summary\n\nThis blog mainly talks about how log mechanism works in leveldb. Have fun~~~\n\n\n\n\n\n","source":"_posts/How-leveldb-log-works.md","raw":"---\ntitle: How leveldb log works\ndate: 2020-05-31 21:56:02\ntags: [database, leveldb]\n---\n\n### Introduction\n\nIn leveldb, a log file (*.log) stores a sequence of recent updates. Each update is appended to the current log file. \n\nThere are several benefits of log file for leveldb:\n\n- Boosting performance by converting random write into sequential write automatically in the underlying hard drives\n- Satisfying the atomicity and durability requirements of database properties\n\nWhen the user issues an update operation(put or delete), a corresponding record will be appended to the log file firstly. Only when the record is persisted successfully will a successful status be returned to the user.\n\n<!-- more -->\n\n\nfunction `DBImpl::Write` is the critical path for all update operations. Inside this function, statement `status = log_->AddRecord(WriteBatchInternal::Contents(updates))` appends update information to the current log file. And if it fails, it will return corresponding error code to the upper layer.\n\n```c++\nStatus DBImpl::Write(const WriteOptions& options, WriteBatch* my_batch) {\n  Writer w(&mutex_);\n  w.batch = my_batch;\n  w.sync = options.sync;\n  w.done = false;\n\n  MutexLock l(&mutex_);\n  writers_.push_back(&w);\n  while (!w.done && &w != writers_.front()) {\n    w.cv.Wait();\n  }\n  if (w.done) {\n    return w.status;\n  }\n\n  // May temporarily unlock and wait.\n  Status status = MakeRoomForWrite(my_batch == NULL);\n  uint64_t last_sequence = versions_->LastSequence();\n  Writer* last_writer = &w;\n  if (status.ok() && my_batch != NULL) {  // NULL batch is for compactions\n    WriteBatch* updates = BuildBatchGroup(&last_writer);\n    WriteBatchInternal::SetSequence(updates, last_sequence + 1);\n    last_sequence += WriteBatchInternal::Count(updates);\n\n    // Add to log and apply to memtable.  We can release the lock\n    // during this phase since &w is currently responsible for logging\n    // and protects against concurrent loggers and concurrent writes\n    // into mem_.\n    {\n      mutex_.Unlock();\n      status = log_->AddRecord(WriteBatchInternal::Contents(updates));\n      bool sync_error = false;\n      if (status.ok() && options.sync) {\n        status = logfile_->Sync();\n        if (!status.ok()) {\n          sync_error = true;\n        }\n      }\n      if (status.ok()) {\n        status = WriteBatchInternal::InsertInto(updates, mem_);\n      }\n      mutex_.Lock();\n      if (sync_error) {\n        // The state of the log file is indeterminate: the log record we\n        // just added may or may not show up when the DB is re-opened.\n        // So we force the DB into a mode where all future writes fail.\n        RecordBackgroundError(status);\n      }\n    }\n    if (updates == tmp_batch_) tmp_batch_->Clear();\n\n    versions_->SetLastSequence(last_sequence);\n  }\n\n  while (true) {\n    Writer* ready = writers_.front();\n    writers_.pop_front();\n    if (ready != &w) {\n      ready->status = status;\n      ready->done = true;\n      ready->cv.Signal();\n    }\n    if (ready == last_writer) break;\n  }\n\n  // Notify new head of write queue\n  if (!writers_.empty()) {\n    writers_.front()->cv.Signal();\n  }\n\n  return status;\n}\n```\n\n### Layout of Log File\n\nIn a log file, information is stored and retrieved with the unit of **record**. The record consists of two parts: header and payload. The header part is filled with CRC(4 bytes), Length(2 bytes), Type(1 byte). While the payload type is just all the content encapsulated by [WriteBatch](http://tsaijin.github.io/2020/05/16/The-mechanism-behind-WriteBatch-in-leveldb) class. \n\n{% asset_img record_format.png image of record format %}\n\n```c++\n// Header is checksum (4 bytes), length (2 bytes), type (1 byte).\nstatic const int kHeaderSize = 4 + 2 + 1\n```\n\nA more detailed description about the header:\n\n- CRC: the crc sum of the record type and the payload\n- Len: The total length of the payload\n- Type: The record type, candidates are `kFullType`, `kFirstType`, `kMiddleType` and `kLastType`\n\n```c++\nStatus Writer::EmitPhysicalRecord(RecordType t, const char* ptr, size_t n) {\n  assert(n <= 0xffff);  // Must fit in two bytes\n  assert(block_offset_ + kHeaderSize + n <= kBlockSize);\n\n  // Format the header\n  char buf[kHeaderSize];\n  buf[4] = static_cast<char>(n & 0xff);\n  buf[5] = static_cast<char>(n >> 8);\n  buf[6] = static_cast<char>(t);\n\n  // Compute the crc of the record type and the payload.\n  uint32_t crc = crc32c::Extend(type_crc_[t], ptr, n);\n  crc = crc32c::Mask(crc);                 // Adjust for storage\n  EncodeFixed32(buf, crc);\n\n  // Write the header and the payload\n  Status s = dest_->Append(Slice(buf, kHeaderSize));\n  if (s.ok()) {\n    s = dest_->Append(Slice(ptr, n));\n    if (s.ok()) {\n      s = dest_->Flush();\n    }\n  }\n  block_offset_ += kHeaderSize + n;\n  return s;\n}\n```\n\n\n\nThe block size of log file is designed  to 32KB intentionally. When leveldb tries to append a record to the tail of the log file, it must take block size into consideration.\n\n```c++\nstatic const int kBlockSize = 32768;\n```\n\nThere are several cases to handle when appending a record:\n\n- The remaining space inside a block cannot even accommodate the header(kHeaderSize = 7 bytes)\n\n- The remainning space inside a block can accommodate  the whole record\n- The whole record consumes space across two continuous blocks\n- The whole record consumes space across multiple(at least three) continuous blocks\n\nNow, let's talk about the first case: **The remaining space inside a block cannot even accommodate the header(kHeaderSize = 7 bytes)**.\n\nBecause each record must begin with a header, so when the remaning space inside a block is not enough to handle the header, the remaning space will be just padding with dummy data and the next continuous block will be chosen as an operating block.\n\n```c++\nconst int leftover = kBlockSize - block_offset_;\nassert(leftover >= 0);\nif (leftover < kHeaderSize) {\n\t// Switch to a new block\n  if (leftover > 0) {\n  // Fill the trailer (literal below relies on kHeaderSize being 7)\n  assert(kHeaderSize == 7);\n  dest_->Append(Slice(\"\\x00\\x00\\x00\\x00\\x00\\x00\", leftover));\n  }\n  block_offset_ = 0;\n}\n```\n\n{% asset_img record_not_for_header.png image of dummy data %}\n\n\n\nObviously, the new block will begin to handle the record.\n\nSecond case: **The remaining space inside a block can accommodate  the whole record**. In this case, a record with type `kFullType` will be appended to this block and the payload field contains all the data.\n\n{% asset_img record_within_one_block.png image of full record %}\n\n\n\nThird case: **The whole record consumes space across two continuous blocks**. The record will be splitted into two subrecords, the first subrecord is `kFirstType` type and the second subrecord is `kLastType` type.\n\n{% asset_img record_across_two_blocks.png  image of across two blocks %}\n\n\n\nFourth case: **The whole record consumes space across multiple(at least three) continuous blocks**. The record will be splitted into multiple subrecords. The first subrecord is `kFirstType` type and the last subrecord is `kLastType` type, all the other subrecords are `kMiddleType` type.\n\n{% asset_img record_across_multipe_blocks.png image of across multiple blocks %}\n\n```c++\nStatus Writer::AddRecord(const Slice& slice) {\n  const char* ptr = slice.data();\n  size_t left = slice.size();\n\n  // Fragment the record if necessary and emit it.  Note that if slice\n  // is empty, we still want to iterate once to emit a single\n  // zero-length record\n  Status s;\n  bool begin = true;\n  do {\n    const int leftover = kBlockSize - block_offset_;\n    assert(leftover >= 0);\n    if (leftover < kHeaderSize) {\n      // Switch to a new block\n      if (leftover > 0) {\n        // Fill the trailer (literal below relies on kHeaderSize being 7)\n        assert(kHeaderSize == 7);\n        dest_->Append(Slice(\"\\x00\\x00\\x00\\x00\\x00\\x00\", leftover));\n      }\n      block_offset_ = 0;\n    }\n\n    // Invariant: we never leave < kHeaderSize bytes in a block.\n    assert(kBlockSize - block_offset_ - kHeaderSize >= 0);\n\n    const size_t avail = kBlockSize - block_offset_ - kHeaderSize;\n    const size_t fragment_length = (left < avail) ? left : avail;\n\n    RecordType type;\n    const bool end = (left == fragment_length);\n    if (begin && end) {\n      type = kFullType;\n    } else if (begin) {\n      type = kFirstType;\n    } else if (end) {\n      type = kLastType;\n    } else {\n      type = kMiddleType;\n    }\n\n    s = EmitPhysicalRecord(type, ptr, fragment_length);\n    ptr += fragment_length;\n    left -= fragment_length;\n    begin = false;\n  } while (s.ok() && left > 0);\n  return s;\n}\n```\n\n\n\nIn summary, **no physical record on the disk is allowed to persist across blocks.**\n\n\n\n### Log Recovery\n\nLeveldb will call `DBImpl::Recover` to try to recover data from the log file when a database is opened each time. Function `DBImpl::Recover` replays all existing log files with the help of function `DBImpl::RecoverLogFile` in chronological order.\n\n```c++\n// Recover from all newer log files than the ones named in the\n  // descriptor (new log files may have been added by the previous\n  // incarnation without registering them in the descriptor).\n  //\n  // Note that PrevLogNumber() is no longer used, but we pay\n  // attention to it in case we are recovering a database\n  // produced by an older version of leveldb.\n  const uint64_t min_log = versions_->LogNumber();\n  const uint64_t prev_log = versions_->PrevLogNumber();\n  std::vector<std::string> filenames;\n  s = env_->GetChildren(dbname_, &filenames);\n  if (!s.ok()) {\n    return s;\n  }\n  std::set<uint64_t> expected;\n  versions_->AddLiveFiles(&expected);\n  uint64_t number;\n  FileType type;\n  std::vector<uint64_t> logs;\n  for (size_t i = 0; i < filenames.size(); i++) {\n    if (ParseFileName(filenames[i], &number, &type)) {\n      expected.erase(number);\n      if (type == kLogFile && ((number >= min_log) || (number == prev_log)))\n        logs.push_back(number);\n    }\n  }\n  if (!expected.empty()) {\n    char buf[50];\n    snprintf(buf, sizeof(buf), \"%d missing files; e.g.\",\n             static_cast<int>(expected.size()));\n    return Status::Corruption(buf, TableFileName(dbname_, *(expected.begin())));\n  }\n\n  // Recover in the order in which the logs were generated\n  std::sort(logs.begin(), logs.end());\n  for (size_t i = 0; i < logs.size(); i++) {\n    s = RecoverLogFile(logs[i], (i == logs.size() - 1), save_manifest, edit,\n                       &max_sequence);\n    if (!s.ok()) {\n      return s;\n    }\n\n    // The previous incarnation may not have written any MANIFEST\n    // records after allocating this log number.  So we manually\n    // update the file number allocation counter in VersionSet.\n    versions_->MarkFileNumberUsed(logs[i]);\n  }\n```\n\n\n\nIn function `DBImpl::RecoverLogFile`, it will read the log file sequentially from beginning to end.\n\n```c++\nStatus DBImpl::RecoverLogFile(uint64_t log_number, bool last_log,\n                              bool* save_manifest, VersionEdit* edit,\n                              SequenceNumber* max_sequence) {\n  struct LogReporter : public log::Reader::Reporter {\n    Env* env;\n    Logger* info_log;\n    const char* fname;\n    Status* status;  // NULL if options_.paranoid_checks==false\n    virtual void Corruption(size_t bytes, const Status& s) {\n      Log(info_log, \"%s%s: dropping %d bytes; %s\",\n          (this->status == NULL ? \"(ignoring error) \" : \"\"),\n          fname, static_cast<int>(bytes), s.ToString().c_str());\n      if (this->status != NULL && this->status->ok()) *this->status = s;\n    }\n  };\n\n  mutex_.AssertHeld();\n\n  // Open the log file\n  std::string fname = LogFileName(dbname_, log_number);\n  SequentialFile* file;\n  Status status = env_->NewSequentialFile(fname, &file);\n  if (!status.ok()) {\n    MaybeIgnoreError(&status);\n    return status;\n  }\n\n  // Create the log reader.\n  LogReporter reporter;\n  reporter.env = env_;\n  reporter.info_log = options_.info_log;\n  reporter.fname = fname.c_str();\n  reporter.status = (options_.paranoid_checks ? &status : NULL);\n  // We intentionally make log::Reader do checksumming even if\n  // paranoid_checks==false so that corruptions cause entire commits\n  // to be skipped instead of propagating bad information (like overly\n  // large sequence numbers).\n  log::Reader reader(file, &reporter, true/*checksum*/,\n                     0/*initial_offset*/);\n  Log(options_.info_log, \"Recovering log #%llu\",\n      (unsigned long long) log_number);\n\n  // Read all the records and add to a memtable\n  std::string scratch;\n  Slice record;\n  WriteBatch batch;\n  int compactions = 0;\n  MemTable* mem = NULL;\n  while (reader.ReadRecord(&record, &scratch) &&\n         status.ok()) {\n    if (record.size() < 12) {\n      reporter.Corruption(\n          record.size(), Status::Corruption(\"log record too small\"));\n      continue;\n    }\n    WriteBatchInternal::SetContents(&batch, record);\n\n    if (mem == NULL) {\n      mem = new MemTable(internal_comparator_);\n      mem->Ref();\n    }\n    status = WriteBatchInternal::InsertInto(&batch, mem);\n    MaybeIgnoreError(&status);\n    if (!status.ok()) {\n      break;\n    }\n    const SequenceNumber last_seq =\n        WriteBatchInternal::Sequence(&batch) +\n        WriteBatchInternal::Count(&batch) - 1;\n    if (last_seq > *max_sequence) {\n      *max_sequence = last_seq;\n    }\n\n    if (mem->ApproximateMemoryUsage() > options_.write_buffer_size) {\n      compactions++;\n      *save_manifest = true;\n      status = WriteLevel0Table(mem, edit, NULL);\n      mem->Unref();\n      mem = NULL;\n      if (!status.ok()) {\n        // Reflect errors immediately so that conditions like full\n        // file-systems cause the DB::Open() to fail.\n        break;\n      }\n    }\n  }\n\n  delete file;\n\n  // See if we should keep reusing the last log file.\n  if (status.ok() && options_.reuse_logs && last_log && compactions == 0) {\n    assert(logfile_ == NULL);\n    assert(log_ == NULL);\n    assert(mem_ == NULL);\n    uint64_t lfile_size;\n    if (env_->GetFileSize(fname, &lfile_size).ok() &&\n        env_->NewAppendableFile(fname, &logfile_).ok()) {\n      Log(options_.info_log, \"Reusing old log %s \\n\", fname.c_str());\n      log_ = new log::Writer(logfile_, lfile_size);\n      logfile_number_ = log_number;\n      if (mem != NULL) {\n        mem_ = mem;\n        mem = NULL;\n      } else {\n        // mem can be NULL if lognum exists but was empty.\n        mem_ = new MemTable(internal_comparator_);\n        mem_->Ref();\n      }\n    }\n  }\n\n  if (mem != NULL) {\n    // mem did not get reused; compact it.\n    if (status.ok()) {\n      *save_manifest = true;\n      status = WriteLevel0Table(mem, edit, NULL);\n    }\n    mem->Unref();\n  }\n\n  return status;\n}\n```\n\n\n\nIn function `Reader::ReadRecord`, there are several important variables:\n\n- `initial_offset_`: Offset at which to start looking for the first record to return. It is initialized in the Reader constructor function\n- `last_record_offset_`: Offset of the last record returned by ReadRecord\n\n\n\nIf `last_record_offset_` is less than `initial_offset_`, it will call `Reader::ReadRecord` to go to the correct block position to read the next record:\n\n```c++\nbool Reader::SkipToInitialBlock() {\n  size_t offset_in_block = initial_offset_ % kBlockSize;\n  uint64_t block_start_location = initial_offset_ - offset_in_block;\n\n  // Don't search a block if we'd be in the trailer\n  if (offset_in_block > kBlockSize - 6) {\n    offset_in_block = 0;\n    block_start_location += kBlockSize;\n  }\n\n  end_of_buffer_offset_ = block_start_location;\n\n  // Skip to start of first block that can contain the initial record\n  if (block_start_location > 0) {\n    Status skip_status = file_->Skip(block_start_location);\n    if (!skip_status.ok()) {\n      ReportDrop(block_start_location, skip_status);\n      return false;\n    }\n  }\n\n  return true;\n}\n```\n\n\n\nAs we see, a logical record maybe splitted into multiple subrecords. So there is another function named `Reader::ReadPhysicalRecord` to read the physical record on disk. \n\n```c++\nunsigned int Reader::ReadPhysicalRecord(Slice* result) {\n  while (true) {\n    if (buffer_.size() < kHeaderSize) {\n      if (!eof_) {\n        // Last read was a full read, so this is a trailer to skip\n        buffer_.clear();\n        Status status = file_->Read(kBlockSize, &buffer_, backing_store_);\n        end_of_buffer_offset_ += buffer_.size();\n        if (!status.ok()) {\n          buffer_.clear();\n          ReportDrop(kBlockSize, status);\n          eof_ = true;\n          return kEof;\n        } else if (buffer_.size() < kBlockSize) {\n          eof_ = true;\n        }\n        continue;\n      } else {\n        // Note that if buffer_ is non-empty, we have a truncated header at the\n        // end of the file, which can be caused by the writer crashing in the\n        // middle of writing the header. Instead of considering this an error,\n        // just report EOF.\n        buffer_.clear();\n        return kEof;\n      }\n    }\n\n    // Parse the header\n    const char* header = buffer_.data();\n    const uint32_t a = static_cast<uint32_t>(header[4]) & 0xff;\n    const uint32_t b = static_cast<uint32_t>(header[5]) & 0xff;\n    const unsigned int type = header[6];\n    const uint32_t length = a | (b << 8);\n    if (kHeaderSize + length > buffer_.size()) {\n      size_t drop_size = buffer_.size();\n      buffer_.clear();\n      if (!eof_) {\n        ReportCorruption(drop_size, \"bad record length\");\n        return kBadRecord;\n      }\n      // If the end of the file has been reached without reading |length| bytes\n      // of payload, assume the writer died in the middle of writing the record.\n      // Don't report a corruption.\n      return kEof;\n    }\n\n    if (type == kZeroType && length == 0) {\n      // Skip zero length record without reporting any drops since\n      // such records are produced by the mmap based writing code in\n      // env_posix.cc that preallocates file regions.\n      buffer_.clear();\n      return kBadRecord;\n    }\n\n    // Check crc\n    if (checksum_) {\n      uint32_t expected_crc = crc32c::Unmask(DecodeFixed32(header));\n      uint32_t actual_crc = crc32c::Value(header + 6, 1 + length);\n      if (actual_crc != expected_crc) {\n        // Drop the rest of the buffer since \"length\" itself may have\n        // been corrupted and if we trust it, we could find some\n        // fragment of a real log record that just happens to look\n        // like a valid log record.\n        size_t drop_size = buffer_.size();\n        buffer_.clear();\n        ReportCorruption(drop_size, \"checksum mismatch\");\n        return kBadRecord;\n      }\n    }\n\n    buffer_.remove_prefix(kHeaderSize + length);\n\n    // Skip physical record that started before initial_offset_\n    if (end_of_buffer_offset_ - buffer_.size() - kHeaderSize - length <\n        initial_offset_) {\n      result->clear();\n      return kBadRecord;\n    }\n\n    *result = Slice(header + kHeaderSize, length);\n    return type;\n  }\n}\n```\n\nIn `Reader::ReadPhysicalRecord`, it will parse the record header to get crc value and length value. With this information, it can check whether the data is intact or not.\n\nAnd if multiple physical records belong to one logical record, the `Reader::ReadRecord` will combine them together and return to the upper layer:\n\n```c++\nwhile (true) {\n    const unsigned int record_type = ReadPhysicalRecord(&fragment);\n\n    // ReadPhysicalRecord may have only had an empty trailer remaining in its\n    // internal buffer. Calculate the offset of the next physical record now\n    // that it has returned, properly accounting for its header size.\n    uint64_t physical_record_offset =\n        end_of_buffer_offset_ - buffer_.size() - kHeaderSize - fragment.size();\n\n    if (resyncing_) {\n      if (record_type == kMiddleType) {\n        continue;\n      } else if (record_type == kLastType) {\n        resyncing_ = false;\n        continue;\n      } else {\n        resyncing_ = false;\n      }\n    }\n\n    switch (record_type) {\n      case kFullType:\n        if (in_fragmented_record) {\n          // Handle bug in earlier versions of log::Writer where\n          // it could emit an empty kFirstType record at the tail end\n          // of a block followed by a kFullType or kFirstType record\n          // at the beginning of the next block.\n          if (scratch->empty()) {\n            in_fragmented_record = false;\n          } else {\n            ReportCorruption(scratch->size(), \"partial record without end(1)\");\n          }\n        }\n        prospective_record_offset = physical_record_offset;\n        scratch->clear();\n        *record = fragment;\n        last_record_offset_ = prospective_record_offset;\n        return true;\n\n      case kFirstType:\n        if (in_fragmented_record) {\n          // Handle bug in earlier versions of log::Writer where\n          // it could emit an empty kFirstType record at the tail end\n          // of a block followed by a kFullType or kFirstType record\n          // at the beginning of the next block.\n          if (scratch->empty()) {\n            in_fragmented_record = false;\n          } else {\n            ReportCorruption(scratch->size(), \"partial record without end(2)\");\n          }\n        }\n        prospective_record_offset = physical_record_offset;\n        scratch->assign(fragment.data(), fragment.size());\n        in_fragmented_record = true;\n        break;\n\n      case kMiddleType:\n        if (!in_fragmented_record) {\n          ReportCorruption(fragment.size(),\n                           \"missing start of fragmented record(1)\");\n        } else {\n          scratch->append(fragment.data(), fragment.size());\n        }\n        break;\n\n      case kLastType:\n        if (!in_fragmented_record) {\n          ReportCorruption(fragment.size(),\n                           \"missing start of fragmented record(2)\");\n        } else {\n          scratch->append(fragment.data(), fragment.size());\n          *record = Slice(*scratch);\n          last_record_offset_ = prospective_record_offset;\n          return true;\n        }\n        break;\n\n      case kEof:\n        if (in_fragmented_record) {\n          // This can be caused by the writer dying immediately after\n          // writing a physical record but before completing the next; don't\n          // treat it as a corruption, just ignore the entire logical record.\n          scratch->clear();\n        }\n        return false;\n\n      case kBadRecord:\n        if (in_fragmented_record) {\n          ReportCorruption(scratch->size(), \"error in middle of record\");\n          in_fragmented_record = false;\n          scratch->clear();\n        }\n        break;\n\n      default: {\n        char buf[40];\n        snprintf(buf, sizeof(buf), \"unknown record type %u\", record_type);\n        ReportCorruption(\n            (fragment.size() + (in_fragmented_record ? scratch->size() : 0)),\n            buf);\n        in_fragmented_record = false;\n        scratch->clear();\n        break;\n      }\n    }\n  }\n```\n\n\n\nFrom the code above, we know that in normal cases only when a record type of `kFullType` or `kLastType` is met can the function return to the upper layer.\n\nAnd the upper layer replays the whole operation based on the logical record information returned by `Reader::ReadRecord`. \n\n\n\n### Summary\n\nThis blog mainly talks about how log mechanism works in leveldb. Have fun~~~\n\n\n\n\n\n","slug":"How-leveldb-log-works","published":1,"updated":"2020-05-31T14:05:56.554Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckav506w900001rpkamjg6zc9","content":"<html><head></head><body><h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>In leveldb, a log file (*.log) stores a sequence of recent updates. Each update is appended to the current log file. </p>\n<p>There are several benefits of log file for leveldb:</p>\n<ul>\n<li>Boosting performance by converting random write into sequential write automatically in the underlying hard drives</li>\n<li>Satisfying the atomicity and durability requirements of database properties</li>\n</ul>\n<p>When the user issues an update operation(put or delete), a corresponding record will be appended to the log file firstly. Only when the record is persisted successfully will a successful status be returned to the user.</p>\n<a id=\"more\"></a>\n\n\n<p>function <code>DBImpl::Write</code> is the critical path for all update operations. Inside this function, statement <code>status = log_-&gt;AddRecord(WriteBatchInternal::Contents(updates))</code> appends update information to the current log file. And if it fails, it will return corresponding error code to the upper layer.</p>\n<p></p><figure class=\"highlight c++ hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-function\">Status <span class=\"hljs-title\">DBImpl::Write</span><span class=\"hljs-params\">(<span class=\"hljs-keyword\">const</span> WriteOptions&amp; options, WriteBatch* my_batch)</span> </span>{</span><br><span class=\"line\">  <span class=\"hljs-function\">Writer <span class=\"hljs-title\">w</span><span class=\"hljs-params\">(&amp;mutex_)</span></span>;</span><br><span class=\"line\">  w.batch = my_batch;</span><br><span class=\"line\">  w.sync = options.sync;</span><br><span class=\"line\">  w.done = <span class=\"hljs-literal\">false</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-function\">MutexLock <span class=\"hljs-title\">l</span><span class=\"hljs-params\">(&amp;mutex_)</span></span>;</span><br><span class=\"line\">  writers_.push_back(&amp;w);</span><br><span class=\"line\">  <span class=\"hljs-keyword\">while</span> (!w.done &amp;&amp; &amp;w != writers_.front()) {</span><br><span class=\"line\">    w.cv.Wait();</span><br><span class=\"line\">  }</span><br><span class=\"line\">  <span class=\"hljs-keyword\">if</span> (w.done) {</span><br><span class=\"line\">    <span class=\"hljs-keyword\">return</span> w.status;</span><br><span class=\"line\">  }</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-comment\">// May temporarily unlock and wait.</span></span><br><span class=\"line\">  Status status = MakeRoomForWrite(my_batch == <span class=\"hljs-literal\">NULL</span>);</span><br><span class=\"line\">  <span class=\"hljs-keyword\">uint64_t</span> last_sequence = versions_-&gt;LastSequence();</span><br><span class=\"line\">  Writer* last_writer = &amp;w;</span><br><span class=\"line\">  <span class=\"hljs-keyword\">if</span> (status.ok() &amp;&amp; my_batch != <span class=\"hljs-literal\">NULL</span>) {  <span class=\"hljs-comment\">// NULL batch is for compactions</span></span><br><span class=\"line\">    WriteBatch* updates = BuildBatchGroup(&amp;last_writer);</span><br><span class=\"line\">    WriteBatchInternal::SetSequence(updates, last_sequence + <span class=\"hljs-number\">1</span>);</span><br><span class=\"line\">    last_sequence += WriteBatchInternal::Count(updates);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"hljs-comment\">// Add to log and apply to memtable.  We can release the lock</span></span><br><span class=\"line\">    <span class=\"hljs-comment\">// during this phase since &amp;w is currently responsible for logging</span></span><br><span class=\"line\">    <span class=\"hljs-comment\">// and protects against concurrent loggers and concurrent writes</span></span><br><span class=\"line\">    <span class=\"hljs-comment\">// into mem_.</span></span><br><span class=\"line\">    {</span><br><span class=\"line\">      mutex_.Unlock();</span><br><span class=\"line\">      status = log_-&gt;AddRecord(WriteBatchInternal::Contents(updates));</span><br><span class=\"line\">      <span class=\"hljs-keyword\">bool</span> sync_error = <span class=\"hljs-literal\">false</span>;</span><br><span class=\"line\">      <span class=\"hljs-keyword\">if</span> (status.ok() &amp;&amp; options.sync) {</span><br><span class=\"line\">        status = logfile_-&gt;Sync();</span><br><span class=\"line\">        <span class=\"hljs-keyword\">if</span> (!status.ok()) {</span><br><span class=\"line\">          sync_error = <span class=\"hljs-literal\">true</span>;</span><br><span class=\"line\">        }</span><br><span class=\"line\">      }</span><br><span class=\"line\">      <span class=\"hljs-keyword\">if</span> (status.ok()) {</span><br><span class=\"line\">        status = WriteBatchInternal::InsertInto(updates, mem_);</span><br><span class=\"line\">      }</span><br><span class=\"line\">      mutex_.Lock();</span><br><span class=\"line\">      <span class=\"hljs-keyword\">if</span> (sync_error) {</span><br><span class=\"line\">        <span class=\"hljs-comment\">// The state of the log file is indeterminate: the log record we</span></span><br><span class=\"line\">        <span class=\"hljs-comment\">// just added may or may not show up when the DB is re-opened.</span></span><br><span class=\"line\">        <span class=\"hljs-comment\">// So we force the DB into a mode where all future writes fail.</span></span><br><span class=\"line\">        RecordBackgroundError(status);</span><br><span class=\"line\">      }</span><br><span class=\"line\">    }</span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (updates == tmp_batch_) tmp_batch_-&gt;Clear();</span><br><span class=\"line\"></span><br><span class=\"line\">    versions_-&gt;SetLastSequence(last_sequence);</span><br><span class=\"line\">  }</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-keyword\">while</span> (<span class=\"hljs-literal\">true</span>) {</span><br><span class=\"line\">    Writer* <span class=\"hljs-built_in\">ready</span> = writers_.front();</span><br><span class=\"line\">    writers_.pop_front();</span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (<span class=\"hljs-built_in\">ready</span> != &amp;w) {</span><br><span class=\"line\">      <span class=\"hljs-built_in\">ready</span>-&gt;status = status;</span><br><span class=\"line\">      <span class=\"hljs-built_in\">ready</span>-&gt;done = <span class=\"hljs-literal\">true</span>;</span><br><span class=\"line\">      <span class=\"hljs-built_in\">ready</span>-&gt;cv.Signal();</span><br><span class=\"line\">    }</span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (<span class=\"hljs-built_in\">ready</span> == last_writer) <span class=\"hljs-keyword\">break</span>;</span><br><span class=\"line\">  }</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-comment\">// Notify new head of write queue</span></span><br><span class=\"line\">  <span class=\"hljs-keyword\">if</span> (!writers_.empty()) {</span><br><span class=\"line\">    writers_.front()-&gt;cv.Signal();</span><br><span class=\"line\">  }</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-keyword\">return</span> status;</span><br><span class=\"line\">}</span><br></pre></td></tr></tbody></table></figure><p></p>\n<h3 id=\"Layout-of-Log-File\"><a href=\"#Layout-of-Log-File\" class=\"headerlink\" title=\"Layout of Log File\"></a>Layout of Log File</h3><p>In a log file, information is stored and retrieved with the unit of <strong>record</strong>. The record consists of two parts: header and payload. The header part is filled with CRC(4 bytes), Length(2 bytes), Type(1 byte). While the payload type is just all the content encapsulated by <a href=\"http://tsaijin.github.io/2020/05/16/The-mechanism-behind-WriteBatch-in-leveldb\">WriteBatch</a> class. </p>\n<img src=\"/2020/05/31/How-leveldb-log-works/record_format.png\" class title=\"image of record format\">\n\n<p></p><figure class=\"highlight c++ hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-comment\">// Header is checksum (4 bytes), length (2 bytes), type (1 byte).</span></span><br><span class=\"line\"><span class=\"hljs-keyword\">static</span> <span class=\"hljs-keyword\">const</span> <span class=\"hljs-keyword\">int</span> kHeaderSize = <span class=\"hljs-number\">4</span> + <span class=\"hljs-number\">2</span> + <span class=\"hljs-number\">1</span></span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>A more detailed description about the header:</p>\n<ul>\n<li>CRC: the crc sum of the record type and the payload</li>\n<li>Len: The total length of the payload</li>\n<li>Type: The record type, candidates are <code>kFullType</code>, <code>kFirstType</code>, <code>kMiddleType</code> and <code>kLastType</code></li>\n</ul>\n<p></p><figure class=\"highlight c++ hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-function\">Status <span class=\"hljs-title\">Writer::EmitPhysicalRecord</span><span class=\"hljs-params\">(RecordType t, <span class=\"hljs-keyword\">const</span> <span class=\"hljs-keyword\">char</span>* ptr, <span class=\"hljs-keyword\">size_t</span> n)</span> </span>{</span><br><span class=\"line\">  assert(n &lt;= <span class=\"hljs-number\">0xffff</span>);  <span class=\"hljs-comment\">// Must fit in two bytes</span></span><br><span class=\"line\">  assert(block_offset_ + kHeaderSize + n &lt;= kBlockSize);</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-comment\">// Format the header</span></span><br><span class=\"line\">  <span class=\"hljs-keyword\">char</span> buf[kHeaderSize];</span><br><span class=\"line\">  buf[<span class=\"hljs-number\">4</span>] = <span class=\"hljs-keyword\">static_cast</span>&lt;<span class=\"hljs-keyword\">char</span>&gt;(n &amp; <span class=\"hljs-number\">0xff</span>);</span><br><span class=\"line\">  buf[<span class=\"hljs-number\">5</span>] = <span class=\"hljs-keyword\">static_cast</span>&lt;<span class=\"hljs-keyword\">char</span>&gt;(n &gt;&gt; <span class=\"hljs-number\">8</span>);</span><br><span class=\"line\">  buf[<span class=\"hljs-number\">6</span>] = <span class=\"hljs-keyword\">static_cast</span>&lt;<span class=\"hljs-keyword\">char</span>&gt;(t);</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-comment\">// Compute the crc of the record type and the payload.</span></span><br><span class=\"line\">  <span class=\"hljs-keyword\">uint32_t</span> crc = crc32c::Extend(type_crc_[t], ptr, n);</span><br><span class=\"line\">  crc = crc32c::Mask(crc);                 <span class=\"hljs-comment\">// Adjust for storage</span></span><br><span class=\"line\">  EncodeFixed32(buf, crc);</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-comment\">// Write the header and the payload</span></span><br><span class=\"line\">  Status s = dest_-&gt;Append(Slice(buf, kHeaderSize));</span><br><span class=\"line\">  <span class=\"hljs-keyword\">if</span> (s.ok()) {</span><br><span class=\"line\">    s = dest_-&gt;Append(Slice(ptr, n));</span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (s.ok()) {</span><br><span class=\"line\">      s = dest_-&gt;Flush();</span><br><span class=\"line\">    }</span><br><span class=\"line\">  }</span><br><span class=\"line\">  block_offset_ += kHeaderSize + n;</span><br><span class=\"line\">  <span class=\"hljs-keyword\">return</span> s;</span><br><span class=\"line\">}</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>The block size of log file is designed  to 32KB intentionally. When leveldb tries to append a record to the tail of the log file, it must take block size into consideration.</p>\n<p></p><figure class=\"highlight c++ hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-keyword\">static</span> <span class=\"hljs-keyword\">const</span> <span class=\"hljs-keyword\">int</span> kBlockSize = <span class=\"hljs-number\">32768</span>;</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>There are several cases to handle when appending a record:</p>\n<ul>\n<li><p>The remaining space inside a block cannot even accommodate the header(kHeaderSize = 7 bytes)</p>\n</li>\n<li><p>The remainning space inside a block can accommodate  the whole record</p>\n</li>\n<li><p>The whole record consumes space across two continuous blocks</p>\n</li>\n<li><p>The whole record consumes space across multiple(at least three) continuous blocks</p>\n</li>\n</ul>\n<p>Now, let&#x2019;s talk about the first case: <strong>The remaining space inside a block cannot even accommodate the header(kHeaderSize = 7 bytes)</strong>.</p>\n<p>Because each record must begin with a header, so when the remaning space inside a block is not enough to handle the header, the remaning space will be just padding with dummy data and the next continuous block will be chosen as an operating block.</p>\n<p></p><figure class=\"highlight c++ hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-keyword\">const</span> <span class=\"hljs-keyword\">int</span> leftover = kBlockSize - block_offset_;</span><br><span class=\"line\">assert(leftover &gt;= <span class=\"hljs-number\">0</span>);</span><br><span class=\"line\"><span class=\"hljs-keyword\">if</span> (leftover &lt; kHeaderSize) {</span><br><span class=\"line\">\t<span class=\"hljs-comment\">// Switch to a new block</span></span><br><span class=\"line\">  <span class=\"hljs-keyword\">if</span> (leftover &gt; <span class=\"hljs-number\">0</span>) {</span><br><span class=\"line\">  <span class=\"hljs-comment\">// Fill the trailer (literal below relies on kHeaderSize being 7)</span></span><br><span class=\"line\">  assert(kHeaderSize == <span class=\"hljs-number\">7</span>);</span><br><span class=\"line\">  dest_-&gt;Append(Slice(<span class=\"hljs-string\">&quot;\\x00\\x00\\x00\\x00\\x00\\x00&quot;</span>, leftover));</span><br><span class=\"line\">  }</span><br><span class=\"line\">  block_offset_ = <span class=\"hljs-number\">0</span>;</span><br><span class=\"line\">}</span><br></pre></td></tr></tbody></table></figure><p></p>\n<img src=\"/2020/05/31/How-leveldb-log-works/record_not_for_header.png\" class title=\"image of dummy data\">\n\n\n\n<p>Obviously, the new block will begin to handle the record.</p>\n<p>Second case: <strong>The remaining space inside a block can accommodate  the whole record</strong>. In this case, a record with type <code>kFullType</code> will be appended to this block and the payload field contains all the data.</p>\n<img src=\"/2020/05/31/How-leveldb-log-works/record_within_one_block.png\" class title=\"image of full record\">\n\n\n\n<p>Third case: <strong>The whole record consumes space across two continuous blocks</strong>. The record will be splitted into two subrecords, the first subrecord is <code>kFirstType</code> type and the second subrecord is <code>kLastType</code> type.</p>\n<img src=\"/2020/05/31/How-leveldb-log-works/record_across_two_blocks.png\" class title=\"image of across two blocks\">\n\n\n\n<p>Fourth case: <strong>The whole record consumes space across multiple(at least three) continuous blocks</strong>. The record will be splitted into multiple subrecords. The first subrecord is <code>kFirstType</code> type and the last subrecord is <code>kLastType</code> type, all the other subrecords are <code>kMiddleType</code> type.</p>\n<img src=\"/2020/05/31/How-leveldb-log-works/record_across_multipe_blocks.png\" class title=\"image of across multiple blocks\">\n\n<p></p><figure class=\"highlight c++ hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-function\">Status <span class=\"hljs-title\">Writer::AddRecord</span><span class=\"hljs-params\">(<span class=\"hljs-keyword\">const</span> Slice&amp; slice)</span> </span>{</span><br><span class=\"line\">  <span class=\"hljs-keyword\">const</span> <span class=\"hljs-keyword\">char</span>* ptr = slice.data();</span><br><span class=\"line\">  <span class=\"hljs-keyword\">size_t</span> left = slice.<span class=\"hljs-built_in\">size</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-comment\">// Fragment the record if necessary and emit it.  Note that if slice</span></span><br><span class=\"line\">  <span class=\"hljs-comment\">// is empty, we still want to iterate once to emit a single</span></span><br><span class=\"line\">  <span class=\"hljs-comment\">// zero-length record</span></span><br><span class=\"line\">  Status s;</span><br><span class=\"line\">  <span class=\"hljs-keyword\">bool</span> <span class=\"hljs-built_in\">begin</span> = <span class=\"hljs-literal\">true</span>;</span><br><span class=\"line\">  <span class=\"hljs-keyword\">do</span> {</span><br><span class=\"line\">    <span class=\"hljs-keyword\">const</span> <span class=\"hljs-keyword\">int</span> leftover = kBlockSize - block_offset_;</span><br><span class=\"line\">    assert(leftover &gt;= <span class=\"hljs-number\">0</span>);</span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (leftover &lt; kHeaderSize) {</span><br><span class=\"line\">      <span class=\"hljs-comment\">// Switch to a new block</span></span><br><span class=\"line\">      <span class=\"hljs-keyword\">if</span> (leftover &gt; <span class=\"hljs-number\">0</span>) {</span><br><span class=\"line\">        <span class=\"hljs-comment\">// Fill the trailer (literal below relies on kHeaderSize being 7)</span></span><br><span class=\"line\">        assert(kHeaderSize == <span class=\"hljs-number\">7</span>);</span><br><span class=\"line\">        dest_-&gt;Append(Slice(<span class=\"hljs-string\">&quot;\\x00\\x00\\x00\\x00\\x00\\x00&quot;</span>, leftover));</span><br><span class=\"line\">      }</span><br><span class=\"line\">      block_offset_ = <span class=\"hljs-number\">0</span>;</span><br><span class=\"line\">    }</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"hljs-comment\">// Invariant: we never leave &lt; kHeaderSize bytes in a block.</span></span><br><span class=\"line\">    assert(kBlockSize - block_offset_ - kHeaderSize &gt;= <span class=\"hljs-number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"hljs-keyword\">const</span> <span class=\"hljs-keyword\">size_t</span> avail = kBlockSize - block_offset_ - kHeaderSize;</span><br><span class=\"line\">    <span class=\"hljs-keyword\">const</span> <span class=\"hljs-keyword\">size_t</span> fragment_length = (left &lt; avail) ? left : avail;</span><br><span class=\"line\"></span><br><span class=\"line\">    RecordType type;</span><br><span class=\"line\">    <span class=\"hljs-keyword\">const</span> <span class=\"hljs-keyword\">bool</span> <span class=\"hljs-built_in\">end</span> = (left == fragment_length);</span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (<span class=\"hljs-built_in\">begin</span> &amp;&amp; <span class=\"hljs-built_in\">end</span>) {</span><br><span class=\"line\">      type = kFullType;</span><br><span class=\"line\">    } <span class=\"hljs-keyword\">else</span> <span class=\"hljs-keyword\">if</span> (<span class=\"hljs-built_in\">begin</span>) {</span><br><span class=\"line\">      type = kFirstType;</span><br><span class=\"line\">    } <span class=\"hljs-keyword\">else</span> <span class=\"hljs-keyword\">if</span> (<span class=\"hljs-built_in\">end</span>) {</span><br><span class=\"line\">      type = kLastType;</span><br><span class=\"line\">    } <span class=\"hljs-keyword\">else</span> {</span><br><span class=\"line\">      type = kMiddleType;</span><br><span class=\"line\">    }</span><br><span class=\"line\"></span><br><span class=\"line\">    s = EmitPhysicalRecord(type, ptr, fragment_length);</span><br><span class=\"line\">    ptr += fragment_length;</span><br><span class=\"line\">    left -= fragment_length;</span><br><span class=\"line\">    <span class=\"hljs-built_in\">begin</span> = <span class=\"hljs-literal\">false</span>;</span><br><span class=\"line\">  } <span class=\"hljs-keyword\">while</span> (s.ok() &amp;&amp; left &gt; <span class=\"hljs-number\">0</span>);</span><br><span class=\"line\">  <span class=\"hljs-keyword\">return</span> s;</span><br><span class=\"line\">}</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>In summary, <strong>no physical record on the disk is allowed to persist across blocks.</strong></p>\n<h3 id=\"Log-Recovery\"><a href=\"#Log-Recovery\" class=\"headerlink\" title=\"Log Recovery\"></a>Log Recovery</h3><p>Leveldb will call <code>DBImpl::Recover</code> to try to recover data from the log file when a database is opened each time. Function <code>DBImpl::Recover</code> replays all existing log files with the help of function <code>DBImpl::RecoverLogFile</code> in chronological order.</p>\n<p></p><figure class=\"highlight c++ hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-comment\">// Recover from all newer log files than the ones named in the</span></span><br><span class=\"line\">  <span class=\"hljs-comment\">// descriptor (new log files may have been added by the previous</span></span><br><span class=\"line\">  <span class=\"hljs-comment\">// incarnation without registering them in the descriptor).</span></span><br><span class=\"line\">  <span class=\"hljs-comment\">//</span></span><br><span class=\"line\">  <span class=\"hljs-comment\">// Note that PrevLogNumber() is no longer used, but we pay</span></span><br><span class=\"line\">  <span class=\"hljs-comment\">// attention to it in case we are recovering a database</span></span><br><span class=\"line\">  <span class=\"hljs-comment\">// produced by an older version of leveldb.</span></span><br><span class=\"line\">  <span class=\"hljs-keyword\">const</span> <span class=\"hljs-keyword\">uint64_t</span> min_log = versions_-&gt;LogNumber();</span><br><span class=\"line\">  <span class=\"hljs-keyword\">const</span> <span class=\"hljs-keyword\">uint64_t</span> prev_log = versions_-&gt;PrevLogNumber();</span><br><span class=\"line\">  <span class=\"hljs-built_in\">std</span>::<span class=\"hljs-built_in\">vector</span>&lt;<span class=\"hljs-built_in\">std</span>::<span class=\"hljs-built_in\">string</span>&gt; filenames;</span><br><span class=\"line\">  s = env_-&gt;GetChildren(dbname_, &amp;filenames);</span><br><span class=\"line\">  <span class=\"hljs-keyword\">if</span> (!s.ok()) {</span><br><span class=\"line\">    <span class=\"hljs-keyword\">return</span> s;</span><br><span class=\"line\">  }</span><br><span class=\"line\">  <span class=\"hljs-built_in\">std</span>::<span class=\"hljs-built_in\">set</span>&lt;<span class=\"hljs-keyword\">uint64_t</span>&gt; expected;</span><br><span class=\"line\">  versions_-&gt;AddLiveFiles(&amp;expected);</span><br><span class=\"line\">  <span class=\"hljs-keyword\">uint64_t</span> number;</span><br><span class=\"line\">  FileType type;</span><br><span class=\"line\">  <span class=\"hljs-built_in\">std</span>::<span class=\"hljs-built_in\">vector</span>&lt;<span class=\"hljs-keyword\">uint64_t</span>&gt; logs;</span><br><span class=\"line\">  <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-keyword\">size_t</span> i = <span class=\"hljs-number\">0</span>; i &lt; filenames.<span class=\"hljs-built_in\">size</span>(); i++) {</span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (ParseFileName(filenames[i], &amp;number, &amp;type)) {</span><br><span class=\"line\">      expected.erase(number);</span><br><span class=\"line\">      <span class=\"hljs-keyword\">if</span> (type == kLogFile &amp;&amp; ((number &gt;= min_log) || (number == prev_log)))</span><br><span class=\"line\">        logs.push_back(number);</span><br><span class=\"line\">    }</span><br><span class=\"line\">  }</span><br><span class=\"line\">  <span class=\"hljs-keyword\">if</span> (!expected.empty()) {</span><br><span class=\"line\">    <span class=\"hljs-keyword\">char</span> buf[<span class=\"hljs-number\">50</span>];</span><br><span class=\"line\">    <span class=\"hljs-built_in\">snprintf</span>(buf, <span class=\"hljs-keyword\">sizeof</span>(buf), <span class=\"hljs-string\">&quot;%d missing files; e.g.&quot;</span>,</span><br><span class=\"line\">             <span class=\"hljs-keyword\">static_cast</span>&lt;<span class=\"hljs-keyword\">int</span>&gt;(expected.<span class=\"hljs-built_in\">size</span>()));</span><br><span class=\"line\">    <span class=\"hljs-keyword\">return</span> Status::Corruption(buf, TableFileName(dbname_, *(expected.<span class=\"hljs-built_in\">begin</span>())));</span><br><span class=\"line\">  }</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-comment\">// Recover in the order in which the logs were generated</span></span><br><span class=\"line\">  <span class=\"hljs-built_in\">std</span>::sort(logs.<span class=\"hljs-built_in\">begin</span>(), logs.<span class=\"hljs-built_in\">end</span>());</span><br><span class=\"line\">  <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-keyword\">size_t</span> i = <span class=\"hljs-number\">0</span>; i &lt; logs.<span class=\"hljs-built_in\">size</span>(); i++) {</span><br><span class=\"line\">    s = RecoverLogFile(logs[i], (i == logs.<span class=\"hljs-built_in\">size</span>() - <span class=\"hljs-number\">1</span>), save_manifest, edit,</span><br><span class=\"line\">                       &amp;max_sequence);</span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (!s.ok()) {</span><br><span class=\"line\">      <span class=\"hljs-keyword\">return</span> s;</span><br><span class=\"line\">    }</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"hljs-comment\">// The previous incarnation may not have written any MANIFEST</span></span><br><span class=\"line\">    <span class=\"hljs-comment\">// records after allocating this log number.  So we manually</span></span><br><span class=\"line\">    <span class=\"hljs-comment\">// update the file number allocation counter in VersionSet.</span></span><br><span class=\"line\">    versions_-&gt;MarkFileNumberUsed(logs[i]);</span><br><span class=\"line\">  }</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>In function <code>DBImpl::RecoverLogFile</code>, it will read the log file sequentially from beginning to end.</p>\n<p></p><figure class=\"highlight c++ hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-function\">Status <span class=\"hljs-title\">DBImpl::RecoverLogFile</span><span class=\"hljs-params\">(<span class=\"hljs-keyword\">uint64_t</span> log_number, <span class=\"hljs-keyword\">bool</span> last_log,</span></span></span><br><span class=\"line\"><span class=\"hljs-function\"><span class=\"hljs-params\">                              <span class=\"hljs-keyword\">bool</span>* save_manifest, VersionEdit* edit,</span></span></span><br><span class=\"line\"><span class=\"hljs-function\"><span class=\"hljs-params\">                              SequenceNumber* max_sequence)</span> </span>{</span><br><span class=\"line\">  <span class=\"hljs-class\"><span class=\"hljs-keyword\">struct</span> <span class=\"hljs-title\">LogReporter</span> :</span> <span class=\"hljs-keyword\">public</span> <span class=\"hljs-built_in\">log</span>::Reader::Reporter {</span><br><span class=\"line\">    Env* env;</span><br><span class=\"line\">    Logger* info_log;</span><br><span class=\"line\">    <span class=\"hljs-keyword\">const</span> <span class=\"hljs-keyword\">char</span>* fname;</span><br><span class=\"line\">    Status* status;  <span class=\"hljs-comment\">// NULL if options_.paranoid_checks==false</span></span><br><span class=\"line\">    <span class=\"hljs-function\"><span class=\"hljs-keyword\">virtual</span> <span class=\"hljs-keyword\">void</span> <span class=\"hljs-title\">Corruption</span><span class=\"hljs-params\">(<span class=\"hljs-keyword\">size_t</span> bytes, <span class=\"hljs-keyword\">const</span> Status&amp; s)</span> </span>{</span><br><span class=\"line\">      Log(info_log, <span class=\"hljs-string\">&quot;%s%s: dropping %d bytes; %s&quot;</span>,</span><br><span class=\"line\">          (<span class=\"hljs-keyword\">this</span>-&gt;status == <span class=\"hljs-literal\">NULL</span> ? <span class=\"hljs-string\">&quot;(ignoring error) &quot;</span> : <span class=\"hljs-string\">&quot;&quot;</span>),</span><br><span class=\"line\">          fname, <span class=\"hljs-keyword\">static_cast</span>&lt;<span class=\"hljs-keyword\">int</span>&gt;(bytes), s.ToString().c_str());</span><br><span class=\"line\">      <span class=\"hljs-keyword\">if</span> (<span class=\"hljs-keyword\">this</span>-&gt;status != <span class=\"hljs-literal\">NULL</span> &amp;&amp; <span class=\"hljs-keyword\">this</span>-&gt;status-&gt;ok()) *<span class=\"hljs-keyword\">this</span>-&gt;status = s;</span><br><span class=\"line\">    }</span><br><span class=\"line\">  };</span><br><span class=\"line\"></span><br><span class=\"line\">  mutex_.AssertHeld();</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-comment\">// Open the log file</span></span><br><span class=\"line\">  <span class=\"hljs-built_in\">std</span>::<span class=\"hljs-built_in\">string</span> fname = LogFileName(dbname_, log_number);</span><br><span class=\"line\">  SequentialFile* file;</span><br><span class=\"line\">  Status status = env_-&gt;NewSequentialFile(fname, &amp;file);</span><br><span class=\"line\">  <span class=\"hljs-keyword\">if</span> (!status.ok()) {</span><br><span class=\"line\">    MaybeIgnoreError(&amp;status);</span><br><span class=\"line\">    <span class=\"hljs-keyword\">return</span> status;</span><br><span class=\"line\">  }</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-comment\">// Create the log reader.</span></span><br><span class=\"line\">  LogReporter reporter;</span><br><span class=\"line\">  reporter.env = env_;</span><br><span class=\"line\">  reporter.info_log = options_.info_log;</span><br><span class=\"line\">  reporter.fname = fname.c_str();</span><br><span class=\"line\">  reporter.status = (options_.paranoid_checks ? &amp;status : <span class=\"hljs-literal\">NULL</span>);</span><br><span class=\"line\">  <span class=\"hljs-comment\">// We intentionally make log::Reader do checksumming even if</span></span><br><span class=\"line\">  <span class=\"hljs-comment\">// paranoid_checks==false so that corruptions cause entire commits</span></span><br><span class=\"line\">  <span class=\"hljs-comment\">// to be skipped instead of propagating bad information (like overly</span></span><br><span class=\"line\">  <span class=\"hljs-comment\">// large sequence numbers).</span></span><br><span class=\"line\">  <span class=\"hljs-function\"><span class=\"hljs-built_in\">log</span>::Reader <span class=\"hljs-title\">reader</span><span class=\"hljs-params\">(file, &amp;reporter, <span class=\"hljs-literal\">true</span><span class=\"hljs-comment\">/*checksum*/</span>,</span></span></span><br><span class=\"line\"><span class=\"hljs-function\"><span class=\"hljs-params\">                     <span class=\"hljs-number\">0</span><span class=\"hljs-comment\">/*initial_offset*/</span>)</span></span>;</span><br><span class=\"line\">  Log(options_.info_log, <span class=\"hljs-string\">&quot;Recovering log #%llu&quot;</span>,</span><br><span class=\"line\">      (<span class=\"hljs-keyword\">unsigned</span> <span class=\"hljs-keyword\">long</span> <span class=\"hljs-keyword\">long</span>) log_number);</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-comment\">// Read all the records and add to a memtable</span></span><br><span class=\"line\">  <span class=\"hljs-built_in\">std</span>::<span class=\"hljs-built_in\">string</span> scratch;</span><br><span class=\"line\">  Slice record;</span><br><span class=\"line\">  WriteBatch batch;</span><br><span class=\"line\">  <span class=\"hljs-keyword\">int</span> compactions = <span class=\"hljs-number\">0</span>;</span><br><span class=\"line\">  MemTable* mem = <span class=\"hljs-literal\">NULL</span>;</span><br><span class=\"line\">  <span class=\"hljs-keyword\">while</span> (reader.ReadRecord(&amp;record, &amp;scratch) &amp;&amp;</span><br><span class=\"line\">         status.ok()) {</span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (record.<span class=\"hljs-built_in\">size</span>() &lt; <span class=\"hljs-number\">12</span>) {</span><br><span class=\"line\">      reporter.Corruption(</span><br><span class=\"line\">          record.<span class=\"hljs-built_in\">size</span>(), Status::Corruption(<span class=\"hljs-string\">&quot;log record too small&quot;</span>));</span><br><span class=\"line\">      <span class=\"hljs-keyword\">continue</span>;</span><br><span class=\"line\">    }</span><br><span class=\"line\">    WriteBatchInternal::SetContents(&amp;batch, record);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (mem == <span class=\"hljs-literal\">NULL</span>) {</span><br><span class=\"line\">      mem = <span class=\"hljs-keyword\">new</span> MemTable(internal_comparator_);</span><br><span class=\"line\">      mem-&gt;Ref();</span><br><span class=\"line\">    }</span><br><span class=\"line\">    status = WriteBatchInternal::InsertInto(&amp;batch, mem);</span><br><span class=\"line\">    MaybeIgnoreError(&amp;status);</span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (!status.ok()) {</span><br><span class=\"line\">      <span class=\"hljs-keyword\">break</span>;</span><br><span class=\"line\">    }</span><br><span class=\"line\">    <span class=\"hljs-keyword\">const</span> SequenceNumber last_seq =</span><br><span class=\"line\">        WriteBatchInternal::Sequence(&amp;batch) +</span><br><span class=\"line\">        WriteBatchInternal::Count(&amp;batch) - <span class=\"hljs-number\">1</span>;</span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (last_seq &gt; *max_sequence) {</span><br><span class=\"line\">      *max_sequence = last_seq;</span><br><span class=\"line\">    }</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (mem-&gt;ApproximateMemoryUsage() &gt; options_.write_buffer_size) {</span><br><span class=\"line\">      compactions++;</span><br><span class=\"line\">      *save_manifest = <span class=\"hljs-literal\">true</span>;</span><br><span class=\"line\">      status = WriteLevel0Table(mem, edit, <span class=\"hljs-literal\">NULL</span>);</span><br><span class=\"line\">      mem-&gt;Unref();</span><br><span class=\"line\">      mem = <span class=\"hljs-literal\">NULL</span>;</span><br><span class=\"line\">      <span class=\"hljs-keyword\">if</span> (!status.ok()) {</span><br><span class=\"line\">        <span class=\"hljs-comment\">// Reflect errors immediately so that conditions like full</span></span><br><span class=\"line\">        <span class=\"hljs-comment\">// file-systems cause the DB::Open() to fail.</span></span><br><span class=\"line\">        <span class=\"hljs-keyword\">break</span>;</span><br><span class=\"line\">      }</span><br><span class=\"line\">    }</span><br><span class=\"line\">  }</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-keyword\">delete</span> file;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-comment\">// See if we should keep reusing the last log file.</span></span><br><span class=\"line\">  <span class=\"hljs-keyword\">if</span> (status.ok() &amp;&amp; options_.reuse_logs &amp;&amp; last_log &amp;&amp; compactions == <span class=\"hljs-number\">0</span>) {</span><br><span class=\"line\">    assert(logfile_ == <span class=\"hljs-literal\">NULL</span>);</span><br><span class=\"line\">    assert(log_ == <span class=\"hljs-literal\">NULL</span>);</span><br><span class=\"line\">    assert(mem_ == <span class=\"hljs-literal\">NULL</span>);</span><br><span class=\"line\">    <span class=\"hljs-keyword\">uint64_t</span> lfile_size;</span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (env_-&gt;GetFileSize(fname, &amp;lfile_size).ok() &amp;&amp;</span><br><span class=\"line\">        env_-&gt;NewAppendableFile(fname, &amp;logfile_).ok()) {</span><br><span class=\"line\">      Log(options_.info_log, <span class=\"hljs-string\">&quot;Reusing old log %s \\n&quot;</span>, fname.c_str());</span><br><span class=\"line\">      log_ = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-built_in\">log</span>::Writer(logfile_, lfile_size);</span><br><span class=\"line\">      logfile_number_ = log_number;</span><br><span class=\"line\">      <span class=\"hljs-keyword\">if</span> (mem != <span class=\"hljs-literal\">NULL</span>) {</span><br><span class=\"line\">        mem_ = mem;</span><br><span class=\"line\">        mem = <span class=\"hljs-literal\">NULL</span>;</span><br><span class=\"line\">      } <span class=\"hljs-keyword\">else</span> {</span><br><span class=\"line\">        <span class=\"hljs-comment\">// mem can be NULL if lognum exists but was empty.</span></span><br><span class=\"line\">        mem_ = <span class=\"hljs-keyword\">new</span> MemTable(internal_comparator_);</span><br><span class=\"line\">        mem_-&gt;Ref();</span><br><span class=\"line\">      }</span><br><span class=\"line\">    }</span><br><span class=\"line\">  }</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-keyword\">if</span> (mem != <span class=\"hljs-literal\">NULL</span>) {</span><br><span class=\"line\">    <span class=\"hljs-comment\">// mem did not get reused; compact it.</span></span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (status.ok()) {</span><br><span class=\"line\">      *save_manifest = <span class=\"hljs-literal\">true</span>;</span><br><span class=\"line\">      status = WriteLevel0Table(mem, edit, <span class=\"hljs-literal\">NULL</span>);</span><br><span class=\"line\">    }</span><br><span class=\"line\">    mem-&gt;Unref();</span><br><span class=\"line\">  }</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-keyword\">return</span> status;</span><br><span class=\"line\">}</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>In function <code>Reader::ReadRecord</code>, there are several important variables:</p>\n<ul>\n<li><code>initial_offset_</code>: Offset at which to start looking for the first record to return. It is initialized in the Reader constructor function</li>\n<li><code>last_record_offset_</code>: Offset of the last record returned by ReadRecord</li>\n</ul>\n<p>If <code>last_record_offset_</code> is less than <code>initial_offset_</code>, it will call <code>Reader::ReadRecord</code> to go to the correct block position to read the next record:</p>\n<p></p><figure class=\"highlight c++ hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">bool</span> <span class=\"hljs-title\">Reader::SkipToInitialBlock</span><span class=\"hljs-params\">()</span> </span>{</span><br><span class=\"line\">  <span class=\"hljs-keyword\">size_t</span> offset_in_block = initial_offset_ % kBlockSize;</span><br><span class=\"line\">  <span class=\"hljs-keyword\">uint64_t</span> block_start_location = initial_offset_ - offset_in_block;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-comment\">// Don&apos;t search a block if we&apos;d be in the trailer</span></span><br><span class=\"line\">  <span class=\"hljs-keyword\">if</span> (offset_in_block &gt; kBlockSize - <span class=\"hljs-number\">6</span>) {</span><br><span class=\"line\">    offset_in_block = <span class=\"hljs-number\">0</span>;</span><br><span class=\"line\">    block_start_location += kBlockSize;</span><br><span class=\"line\">  }</span><br><span class=\"line\"></span><br><span class=\"line\">  end_of_buffer_offset_ = block_start_location;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-comment\">// Skip to start of first block that can contain the initial record</span></span><br><span class=\"line\">  <span class=\"hljs-keyword\">if</span> (block_start_location &gt; <span class=\"hljs-number\">0</span>) {</span><br><span class=\"line\">    Status skip_status = file_-&gt;Skip(block_start_location);</span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (!skip_status.ok()) {</span><br><span class=\"line\">      ReportDrop(block_start_location, skip_status);</span><br><span class=\"line\">      <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">false</span>;</span><br><span class=\"line\">    }</span><br><span class=\"line\">  }</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">true</span>;</span><br><span class=\"line\">}</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>As we see, a logical record maybe splitted into multiple subrecords. So there is another function named <code>Reader::ReadPhysicalRecord</code> to read the physical record on disk. </p>\n<p></p><figure class=\"highlight c++ hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">unsigned</span> <span class=\"hljs-keyword\">int</span> <span class=\"hljs-title\">Reader::ReadPhysicalRecord</span><span class=\"hljs-params\">(Slice* result)</span> </span>{</span><br><span class=\"line\">  <span class=\"hljs-keyword\">while</span> (<span class=\"hljs-literal\">true</span>) {</span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (buffer_.<span class=\"hljs-built_in\">size</span>() &lt; kHeaderSize) {</span><br><span class=\"line\">      <span class=\"hljs-keyword\">if</span> (!eof_) {</span><br><span class=\"line\">        <span class=\"hljs-comment\">// Last read was a full read, so this is a trailer to skip</span></span><br><span class=\"line\">        buffer_.<span class=\"hljs-built_in\">clear</span>();</span><br><span class=\"line\">        Status status = file_-&gt;Read(kBlockSize, &amp;buffer_, backing_store_);</span><br><span class=\"line\">        end_of_buffer_offset_ += buffer_.<span class=\"hljs-built_in\">size</span>();</span><br><span class=\"line\">        <span class=\"hljs-keyword\">if</span> (!status.ok()) {</span><br><span class=\"line\">          buffer_.<span class=\"hljs-built_in\">clear</span>();</span><br><span class=\"line\">          ReportDrop(kBlockSize, status);</span><br><span class=\"line\">          eof_ = <span class=\"hljs-literal\">true</span>;</span><br><span class=\"line\">          <span class=\"hljs-keyword\">return</span> kEof;</span><br><span class=\"line\">        } <span class=\"hljs-keyword\">else</span> <span class=\"hljs-keyword\">if</span> (buffer_.<span class=\"hljs-built_in\">size</span>() &lt; kBlockSize) {</span><br><span class=\"line\">          eof_ = <span class=\"hljs-literal\">true</span>;</span><br><span class=\"line\">        }</span><br><span class=\"line\">        <span class=\"hljs-keyword\">continue</span>;</span><br><span class=\"line\">      } <span class=\"hljs-keyword\">else</span> {</span><br><span class=\"line\">        <span class=\"hljs-comment\">// Note that if buffer_ is non-empty, we have a truncated header at the</span></span><br><span class=\"line\">        <span class=\"hljs-comment\">// end of the file, which can be caused by the writer crashing in the</span></span><br><span class=\"line\">        <span class=\"hljs-comment\">// middle of writing the header. Instead of considering this an error,</span></span><br><span class=\"line\">        <span class=\"hljs-comment\">// just report EOF.</span></span><br><span class=\"line\">        buffer_.<span class=\"hljs-built_in\">clear</span>();</span><br><span class=\"line\">        <span class=\"hljs-keyword\">return</span> kEof;</span><br><span class=\"line\">      }</span><br><span class=\"line\">    }</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"hljs-comment\">// Parse the header</span></span><br><span class=\"line\">    <span class=\"hljs-keyword\">const</span> <span class=\"hljs-keyword\">char</span>* header = buffer_.data();</span><br><span class=\"line\">    <span class=\"hljs-keyword\">const</span> <span class=\"hljs-keyword\">uint32_t</span> a = <span class=\"hljs-keyword\">static_cast</span>&lt;<span class=\"hljs-keyword\">uint32_t</span>&gt;(header[<span class=\"hljs-number\">4</span>]) &amp; <span class=\"hljs-number\">0xff</span>;</span><br><span class=\"line\">    <span class=\"hljs-keyword\">const</span> <span class=\"hljs-keyword\">uint32_t</span> b = <span class=\"hljs-keyword\">static_cast</span>&lt;<span class=\"hljs-keyword\">uint32_t</span>&gt;(header[<span class=\"hljs-number\">5</span>]) &amp; <span class=\"hljs-number\">0xff</span>;</span><br><span class=\"line\">    <span class=\"hljs-keyword\">const</span> <span class=\"hljs-keyword\">unsigned</span> <span class=\"hljs-keyword\">int</span> type = header[<span class=\"hljs-number\">6</span>];</span><br><span class=\"line\">    <span class=\"hljs-keyword\">const</span> <span class=\"hljs-keyword\">uint32_t</span> length = a | (b &lt;&lt; <span class=\"hljs-number\">8</span>);</span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (kHeaderSize + length &gt; buffer_.<span class=\"hljs-built_in\">size</span>()) {</span><br><span class=\"line\">      <span class=\"hljs-keyword\">size_t</span> drop_size = buffer_.<span class=\"hljs-built_in\">size</span>();</span><br><span class=\"line\">      buffer_.<span class=\"hljs-built_in\">clear</span>();</span><br><span class=\"line\">      <span class=\"hljs-keyword\">if</span> (!eof_) {</span><br><span class=\"line\">        ReportCorruption(drop_size, <span class=\"hljs-string\">&quot;bad record length&quot;</span>);</span><br><span class=\"line\">        <span class=\"hljs-keyword\">return</span> kBadRecord;</span><br><span class=\"line\">      }</span><br><span class=\"line\">      <span class=\"hljs-comment\">// If the end of the file has been reached without reading |length| bytes</span></span><br><span class=\"line\">      <span class=\"hljs-comment\">// of payload, assume the writer died in the middle of writing the record.</span></span><br><span class=\"line\">      <span class=\"hljs-comment\">// Don&apos;t report a corruption.</span></span><br><span class=\"line\">      <span class=\"hljs-keyword\">return</span> kEof;</span><br><span class=\"line\">    }</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (type == kZeroType &amp;&amp; length == <span class=\"hljs-number\">0</span>) {</span><br><span class=\"line\">      <span class=\"hljs-comment\">// Skip zero length record without reporting any drops since</span></span><br><span class=\"line\">      <span class=\"hljs-comment\">// such records are produced by the mmap based writing code in</span></span><br><span class=\"line\">      <span class=\"hljs-comment\">// env_posix.cc that preallocates file regions.</span></span><br><span class=\"line\">      buffer_.<span class=\"hljs-built_in\">clear</span>();</span><br><span class=\"line\">      <span class=\"hljs-keyword\">return</span> kBadRecord;</span><br><span class=\"line\">    }</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"hljs-comment\">// Check crc</span></span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (checksum_) {</span><br><span class=\"line\">      <span class=\"hljs-keyword\">uint32_t</span> expected_crc = crc32c::Unmask(DecodeFixed32(header));</span><br><span class=\"line\">      <span class=\"hljs-keyword\">uint32_t</span> actual_crc = crc32c::Value(header + <span class=\"hljs-number\">6</span>, <span class=\"hljs-number\">1</span> + length);</span><br><span class=\"line\">      <span class=\"hljs-keyword\">if</span> (actual_crc != expected_crc) {</span><br><span class=\"line\">        <span class=\"hljs-comment\">// Drop the rest of the buffer since &quot;length&quot; itself may have</span></span><br><span class=\"line\">        <span class=\"hljs-comment\">// been corrupted and if we trust it, we could find some</span></span><br><span class=\"line\">        <span class=\"hljs-comment\">// fragment of a real log record that just happens to look</span></span><br><span class=\"line\">        <span class=\"hljs-comment\">// like a valid log record.</span></span><br><span class=\"line\">        <span class=\"hljs-keyword\">size_t</span> drop_size = buffer_.<span class=\"hljs-built_in\">size</span>();</span><br><span class=\"line\">        buffer_.<span class=\"hljs-built_in\">clear</span>();</span><br><span class=\"line\">        ReportCorruption(drop_size, <span class=\"hljs-string\">&quot;checksum mismatch&quot;</span>);</span><br><span class=\"line\">        <span class=\"hljs-keyword\">return</span> kBadRecord;</span><br><span class=\"line\">      }</span><br><span class=\"line\">    }</span><br><span class=\"line\"></span><br><span class=\"line\">    buffer_.remove_prefix(kHeaderSize + length);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"hljs-comment\">// Skip physical record that started before initial_offset_</span></span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (end_of_buffer_offset_ - buffer_.<span class=\"hljs-built_in\">size</span>() - kHeaderSize - length &lt;</span><br><span class=\"line\">        initial_offset_) {</span><br><span class=\"line\">      result-&gt;<span class=\"hljs-built_in\">clear</span>();</span><br><span class=\"line\">      <span class=\"hljs-keyword\">return</span> kBadRecord;</span><br><span class=\"line\">    }</span><br><span class=\"line\"></span><br><span class=\"line\">    *result = Slice(header + kHeaderSize, length);</span><br><span class=\"line\">    <span class=\"hljs-keyword\">return</span> type;</span><br><span class=\"line\">  }</span><br><span class=\"line\">}</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>In <code>Reader::ReadPhysicalRecord</code>, it will parse the record header to get crc value and length value. With this information, it can check whether the data is intact or not.</p>\n<p>And if multiple physical records belong to one logical record, the <code>Reader::ReadRecord</code> will combine them together and return to the upper layer:</p>\n<p></p><figure class=\"highlight c++ hljs\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-keyword\">while</span> (<span class=\"hljs-literal\">true</span>) {</span><br><span class=\"line\">    <span class=\"hljs-keyword\">const</span> <span class=\"hljs-keyword\">unsigned</span> <span class=\"hljs-keyword\">int</span> record_type = ReadPhysicalRecord(&amp;fragment);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"hljs-comment\">// ReadPhysicalRecord may have only had an empty trailer remaining in its</span></span><br><span class=\"line\">    <span class=\"hljs-comment\">// internal buffer. Calculate the offset of the next physical record now</span></span><br><span class=\"line\">    <span class=\"hljs-comment\">// that it has returned, properly accounting for its header size.</span></span><br><span class=\"line\">    <span class=\"hljs-keyword\">uint64_t</span> physical_record_offset =</span><br><span class=\"line\">        end_of_buffer_offset_ - buffer_.<span class=\"hljs-built_in\">size</span>() - kHeaderSize - fragment.<span class=\"hljs-built_in\">size</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"hljs-keyword\">if</span> (resyncing_) {</span><br><span class=\"line\">      <span class=\"hljs-keyword\">if</span> (record_type == kMiddleType) {</span><br><span class=\"line\">        <span class=\"hljs-keyword\">continue</span>;</span><br><span class=\"line\">      } <span class=\"hljs-keyword\">else</span> <span class=\"hljs-keyword\">if</span> (record_type == kLastType) {</span><br><span class=\"line\">        resyncing_ = <span class=\"hljs-literal\">false</span>;</span><br><span class=\"line\">        <span class=\"hljs-keyword\">continue</span>;</span><br><span class=\"line\">      } <span class=\"hljs-keyword\">else</span> {</span><br><span class=\"line\">        resyncing_ = <span class=\"hljs-literal\">false</span>;</span><br><span class=\"line\">      }</span><br><span class=\"line\">    }</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"hljs-keyword\">switch</span> (record_type) {</span><br><span class=\"line\">      <span class=\"hljs-keyword\">case</span> kFullType:</span><br><span class=\"line\">        <span class=\"hljs-keyword\">if</span> (in_fragmented_record) {</span><br><span class=\"line\">          <span class=\"hljs-comment\">// Handle bug in earlier versions of log::Writer where</span></span><br><span class=\"line\">          <span class=\"hljs-comment\">// it could emit an empty kFirstType record at the tail end</span></span><br><span class=\"line\">          <span class=\"hljs-comment\">// of a block followed by a kFullType or kFirstType record</span></span><br><span class=\"line\">          <span class=\"hljs-comment\">// at the beginning of the next block.</span></span><br><span class=\"line\">          <span class=\"hljs-keyword\">if</span> (scratch-&gt;empty()) {</span><br><span class=\"line\">            in_fragmented_record = <span class=\"hljs-literal\">false</span>;</span><br><span class=\"line\">          } <span class=\"hljs-keyword\">else</span> {</span><br><span class=\"line\">            ReportCorruption(scratch-&gt;<span class=\"hljs-built_in\">size</span>(), <span class=\"hljs-string\">&quot;partial record without end(1)&quot;</span>);</span><br><span class=\"line\">          }</span><br><span class=\"line\">        }</span><br><span class=\"line\">        prospective_record_offset = physical_record_offset;</span><br><span class=\"line\">        scratch-&gt;<span class=\"hljs-built_in\">clear</span>();</span><br><span class=\"line\">        *record = fragment;</span><br><span class=\"line\">        last_record_offset_ = prospective_record_offset;</span><br><span class=\"line\">        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">true</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"hljs-keyword\">case</span> kFirstType:</span><br><span class=\"line\">        <span class=\"hljs-keyword\">if</span> (in_fragmented_record) {</span><br><span class=\"line\">          <span class=\"hljs-comment\">// Handle bug in earlier versions of log::Writer where</span></span><br><span class=\"line\">          <span class=\"hljs-comment\">// it could emit an empty kFirstType record at the tail end</span></span><br><span class=\"line\">          <span class=\"hljs-comment\">// of a block followed by a kFullType or kFirstType record</span></span><br><span class=\"line\">          <span class=\"hljs-comment\">// at the beginning of the next block.</span></span><br><span class=\"line\">          <span class=\"hljs-keyword\">if</span> (scratch-&gt;empty()) {</span><br><span class=\"line\">            in_fragmented_record = <span class=\"hljs-literal\">false</span>;</span><br><span class=\"line\">          } <span class=\"hljs-keyword\">else</span> {</span><br><span class=\"line\">            ReportCorruption(scratch-&gt;<span class=\"hljs-built_in\">size</span>(), <span class=\"hljs-string\">&quot;partial record without end(2)&quot;</span>);</span><br><span class=\"line\">          }</span><br><span class=\"line\">        }</span><br><span class=\"line\">        prospective_record_offset = physical_record_offset;</span><br><span class=\"line\">        scratch-&gt;assign(fragment.data(), fragment.<span class=\"hljs-built_in\">size</span>());</span><br><span class=\"line\">        in_fragmented_record = <span class=\"hljs-literal\">true</span>;</span><br><span class=\"line\">        <span class=\"hljs-keyword\">break</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"hljs-keyword\">case</span> kMiddleType:</span><br><span class=\"line\">        <span class=\"hljs-keyword\">if</span> (!in_fragmented_record) {</span><br><span class=\"line\">          ReportCorruption(fragment.<span class=\"hljs-built_in\">size</span>(),</span><br><span class=\"line\">                           <span class=\"hljs-string\">&quot;missing start of fragmented record(1)&quot;</span>);</span><br><span class=\"line\">        } <span class=\"hljs-keyword\">else</span> {</span><br><span class=\"line\">          scratch-&gt;append(fragment.data(), fragment.<span class=\"hljs-built_in\">size</span>());</span><br><span class=\"line\">        }</span><br><span class=\"line\">        <span class=\"hljs-keyword\">break</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"hljs-keyword\">case</span> kLastType:</span><br><span class=\"line\">        <span class=\"hljs-keyword\">if</span> (!in_fragmented_record) {</span><br><span class=\"line\">          ReportCorruption(fragment.<span class=\"hljs-built_in\">size</span>(),</span><br><span class=\"line\">                           <span class=\"hljs-string\">&quot;missing start of fragmented record(2)&quot;</span>);</span><br><span class=\"line\">        } <span class=\"hljs-keyword\">else</span> {</span><br><span class=\"line\">          scratch-&gt;append(fragment.data(), fragment.<span class=\"hljs-built_in\">size</span>());</span><br><span class=\"line\">          *record = Slice(*scratch);</span><br><span class=\"line\">          last_record_offset_ = prospective_record_offset;</span><br><span class=\"line\">          <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">true</span>;</span><br><span class=\"line\">        }</span><br><span class=\"line\">        <span class=\"hljs-keyword\">break</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"hljs-keyword\">case</span> kEof:</span><br><span class=\"line\">        <span class=\"hljs-keyword\">if</span> (in_fragmented_record) {</span><br><span class=\"line\">          <span class=\"hljs-comment\">// This can be caused by the writer dying immediately after</span></span><br><span class=\"line\">          <span class=\"hljs-comment\">// writing a physical record but before completing the next; don&apos;t</span></span><br><span class=\"line\">          <span class=\"hljs-comment\">// treat it as a corruption, just ignore the entire logical record.</span></span><br><span class=\"line\">          scratch-&gt;<span class=\"hljs-built_in\">clear</span>();</span><br><span class=\"line\">        }</span><br><span class=\"line\">        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">false</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"hljs-keyword\">case</span> kBadRecord:</span><br><span class=\"line\">        <span class=\"hljs-keyword\">if</span> (in_fragmented_record) {</span><br><span class=\"line\">          ReportCorruption(scratch-&gt;<span class=\"hljs-built_in\">size</span>(), <span class=\"hljs-string\">&quot;error in middle of record&quot;</span>);</span><br><span class=\"line\">          in_fragmented_record = <span class=\"hljs-literal\">false</span>;</span><br><span class=\"line\">          scratch-&gt;<span class=\"hljs-built_in\">clear</span>();</span><br><span class=\"line\">        }</span><br><span class=\"line\">        <span class=\"hljs-keyword\">break</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"hljs-keyword\">default</span>: {</span><br><span class=\"line\">        <span class=\"hljs-keyword\">char</span> buf[<span class=\"hljs-number\">40</span>];</span><br><span class=\"line\">        <span class=\"hljs-built_in\">snprintf</span>(buf, <span class=\"hljs-keyword\">sizeof</span>(buf), <span class=\"hljs-string\">&quot;unknown record type %u&quot;</span>, record_type);</span><br><span class=\"line\">        ReportCorruption(</span><br><span class=\"line\">            (fragment.<span class=\"hljs-built_in\">size</span>() + (in_fragmented_record ? scratch-&gt;<span class=\"hljs-built_in\">size</span>() : <span class=\"hljs-number\">0</span>)),</span><br><span class=\"line\">            buf);</span><br><span class=\"line\">        in_fragmented_record = <span class=\"hljs-literal\">false</span>;</span><br><span class=\"line\">        scratch-&gt;<span class=\"hljs-built_in\">clear</span>();</span><br><span class=\"line\">        <span class=\"hljs-keyword\">break</span>;</span><br><span class=\"line\">      }</span><br><span class=\"line\">    }</span><br><span class=\"line\">  }</span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>From the code above, we know that in normal cases only when a record type of <code>kFullType</code> or <code>kLastType</code> is met can the function return to the upper layer.</p>\n<p>And the upper layer replays the whole operation based on the logical record information returned by <code>Reader::ReadRecord</code>. </p>\n<h3 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h3><p>This blog mainly talks about how log mechanism works in leveldb. Have fun<del>~</del></p>\n</body></html>","site":{"data":{}},"_categories":[],"_tags":[{"name":"database","path":"tags/database/"},{"name":"leveldb","path":"tags/leveldb/"}],"excerpt":"<html><head></head><body><h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>In leveldb, a log file (*.log) stores a sequence of recent updates. Each update is appended to the current log file. </p>\n<p>There are several benefits of log file for leveldb:</p>\n<ul>\n<li>Boosting performance by converting random write into sequential write automatically in the underlying hard drives</li>\n<li>Satisfying the atomicity and durability requirements of database properties</li>\n</ul>\n<p>When the user issues an update operation(put or delete), a corresponding record will be appended to the log file firstly. Only when the record is persisted successfully will a successful status be returned to the user.</p></body></html>","more":"<p>function <code>DBImpl::Write</code> is the critical path for all update operations. Inside this function, statement <code>status = log_-&gt;AddRecord(WriteBatchInternal::Contents(updates))</code> appends update information to the current log file. And if it fails, it will return corresponding error code to the upper layer.</p>\n<p><epacse hidden>0</epacse></p>\n<h3 id=\"Layout-of-Log-File\"><a href=\"#Layout-of-Log-File\" class=\"headerlink\" title=\"Layout of Log File\"></a>Layout of Log File</h3><p>In a log file, information is stored and retrieved with the unit of <strong>record</strong>. The record consists of two parts: header and payload. The header part is filled with CRC(4 bytes), Length(2 bytes), Type(1 byte). While the payload type is just all the content encapsulated by <a href=\"http://tsaijin.github.io/2020/05/16/The-mechanism-behind-WriteBatch-in-leveldb\">WriteBatch</a> class. </p>\n<img src=\"/2020/05/31/How-leveldb-log-works/record_format.png\" class=\"\" title=\"image of record format\">\n\n<p><epacse hidden>1</epacse></p>\n<p>A more detailed description about the header:</p>\n<ul>\n<li>CRC: the crc sum of the record type and the payload</li>\n<li>Len: The total length of the payload</li>\n<li>Type: The record type, candidates are <code>kFullType</code>, <code>kFirstType</code>, <code>kMiddleType</code> and <code>kLastType</code></li>\n</ul>\n<p><epacse hidden>2</epacse></p>\n<p>The block size of log file is designed  to 32KB intentionally. When leveldb tries to append a record to the tail of the log file, it must take block size into consideration.</p>\n<p><epacse hidden>3</epacse></p>\n<p>There are several cases to handle when appending a record:</p>\n<ul>\n<li><p>The remaining space inside a block cannot even accommodate the header(kHeaderSize = 7 bytes)</p>\n</li>\n<li><p>The remainning space inside a block can accommodate  the whole record</p>\n</li>\n<li><p>The whole record consumes space across two continuous blocks</p>\n</li>\n<li><p>The whole record consumes space across multiple(at least three) continuous blocks</p>\n</li>\n</ul>\n<p>Now, lets talk about the first case: <strong>The remaining space inside a block cannot even accommodate the header(kHeaderSize = 7 bytes)</strong>.</p>\n<p>Because each record must begin with a header, so when the remaning space inside a block is not enough to handle the header, the remaning space will be just padding with dummy data and the next continuous block will be chosen as an operating block.</p>\n<p><epacse hidden>4</epacse></p>\n<img src=\"/2020/05/31/How-leveldb-log-works/record_not_for_header.png\" class=\"\" title=\"image of dummy data\">\n\n\n\n<p>Obviously, the new block will begin to handle the record.</p>\n<p>Second case: <strong>The remaining space inside a block can accommodate  the whole record</strong>. In this case, a record with type <code>kFullType</code> will be appended to this block and the payload field contains all the data.</p>\n<img src=\"/2020/05/31/How-leveldb-log-works/record_within_one_block.png\" class=\"\" title=\"image of full record\">\n\n\n\n<p>Third case: <strong>The whole record consumes space across two continuous blocks</strong>. The record will be splitted into two subrecords, the first subrecord is <code>kFirstType</code> type and the second subrecord is <code>kLastType</code> type.</p>\n<img src=\"/2020/05/31/How-leveldb-log-works/record_across_two_blocks.png\" class=\"\" title=\"image of across two blocks\">\n\n\n\n<p>Fourth case: <strong>The whole record consumes space across multiple(at least three) continuous blocks</strong>. The record will be splitted into multiple subrecords. The first subrecord is <code>kFirstType</code> type and the last subrecord is <code>kLastType</code> type, all the other subrecords are <code>kMiddleType</code> type.</p>\n<img src=\"/2020/05/31/How-leveldb-log-works/record_across_multipe_blocks.png\" class=\"\" title=\"image of across multiple blocks\">\n\n<p><epacse hidden>5</epacse></p>\n<p>In summary, <strong>no physical record on the disk is allowed to persist across blocks.</strong></p>\n<h3 id=\"Log-Recovery\"><a href=\"#Log-Recovery\" class=\"headerlink\" title=\"Log Recovery\"></a>Log Recovery</h3><p>Leveldb will call <code>DBImpl::Recover</code> to try to recover data from the log file when a database is opened each time. Function <code>DBImpl::Recover</code> replays all existing log files with the help of function <code>DBImpl::RecoverLogFile</code> in chronological order.</p>\n<p><epacse hidden>6</epacse></p>\n<p>In function <code>DBImpl::RecoverLogFile</code>, it will read the log file sequentially from beginning to end.</p>\n<p><epacse hidden>7</epacse></p>\n<p>In function <code>Reader::ReadRecord</code>, there are several important variables:</p>\n<ul>\n<li><code>initial_offset_</code>: Offset at which to start looking for the first record to return. It is initialized in the Reader constructor function</li>\n<li><code>last_record_offset_</code>: Offset of the last record returned by ReadRecord</li>\n</ul>\n<p>If <code>last_record_offset_</code> is less than <code>initial_offset_</code>, it will call <code>Reader::ReadRecord</code> to go to the correct block position to read the next record:</p>\n<p><epacse hidden>8</epacse></p>\n<p>As we see, a logical record maybe splitted into multiple subrecords. So there is another function named <code>Reader::ReadPhysicalRecord</code> to read the physical record on disk. </p>\n<p><epacse hidden>9</epacse></p>\n<p>In <code>Reader::ReadPhysicalRecord</code>, it will parse the record header to get crc value and length value. With this information, it can check whether the data is intact or not.</p>\n<p>And if multiple physical records belong to one logical record, the <code>Reader::ReadRecord</code> will combine them together and return to the upper layer:</p>\n<p><epacse hidden>10</epacse></p>\n<p>From the code above, we know that in normal cases only when a record type of <code>kFullType</code> or <code>kLastType</code> is met can the function return to the upper layer.</p>\n<p>And the upper layer replays the whole operation based on the logical record information returned by <code>Reader::ReadRecord</code>. </p>\n<h3 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h3><p>This blog mainly talks about how log mechanism works in leveldb. Have fun<del>~</del></p>"}],"PostAsset":[{"_id":"source/_posts/The-mechanism-behind-WriteBatch-in-leveldb/rep_format.png","slug":"rep_format.png","post":"cka9n1t1y0000grpkegovf6kg","modified":0,"renderable":0},{"_id":"source/_posts/Redo-log-in-InnoDB/redo-log-block.jpg","slug":"redo-log-block.jpg","post":"cka9nungs0000acpk3popcxt3","modified":0,"renderable":0},{"_id":"source/_posts/Compile-your-own-linux-kernel/kernel-configuration.png","slug":"kernel-configuration.png","post":"cka9oizps0000r6pkf8gta8e5","modified":0,"renderable":0},{"_id":"source/_posts/Compile-your-own-linux-kernel/kernel-grub.png","slug":"kernel-grub.png","post":"cka9oizps0000r6pkf8gta8e5","modified":0,"renderable":0},{"_id":"source/_posts/Compile-your-own-linux-kernel/kernel-menuconfig.png","slug":"kernel-menuconfig.png","post":"cka9oizps0000r6pkf8gta8e5","modified":0,"renderable":0},{"_id":"source/_posts/Compile-your-own-linux-kernel/kernel-booting.png","slug":"kernel-booting.png","post":"cka9oizps0000r6pkf8gta8e5","modified":0,"renderable":0},{"_id":"source/_posts/Compile-your-own-linux-kernel/kernel-directory.png","slug":"kernel-directory.png","post":"cka9oizps0000r6pkf8gta8e5","modified":0,"renderable":0},{"_id":"source/_posts/How-leveldb-log-works/record_across_multipe_blocks.png","slug":"record_across_multipe_blocks.png","post":"ckav506w900001rpkamjg6zc9","modified":1,"renderable":0},{"_id":"source/_posts/How-leveldb-log-works/record_across_two_blocks.png","slug":"record_across_two_blocks.png","post":"ckav506w900001rpkamjg6zc9","modified":1,"renderable":0},{"_id":"source/_posts/How-leveldb-log-works/record_format.png","post":"ckav506w900001rpkamjg6zc9","slug":"record_format.png","modified":1,"renderable":1},{"_id":"source/_posts/How-leveldb-log-works/record_not_for_header.png","post":"ckav506w900001rpkamjg6zc9","slug":"record_not_for_header.png","modified":1,"renderable":1},{"_id":"source/_posts/How-leveldb-log-works/record_within_one_block.png","post":"ckav506w900001rpkamjg6zc9","slug":"record_within_one_block.png","modified":1,"renderable":1}],"PostCategory":[{"post_id":"cka9oizps0000r6pkf8gta8e5","category_id":"cka9pgaji0000rppkhq8jbvzn","_id":"cka9pgajq0007rppkd1cgfoy9"},{"post_id":"cka9nungs0000acpk3popcxt3","category_id":"cka9pgaji0000rppkhq8jbvzn","_id":"cka9pgajq0009rppk0mjqf05i"},{"post_id":"cka9on2a8000011pk5h00g8jk","category_id":"cka9pgaji0000rppkhq8jbvzn","_id":"cka9pgajr000arppk6g72cyy4"},{"post_id":"cka9n1t1y0000grpkegovf6kg","category_id":"cka9pgaji0000rppkhq8jbvzn","_id":"cka9pgajt000brppk2sj87n3x"}],"PostTag":[{"post_id":"cka9n1t1y0000grpkegovf6kg","tag_id":"cka9n1t240001grpk5iy44al9","_id":"cka9n1t260002grpkhg2z4m16"},{"post_id":"cka9nungs0000acpk3popcxt3","tag_id":"cka9n1t240001grpk5iy44al9","_id":"cka9pgajn0002rppkdtllcrno"},{"post_id":"cka9oizps0000r6pkf8gta8e5","tag_id":"cka9oizpz0001r6pkgqv7cv3q","_id":"cka9pk8ip0000zvpk40tz920e"},{"post_id":"cka9on2a8000011pk5h00g8jk","tag_id":"cka9on2ae000111pk95bl1b6t","_id":"cka9pk8ir0003zvpk0l3n7ma6"},{"post_id":"cka9nungs0000acpk3popcxt3","tag_id":"cka9pqa290000dtpkgprd2bwu","_id":"cka9pqa2c0001dtpk90qweom1"},{"post_id":"cka9oizps0000r6pkf8gta8e5","tag_id":"cka9on2ae000111pk95bl1b6t","_id":"cka9ps08b0000iopk4d6z2fqo"},{"post_id":"cka9n1t1y0000grpkegovf6kg","tag_id":"cka9ps08d0001iopkbk4mbwsq","_id":"cka9ps08f0002iopkdi2y14gb"},{"post_id":"cka9on2a8000011pk5h00g8jk","tag_id":"cka9pk8ip0001zvpk745cai2w","_id":"cka9ps08j0003iopkhrly7zlq"},{"post_id":"ckav506w900001rpkamjg6zc9","tag_id":"cka9n1t240001grpk5iy44al9","_id":"ckav506wf00011rpk13fc7cg9"},{"post_id":"ckav506w900001rpkamjg6zc9","tag_id":"cka9ps08d0001iopkbk4mbwsq","_id":"ckav506wh00021rpkhagu9cmd"}],"Tag":[{"name":"database","_id":"cka9n1t240001grpk5iy44al9"},{"name":"kernel","_id":"cka9oizpz0001r6pkgqv7cv3q"},{"name":"linux","_id":"cka9on2ae000111pk95bl1b6t"},{"name":"kernel, linux","_id":"cka9pgajl0001rppk0sgp35e1"},{"name":"linux, C","_id":"cka9pgajo0004rppkcp7rau1g"},{"name":"C","_id":"cka9pk8ip0001zvpk745cai2w"},{"name":"mysql","_id":"cka9pqa290000dtpkgprd2bwu"},{"name":"leveldb","_id":"cka9ps08d0001iopkbk4mbwsq"}]}}