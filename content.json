{"pages":[],"posts":[{"title":"The mechanism behind WriteBatch in leveldb","text":"IntroductionAs a well-known key-value database, leveldb provides general key-values interfaces like Put, Get and Delete. Besides those interfaces, leveldb provides a batch operation called WriteBatch as well. A batch operation means we can group multiple operations into one and submit this one to leveldb, the atomicity guarantees that either all of those operations are applied or none of them is applied. For example, we can use batch operation in leveldb in the following way: 123456WriteBatch b;batch.Put(&quot;key&quot;, &quot;v1&quot;);batch.Delete(&quot;key&quot;);batch.Put(&quot;key&quot;, &quot;v2&quot;);batch.Put(&quot;key&quot;, &quot;v3&quot;);db-&gt;Write(WriteOptions(), &amp;b); The updates are applied in the order in which they are added to the WriteBatch. And the value of “key” in the above code sample will be “v3” after the batch is written. Implementaion of WriteBatchWell, how does leveldb implement this simple but powerful interface? Let’s figure it out by digging the source code step by step. WriteBatch ClassThe WriteBatch is a class containing member function WriteBatch::Put, WriteBatch::Delete,WriteBatch::Clear and WriteBatch::Iterate. A private member variable of string type called rep_ is also owned by it. In the constructor of WriteBatch, rep_ is cleared firstly and resize to length kHeader: 12345678910static const size_t kHeader = 12;WriteBatch::WriteBatch() { Clear();}void WriteBatch::Clear() { rep_.clear(); rep_.resize(kHeader);} That’s because the first kHeader bytes in rep_ is used to maintain meta information: 8-byte sequence number and followed by a 4-byte count. Each time when the user calls WriteBatch::Put, the rep_ variable gets updated by the following behaviors : 123456void WriteBatch::Put(const Slice&amp; key, const Slice&amp; value) { WriteBatchInternal::SetCount(this, WriteBatchInternal::Count(this) + 1); rep_.push_back(static_cast&lt;char&gt;(kTypeValue)); PutLengthPrefixedSlice(&amp;rep_, key); PutLengthPrefixedSlice(&amp;rep_, value);} Consequently, the 4-byte count in rep_ gets incremented, and an enum char value kTypeValue(means this is a put operation), the key and the value are appended into the end of rep_. Similarly, when the user calls WriteBatch::Delete, the rep_ variable is updated in a similar way: 12345void WriteBatch::Delete(const Slice&amp; key) { WriteBatchInternal::SetCount(this, WriteBatchInternal::Count(this) + 1); rep_.push_back(static_cast&lt;char&gt;(kTypeDeletion)); PutLengthPrefixedSlice(&amp;rep_, key);} Excepting that an enum char value kTypeDeletion(means this is a put operation) and the key information are appended into the rep_. Let’s be more specific about the function PutLengthPrefixedSlice called by both WriteBatch::Deleteand WriteBatch::Put: 1234void PutLengthPrefixedSlice(std::string* dst, const Slice&amp; value) { PutVarint32(dst, value.size()); dst-&gt;append(value.data(), value.size());} It appends the value size into dst and then the value itself into dst. Based on all the information provided above, we know that the Put and Delete member function of WriteBatch only update its member variable rep_. And we can also know the data layout format of rep_: 12345678910WriteBatch::rep_ := sequence: fixed64 count: fixed32 data: record[count]record := kTypeValue varstring varstring | kTypeDeletion varstringvarstring := len: varint32 data: uint8[len] DBImpl::WriteAfter a batch operation is encapsulated, it’s time to call DBImpl::Write to apply the batch operation into the underlying layer to persist it. Inside DBImpl::Write, an instance of Writer called w , which represents the batch operation context, is created and added to the end of the double-ended queue writers_. Because write operation is in strict order, so the write thread must wait until w is the first element of writers_. 12345678910111213Writer w(&amp;mutex_); w.batch = my_batch; w.sync = options.sync; w.done = false; MutexLock l(&amp;mutex_); writers_.push_back(&amp;w); while (!w.done &amp;&amp; &amp;w != writers_.front()) { w.cv.Wait(); } if (w.done) { return w.status; } When the condition is fulfilled, leveldb will make sure there is enough room for this batch operation. Enough room mainly means memtable is prepared well to handle this write (I will write another new article to talk about that). Because w becomes the first element in writers_, the write thread will try to group other writers behind w into a so-called BatchGroup to speed up the io efficiency by the function DBImpl::BuildBatchGroup. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546WriteBatch* DBImpl::BuildBatchGroup(Writer** last_writer) { assert(!writers_.empty()); Writer* first = writers_.front(); WriteBatch* result = first-&gt;batch; assert(result != NULL); size_t size = WriteBatchInternal::ByteSize(first-&gt;batch); // Allow the group to grow up to a maximum size, but if the // original write is small, limit the growth so we do not slow // down the small write too much. size_t max_size = 1 &lt;&lt; 20; // 1M if (size &lt;= (128&lt;&lt;10)) { // 128K max_size = size + (128&lt;&lt;10); } *last_writer = first; std::deque&lt;Writer*&gt;::iterator iter = writers_.begin(); ++iter; // Advance past &quot;first&quot; for (; iter != writers_.end(); ++iter) { Writer* w = *iter; if (w-&gt;sync &amp;&amp; !first-&gt;sync) { // Do not include a sync write into a batch handled by a non-sync write. break; } if (w-&gt;batch != NULL) { size += WriteBatchInternal::ByteSize(w-&gt;batch); if (size &gt; max_size) { // Do not make batch too big break; } // Append to *result if (result == first-&gt;batch) { // Switch to temporary batch instead of disturbing caller&apos;s batch result = tmp_batch_; assert(WriteBatchInternal::Count(result) == 0); WriteBatchInternal::Append(result, first-&gt;batch); } WriteBatchInternal::Append(result, w-&gt;batch); } *last_writer = w; } return result;} Inside DBImpl::BuildBatchGroup, it will traverse the whole writers_ from beginning to end and try to group all those batches into one batch. For convenience to clarify, we call this procedure “batch mergence” However, there are some limitations in batch mergence. First, the maximum size of rep_ in the merged batch group is 1M and if the first writer is a small writer(rep_.size() &lt;= 128K ) it will limit the maximum size to rep_.size() + 128K. The idea behind this is not slowing down the small write too much. Second, it will not include a sync write into the batch handled by a non-sync write. In the batch mergence, with the help of WriteBatchInternal::Append, the data field of rep_ in second batch is appended to the end of the rep_ in first batch: 12345void WriteBatchInternal::Append(WriteBatch* dst, const WriteBatch* src) { SetCount(dst, Count(dst) + Count(src)); assert(src-&gt;rep_.size() &gt;= kHeader); dst-&gt;rep_.append(src-&gt;rep_.data() + kHeader, src-&gt;rep_.size() - kHeader);} After batch mergence is finished, a new batch called updates is generated. And the sequence number in rep_ of updates is set to last_sequence + 1, last_sequnce gets adjusted to a new one(last_sequnce = last_sequnce + count field rep_ of updates) as well. Now, we come to the IO critical path. The WAL in leveldb will persist the contents of the update in an append-only way. After that success, memtable will replay the operations in the batch update and do corresponding behavior in memtable: Apply to WAL and insert into memtable: 123456789101112131415161718192021{ mutex_.Unlock(); status = log_-&gt;AddRecord(WriteBatchInternal::Contents(updates)); bool sync_error = false; if (status.ok() &amp;&amp; options.sync) { status = logfile_-&gt;Sync(); if (!status.ok()) { sync_error = true; } } if (status.ok()) { status = WriteBatchInternal::InsertInto(updates, mem_); } mutex_.Lock(); if (sync_error) { // The state of the log file is indeterminate: the log record we // just added may or may not show up when the DB is re-opened. // So we force the DB into a mode where all future writes fail. RecordBackgroundError(status); } } WriteBatch Parse the whole rep_ string to replay original operations specified by users and call handler(MemTableInserter) to handle these operations. 123456789101112131415161718192021222324252627282930313233343536373839Status WriteBatch::Iterate(Handler* handler) const { Slice input(rep_); if (input.size() &lt; kHeader) { return Status::Corruption(&quot;malformed WriteBatch (too small)&quot;); } input.remove_prefix(kHeader); Slice key, value; int found = 0; while (!input.empty()) { found++; char tag = input[0]; input.remove_prefix(1); switch (tag) { case kTypeValue: if (GetLengthPrefixedSlice(&amp;input, &amp;key) &amp;&amp; GetLengthPrefixedSlice(&amp;input, &amp;value)) { handler-&gt;Put(key, value); } else { return Status::Corruption(&quot;bad WriteBatch Put&quot;); } break; case kTypeDeletion: if (GetLengthPrefixedSlice(&amp;input, &amp;key)) { handler-&gt;Delete(key); } else { return Status::Corruption(&quot;bad WriteBatch Delete&quot;); } break; default: return Status::Corruption(&quot;unknown WriteBatch tag&quot;); } } if (found != WriteBatchInternal::Count(this)) { return Status::Corruption(&quot;WriteBatch has wrong count&quot;); } else { return Status::OK(); }} SummaryHoo, finally get here! This article mainly talks about how batch operation is handled in leveldb. Due to space limitation, other import parts like WAL and Memtable in leveldb cannot be presented here. Have fun~","link":"/2020/05/16/The-mechanism-behind-WriteBatch-in-leveldb/"},{"title":"Redo log in InnoDB","text":"What is redo logFor a relational database, ACID is a set of properties that it must support for a transaction. That is to say, a transaction should be atomic, consistent, isolated and durable under the management of such database.InnoDB, the default storage engine since MySQL 5.5, use a method called redo log to implement the durability of a transaction. Redo log consists of redo log buffer and redo log file.The redo log buffer resides in memory and is volatile while the redo log file resides in disks and is durable. Redo log records the information about a transaction. As the literal meaning of words redo log denotes, you can redo your operations after the system crashes by redo log. How does it workAs a transaction-based storage engine, InnoDB uses “force log at commit” mechanism to achieve durability.So before a transaction is committed, all logs of that transaction must be flushed to redo log files. Even though the whole system crashes during the process of transaction commit, this transaction can be recovered by the redo log file after the system boots up again.There exist a redo log memory buffer where the redo log is written to boost system performance. To ensure that redo log is written to redo log file successfully each time, the InnoDB storage engine need to call the fsync because O_DIRECT flag is not used when open redo log so that redo log is written to file system buffer firstly. Nevertheless, the time consumed by fsync call is up to the performance of disk. Consequently, the performance of disk determines the performance of transaction commit.Reference1 Taking account of the performance problem mentioned previously, the InnoDB storage engine allows user to set up the frequency of calling fsync. Specifically, parameter innodb_flush_log_at_trx_commit is used for frequency management. innodb_flush_log_at_trx_commit controls the balance between strict ACID compliance for commit operations, and higher performance that is possible when commit-related I/O operations are rearranged and done in batches. You can achieve better performance by changing the default value, but then you can lose up to a second of transactions in a crash. The default value of 1 is required for full ACID compliance. With this value, the contents of the InnoDB log buffer are written out to the log file at each transaction commit and the log file is flushed to disk. With a value of 0, the contents of the InnoDB log buffer are written to the log file approximately once per second and the log file is flushed to disk. No writes from the log buffer to the log file are performed at transaction commit. Once-per-second flushing is not 100% guaranteed to happen every second, due to process scheduling issues. Because the flush to disk operation only occurs approximately once per second, you can lose up to a second of transactions with any mysqld process crash With a value of 2, the contents of the InnoDB log buffer are written to the log file after each transaction commit and the log file is flushed to disk approximately once per second. Once-per-second flushing is not 100% guaranteed to happen every second, due to process scheduling issues. Because the flush to disk operation only occurs approximately once per second, you can lose up to a second of transactions in an operating system crash or a power outage.Reference2 The format of redo log fileIn InnoDB storage engine, the redo logs are stored in 512-byte format. This means that redo log cache, redo log files are both kept in blocks and each bock has a size of 512 bytes. Besides the log itself in block, log block header and lock block tailer are also stored in each block. In a redo log block, 12 bytes and 8 bytes are occupied by redo log header and redo log tailer respectively(So real information stored in each block is 492 bytes).","link":"/2016/07/09/Redo-log-in-InnoDB/"},{"title":"Compile your own linux kernel","text":"As we know, linux is one of the greatest open source projects in the world and serves millions of enterprises. An open source project means that you can define your own features catering to different application scenarios. All big Internet firms such as Google, Facebook and Aamazon recompile the linux kernel so that features can be added to or removed from the official kernel release version.Compiling the kernel for linux kernel developers is also unavoidable. In the rest part of this post, attention will focus on tutorials on compiling a linux kernel. 1. Getting the kernel source of official releaseNothing comes from nothing. So the first thing before compiling a customized kernel is getting source code.I strongly recommend using Git to download and manage the linux kernel source: 1git clone source_git_link Surely, you can also download the compressed package of source code and then uncompress it.Go to the source code root directory, there exists a number of directories under it. The following table[1] illustrates explanation about these directories. Directory Description arch Architecture-specific source block Block I/O layer certs SSL/TLS certification crypto Crypto API Documentation Kernel source documentation drivers drivers Device firmware Device firmware needed to use certain drivers fs The VFS and the individual filesystems include Kernel headers init Kernel boot and initialization ipc Interprocess communication code kernel Core subsystems, such as the scheduler lib Helper routines mm Memory management subsystem and the VM net Networking subsystem samples Sample, demonstrative code scripts Scripts used to build the kernel security Linux Security Module sound Sound subsystem usr Early user-space code (called initramfs) tools Tools helpful for developing Linux virt Virtualization infrastructure 2. Building the kernel source codeAfter the first step, you come here. Now what you should do is configuring the kernel before compiling. As mentioned previously, it is possible to compile support into your kernel for only the specific features and drivers you want. Configuring the kernel is a required process before building it. By default, the kernel of official release version provides myriad features and supports a varied basket of hardware. (1). Configurationwhen you change your current working directory to the root directory of linux kernel source code, you will find there is a file named .config. Using command such as cat .config | more you can take a glimpse of its content. 1cat .config | more As shown in the picture, kernel configuration is controlled by configuration options, which are prefixed by CONFIG in the form CONFIG_FEATURE. That is to say, asynchronous IO is controlled by the configuration option CONFIG_AIO. This option enables POSIX asynchronous I/O which may by used by some high performance threaded applicationsReference. When this option is set, AIO is enabled; if unset, AIO is disabled.Configuration options that control the build process are either Booleans or tristates. A Boolean option is either yes or no. Kernel features, such as CONFIG_PREEMPT, are usually Booleans. A tristate option is one of yes, no, or module.The module setting represents a configuration option that is set but is to be compiled as a module (that is, a separate dynamically loadable object). In the case of tristates, a yes option explicitly means to compile the code into the main kernel image and not as a module. Drivers are usually represented by tristates[Reference][3].Configuration options can also be strings or integers.These options do not control the build process but instead specify values that kernel source can access as a preprocessor macro. For example, a configuration option can specify the size of a statically allocated array[Reference][4].Kernel provides multiple choices for you to facilitate configurations. A straightfoward way is using a graphical interactive interface: make menuconfig. 1make menuconfig After typing this command, a graphical interactive interface will appears in your screen like this: And you can move the cursor to different options to set them. Because of space, how to configure these options correctly can not be presented. For more detailed knowledge, you can find them in the linux orgnization. (2). Compile and buildNow, it is time to get into the marrow of the second part: Compile &amp;&amp; Build. Please make sure that command make and gcc is installed on your machine firstly.Just type make and all related source code about kernel will be compiled and built, the default Makefile rule will handle everything. 1make In general, one flaw about the make method is that this action spawns only a single job because Makefiles all too often have incorrect dependency information. With incorrect dependencies, multiple jobs can step on each other’s toes, resulting in errors in the build process. However, The kernel’s Makefile have correct dependency information, so spawning multiple jobs does not result in failures. To build the kernel with multiple make jobs, use 1make -jn The n here is number of jobs to spawn. Usual practice is to spawn one or two jobs per processor. If you have 16 processors in you machine, then you might do 1make -j32 The resulting kernel file is “arch/x86/boot/bzImage” (in x86 platform). 3. InstallationAfter the kernel is built, you can install it. It is possible that the kernel you install cannot boot successfully, so in case of that, you should have at least two kernel installed on you machine so that you can choose the another one to boot. (1). Install modulesInstalling modules, thankfully, is automated and architecture-independent. As root, simply run 1make modules_install After this, you will find a module file under /lib/modules/a.b.c where a.b.c is the kernel version. (2). Install kernelAs root user, simply run 1make install After this, a new kernel file and a new boot image will appear in the /boot directory. (3). Set booting orderIf you execute all the steps normally, new content about the new installed kernel has been added to /boot/grub/grub.conf file. And you can edit the grub.conf file to choose to use which kernel when booting. Reboot the machine, and then you will find the new installed kernel in the booting screen. [1]: Love, Robert Love. (2003). Linux Kernel Development, 3, 40-42 [3]: Love, Robert Love. (2003). Linux Kernel Development, 3, 42-43[4]: Love, Robert Love. (2003). Linux Kernel Development, 3, 43-45","link":"/2016/03/23/Compile-your-own-linux-kernel/"},{"title":"Read and Write functions in linux","text":"Resulting from work, I have learned I/O models of the linux operating system during these days. In linux operating system, various read and write APIs are provided to user space for use. Comparasions between them are illustraed below. read()12#include &lt;unistd.h&gt;ssize_t read(int fd, void *buf, size_t count); read() is the basic read function in linux environment. It attempts to read up to count bytes from file descriptor fd into the buffer starting at buf.It will start from current file offset. And the current file offset will be increased by the number of bytes read. However, if current file offset is at or past the end of operating file, no bytes will be read into buffer.On success, the number of bytes read is returned (zero indicates end of file), and the file position is advanced by this number. It is not an error if this number is smaller than the number of bytes requested; this may happen for example because fewer bytes are actually available right now (maybe because we were close to end-of-file, or because we are reading from a pipe, or from a terminal), or because read() was interrupted by a signal. On error, -1 is returned, and errno is set appropriately. In this case it is left unspecified whether the file position (if any) changes.Referenceread() is thread safe in the sense that your program will not have undefined behavior (crash or hung) if multiple threads perform IO on the same open file using at once. But the order and atomicity of these operations could vary greatly depending on the type of the file and the implementation of program. lseek()123#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;off_t lseek(int fd, off_t offset, int whence); The lseek() function repositions the offset of the open file associated with the file descriptor fd to the argument offset according to the directive whenceThe directive whence can be as follows:SEEK_SET The offset is set to offset bytes.SEEK_CUR The offset is set to its current location plus offset bytes.SEEK_END The offset is set to the size of the file plus offset bytes.When whence is as the last one, lseek() function allows the file offset to be set beyond the size of file while the file size still keeps the same. If data is latter write at this point, subsequent reads of the data in the gap (as a “hole”) return null bytes until data is actually written to this gap. ReferenceThere are some special usage methods about lseek(): lseek(int fildes, 0, SEEK_SET):move the read or write position to the start of the file lseek(int fildes, 0, SEEK_END):move the read or write position to the end of the file lseek(int fildes, 0, SEEK_CUR):get the current read or write position of the file With lseek(), you can implement the random I/O models of read and write easily. pread()12#include &lt;unistd.h&gt;ssize_t pread(int fd, void *buf, size_t count, off_t offset); Similar to read(), pread() attempts to read count bytes from file descriptor fd at offset into buffer starting at buf. Unlike read(), the offset here will be not changed after the call of preadIn many cases pread() is the only option when you’re dealing with threads reading from a database or such.Compared with read(), pread() does more than read() on account of the time to positioning offset. From the work mechanism of pread(), we can find that it is like the combination of read() and lseek(). Nevertheless, performance of pread() is quite higher than the combination of read() and lseek().As mentioned above, read() function will be in mess when multiple threads or processes perform IO operations on the same open file because it will increase the current file offset. On the flip side, pread() do not change the position in the open file so it is more convenient to using in the scenario of multiple threads and processes. pwrite()123#include &lt;unistd.h&gt;ssize_t pwrite(int fd, const void *buf, size_t nbytes, off_t offset); Returns: number of bytes written if OK, −1 on error Calling pwrite() is equivalent to calling lseek() followed by a call to write(). Instead of calling lseesk() and write() separately, the combination of lseek() and write() is atomic operation in pwrite().","link":"/2016/04/12/Read-and-Write-functions-in-linux/"}],"tags":[{"name":"database","slug":"database","link":"/tags/database/"},{"name":"kernel","slug":"kernel","link":"/tags/kernel/"},{"name":"linux","slug":"linux","link":"/tags/linux/"}],"categories":[{"name":"Technology","slug":"Technology","link":"/categories/Technology/"}]}