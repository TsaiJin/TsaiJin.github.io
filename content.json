{"pages":[],"posts":[{"title":"The mechanism behind WriteBatch in leveldb","text":"IntroductionAs a well-known key-value database, leveldb provides general key-values interfaces like Put, Get and Delete. Besides those interfaces, leveldb provides a batch operation called WriteBatch as well. A batch operation means we can group multiple operations into one and submit this one to leveldb, the atomicity guarantees that either all of those operations are applied or none of them is applied. For example, we can use batch operation in leveldb in the following way: 123456WriteBatch b;batch.Put(&quot;key&quot;, &quot;v1&quot;);batch.Delete(&quot;key&quot;);batch.Put(&quot;key&quot;, &quot;v2&quot;);batch.Put(&quot;key&quot;, &quot;v3&quot;);db-&gt;Write(WriteOptions(), &amp;b); The updates are applied in the order in which they are added to the WriteBatch. And the value of “key” in the above code sample will be “v3” after the batch is written. Implementaion of WriteBatchWell, how does leveldb implement this simple but powerful interface? Let’s figure it out by digging the source code step by step. WriteBatch ClassThe WriteBatch is a class containing member function WriteBatch::Put, WriteBatch::Delete,WriteBatch::Clear and WriteBatch::Iterate. A private member variable of string type called rep_ is also owned by it. In the constructor of WriteBatch, rep_ is cleared firstly and resize to length kHeader: 12345678910static const size_t kHeader = 12;WriteBatch::WriteBatch() { Clear();}void WriteBatch::Clear() { rep_.clear(); rep_.resize(kHeader);} That’s because the first kHeader bytes in rep_ is used to maintain meta information: 8-byte sequence number and followed by a 4-byte count. Each time when the user calls WriteBatch::Put, the rep_ variable gets updated by the following behaviors : 123456void WriteBatch::Put(const Slice&amp; key, const Slice&amp; value) { WriteBatchInternal::SetCount(this, WriteBatchInternal::Count(this) + 1); rep_.push_back(static_cast&lt;char&gt;(kTypeValue)); PutLengthPrefixedSlice(&amp;rep_, key); PutLengthPrefixedSlice(&amp;rep_, value);} Consequently, the 4-byte count in rep_ gets incremented, and an enum char value kTypeValue(means this is a put operation), the key and the value are appended into the end of rep_. Similarly, when the user calls WriteBatch::Delete, the rep_ variable is updated in a similar way: 12345void WriteBatch::Delete(const Slice&amp; key) { WriteBatchInternal::SetCount(this, WriteBatchInternal::Count(this) + 1); rep_.push_back(static_cast&lt;char&gt;(kTypeDeletion)); PutLengthPrefixedSlice(&amp;rep_, key);} Excepting that an enum char value kTypeDeletion(means this is a put operation) and the key information are appended into the rep_. Let’s be more specific about the function PutLengthPrefixedSlice called by both WriteBatch::Deleteand WriteBatch::Put: 1234void PutLengthPrefixedSlice(std::string* dst, const Slice&amp; value) { PutVarint32(dst, value.size()); dst-&gt;append(value.data(), value.size());} It appends the value size into dst and then the value itself into dst. Based on all the information provided above, we know that the Put and Delete member function of WriteBatch only update its member variable rep_. And we can also know the data layout format of rep_: 12345678910WriteBatch::rep_ := sequence: fixed64 count: fixed32 data: record[count]record := kTypeValue varstring varstring | kTypeDeletion varstringvarstring := len: varint32 data: uint8[len] DBImpl::WriteAfter a batch operation is encapsulated, it’s time to call DBImpl::Write to apply the batch operation into the underlying layer to persist it. Inside DBImpl::Write, an instance of Writer called w , which represents the batch operation context, is created and added to the end of the double-ended queue writers_. Because write operation is in strict order, so the write thread must wait until w is the first element of writers_. 12345678910111213Writer w(&amp;mutex_); w.batch = my_batch; w.sync = options.sync; w.done = false; MutexLock l(&amp;mutex_); writers_.push_back(&amp;w); while (!w.done &amp;&amp; &amp;w != writers_.front()) { w.cv.Wait(); } if (w.done) { return w.status; } When the condition is fulfilled, leveldb will make sure there is enough room for this batch operation. Enough room mainly means memtable is prepared well to handle this write (I will write another new article to talk about that). Because w becomes the first element in writers_, the write thread will try to group other writers behind w into a so-called BatchGroup to speed up the io efficiency by the function DBImpl::BuildBatchGroup. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546WriteBatch* DBImpl::BuildBatchGroup(Writer** last_writer) { assert(!writers_.empty()); Writer* first = writers_.front(); WriteBatch* result = first-&gt;batch; assert(result != NULL); size_t size = WriteBatchInternal::ByteSize(first-&gt;batch); // Allow the group to grow up to a maximum size, but if the // original write is small, limit the growth so we do not slow // down the small write too much. size_t max_size = 1 &lt;&lt; 20; // 1M if (size &lt;= (128&lt;&lt;10)) { // 128K max_size = size + (128&lt;&lt;10); } *last_writer = first; std::deque&lt;Writer*&gt;::iterator iter = writers_.begin(); ++iter; // Advance past &quot;first&quot; for (; iter != writers_.end(); ++iter) { Writer* w = *iter; if (w-&gt;sync &amp;&amp; !first-&gt;sync) { // Do not include a sync write into a batch handled by a non-sync write. break; } if (w-&gt;batch != NULL) { size += WriteBatchInternal::ByteSize(w-&gt;batch); if (size &gt; max_size) { // Do not make batch too big break; } // Append to *result if (result == first-&gt;batch) { // Switch to temporary batch instead of disturbing caller&apos;s batch result = tmp_batch_; assert(WriteBatchInternal::Count(result) == 0); WriteBatchInternal::Append(result, first-&gt;batch); } WriteBatchInternal::Append(result, w-&gt;batch); } *last_writer = w; } return result;} Inside DBImpl::BuildBatchGroup, it will traverse the whole writers_ from beginning to end and try to group all those batches into one batch. For convenience to clarify, we call this procedure “batch mergence” However, there are some limitations in batch mergence. First, the maximum size of rep_ in the merged batch group is 1M and if the first writer is a small writer(rep_.size() &lt;= 128K ) it will limit the maximum size to rep_.size() + 128K. The idea behind this is not slowing down the small write too much. Second, it will not include a sync write into the batch handled by a non-sync write. In the batch mergence, with the help of WriteBatchInternal::Append, the data field of rep_ in second batch is appended to the end of the rep_ in first batch: 12345void WriteBatchInternal::Append(WriteBatch* dst, const WriteBatch* src) { SetCount(dst, Count(dst) + Count(src)); assert(src-&gt;rep_.size() &gt;= kHeader); dst-&gt;rep_.append(src-&gt;rep_.data() + kHeader, src-&gt;rep_.size() - kHeader);} After batch mergence is finished, a new batch called updates is generated. And the sequence number in rep_ of updates is set to last_sequence + 1, last_sequnce gets adjusted to a new one(last_sequnce = last_sequnce + count field rep_ of updates) as well. Now, we come to the IO critical path. The WAL in leveldb will persist the contents of the update in an append-only way. After that success, memtable will replay the operations in the batch update and do corresponding behavior in memtable: Apply to WAL and insert into memtable: 123456789101112131415161718192021{ mutex_.Unlock(); status = log_-&gt;AddRecord(WriteBatchInternal::Contents(updates)); bool sync_error = false; if (status.ok() &amp;&amp; options.sync) { status = logfile_-&gt;Sync(); if (!status.ok()) { sync_error = true; } } if (status.ok()) { status = WriteBatchInternal::InsertInto(updates, mem_); } mutex_.Lock(); if (sync_error) { // The state of the log file is indeterminate: the log record we // just added may or may not show up when the DB is re-opened. // So we force the DB into a mode where all future writes fail. RecordBackgroundError(status); } } WriteBatch Parse the whole rep_ string to replay original operations specified by users and call handler(MemTableInserter) to handle these operations. 123456789101112131415161718192021222324252627282930313233343536373839Status WriteBatch::Iterate(Handler* handler) const { Slice input(rep_); if (input.size() &lt; kHeader) { return Status::Corruption(&quot;malformed WriteBatch (too small)&quot;); } input.remove_prefix(kHeader); Slice key, value; int found = 0; while (!input.empty()) { found++; char tag = input[0]; input.remove_prefix(1); switch (tag) { case kTypeValue: if (GetLengthPrefixedSlice(&amp;input, &amp;key) &amp;&amp; GetLengthPrefixedSlice(&amp;input, &amp;value)) { handler-&gt;Put(key, value); } else { return Status::Corruption(&quot;bad WriteBatch Put&quot;); } break; case kTypeDeletion: if (GetLengthPrefixedSlice(&amp;input, &amp;key)) { handler-&gt;Delete(key); } else { return Status::Corruption(&quot;bad WriteBatch Delete&quot;); } break; default: return Status::Corruption(&quot;unknown WriteBatch tag&quot;); } } if (found != WriteBatchInternal::Count(this)) { return Status::Corruption(&quot;WriteBatch has wrong count&quot;); } else { return Status::OK(); }} SummaryHoo, finally get here! This article mainly talks about how batch operation is handled in leveldb. Due to space limitation, other import parts like WAL and Memtable in leveldb cannot be presented here. Have fun~","link":"/2020/05/16/The-mechanism-behind-WriteBatch-in-leveldb/"},{"title":"Redo log in InnoDB","text":"What is redo logFor a relational database, ACID is a set of properties that it must support for a transaction. That is to say, a transaction should be atomic, consistent, isolated and durable under the management of such database.InnoDB, the default storage engine since MySQL 5.5, use a method called redo log to implement the durability of a transaction. Redo log consists of redo log buffer and redo log file.The redo log buffer resides in memory and is volatile while the redo log file resides in disks and is durable. Redo log records the information about a transaction. As the literal meaning of words redo log denotes, you can redo your operations after the system crashes by redo log. How does it workAs a transaction-based storage engine, InnoDB uses “force log at commit” mechanism to achieve durability.So before a transaction is committed, all logs of that transaction must be flushed to redo log files. Even though the whole system crashes during the process of transaction commit, this transaction can be recovered by the redo log file after the system boots up again.There exist a redo log memory buffer where the redo log is written to boost system performance. To ensure that redo log is written to redo log file successfully each time, the InnoDB storage engine need to call the fsync because O_DIRECT flag is not used when open redo log so that redo log is written to file system buffer firstly. Nevertheless, the time consumed by fsync call is up to the performance of disk. Consequently, the performance of disk determines the performance of transaction commit.Reference1 Taking account of the performance problem mentioned previously, the InnoDB storage engine allows user to set up the frequency of calling fsync. Specifically, parameter innodb_flush_log_at_trx_commit is used for frequency management. innodb_flush_log_at_trx_commit controls the balance between strict ACID compliance for commit operations, and higher performance that is possible when commit-related I/O operations are rearranged and done in batches. You can achieve better performance by changing the default value, but then you can lose up to a second of transactions in a crash. The default value of 1 is required for full ACID compliance. With this value, the contents of the InnoDB log buffer are written out to the log file at each transaction commit and the log file is flushed to disk. With a value of 0, the contents of the InnoDB log buffer are written to the log file approximately once per second and the log file is flushed to disk. No writes from the log buffer to the log file are performed at transaction commit. Once-per-second flushing is not 100% guaranteed to happen every second, due to process scheduling issues. Because the flush to disk operation only occurs approximately once per second, you can lose up to a second of transactions with any mysqld process crash With a value of 2, the contents of the InnoDB log buffer are written to the log file after each transaction commit and the log file is flushed to disk approximately once per second. Once-per-second flushing is not 100% guaranteed to happen every second, due to process scheduling issues. Because the flush to disk operation only occurs approximately once per second, you can lose up to a second of transactions in an operating system crash or a power outage.Reference2 The format of redo log fileIn InnoDB storage engine, the redo logs are stored in 512-byte format. This means that redo log cache, redo log files are both kept in blocks and each bock has a size of 512 bytes. Besides the log itself in block, log block header and lock block tailer are also stored in each block. In a redo log block, 12 bytes and 8 bytes are occupied by redo log header and redo log tailer respectively(So real information stored in each block is 492 bytes).","link":"/2016/07/09/Redo-log-in-InnoDB/"},{"title":"Compile your own linux kernel","text":"As we know, linux is one of the greatest open source projects in the world and serves millions of enterprises. An open source project means that you can define your own features catering to different application scenarios. All big Internet firms such as Google, Facebook and Aamazon recompile the linux kernel so that features can be added to or removed from the official kernel release version.Compiling the kernel for linux kernel developers is also unavoidable. In the rest part of this post, attention will focus on tutorials on compiling a linux kernel. 1. Getting the kernel source of official releaseNothing comes from nothing. So the first thing before compiling a customized kernel is getting source code.I strongly recommend using Git to download and manage the linux kernel source: 1git clone source_git_link Surely, you can also download the compressed package of source code and then uncompress it.Go to the source code root directory, there exists a number of directories under it. The following table[1] illustrates explanation about these directories. Directory Description arch Architecture-specific source block Block I/O layer certs SSL/TLS certification crypto Crypto API Documentation Kernel source documentation drivers drivers Device firmware Device firmware needed to use certain drivers fs The VFS and the individual filesystems include Kernel headers init Kernel boot and initialization ipc Interprocess communication code kernel Core subsystems, such as the scheduler lib Helper routines mm Memory management subsystem and the VM net Networking subsystem samples Sample, demonstrative code scripts Scripts used to build the kernel security Linux Security Module sound Sound subsystem usr Early user-space code (called initramfs) tools Tools helpful for developing Linux virt Virtualization infrastructure 2. Building the kernel source codeAfter the first step, you come here. Now what you should do is configuring the kernel before compiling. As mentioned previously, it is possible to compile support into your kernel for only the specific features and drivers you want. Configuring the kernel is a required process before building it. By default, the kernel of official release version provides myriad features and supports a varied basket of hardware. (1). Configurationwhen you change your current working directory to the root directory of linux kernel source code, you will find there is a file named .config. Using command such as cat .config | more you can take a glimpse of its content. 1cat .config | more As shown in the picture, kernel configuration is controlled by configuration options, which are prefixed by CONFIG in the form CONFIG_FEATURE. That is to say, asynchronous IO is controlled by the configuration option CONFIG_AIO. This option enables POSIX asynchronous I/O which may by used by some high performance threaded applicationsReference. When this option is set, AIO is enabled; if unset, AIO is disabled.Configuration options that control the build process are either Booleans or tristates. A Boolean option is either yes or no. Kernel features, such as CONFIG_PREEMPT, are usually Booleans. A tristate option is one of yes, no, or module.The module setting represents a configuration option that is set but is to be compiled as a module (that is, a separate dynamically loadable object). In the case of tristates, a yes option explicitly means to compile the code into the main kernel image and not as a module. Drivers are usually represented by tristates[Reference][3].Configuration options can also be strings or integers.These options do not control the build process but instead specify values that kernel source can access as a preprocessor macro. For example, a configuration option can specify the size of a statically allocated array[Reference][4].Kernel provides multiple choices for you to facilitate configurations. A straightfoward way is using a graphical interactive interface: make menuconfig. 1make menuconfig After typing this command, a graphical interactive interface will appears in your screen like this: And you can move the cursor to different options to set them. Because of space, how to configure these options correctly can not be presented. For more detailed knowledge, you can find them in the linux orgnization. (2). Compile and buildNow, it is time to get into the marrow of the second part: Compile &amp;&amp; Build. Please make sure that command make and gcc is installed on your machine firstly.Just type make and all related source code about kernel will be compiled and built, the default Makefile rule will handle everything. 1make In general, one flaw about the make method is that this action spawns only a single job because Makefiles all too often have incorrect dependency information. With incorrect dependencies, multiple jobs can step on each other’s toes, resulting in errors in the build process. However, The kernel’s Makefile have correct dependency information, so spawning multiple jobs does not result in failures. To build the kernel with multiple make jobs, use 1make -jn The n here is number of jobs to spawn. Usual practice is to spawn one or two jobs per processor. If you have 16 processors in you machine, then you might do 1make -j32 The resulting kernel file is “arch/x86/boot/bzImage” (in x86 platform). 3. InstallationAfter the kernel is built, you can install it. It is possible that the kernel you install cannot boot successfully, so in case of that, you should have at least two kernel installed on you machine so that you can choose the another one to boot. (1). Install modulesInstalling modules, thankfully, is automated and architecture-independent. As root, simply run 1make modules_install After this, you will find a module file under /lib/modules/a.b.c where a.b.c is the kernel version. (2). Install kernelAs root user, simply run 1make install After this, a new kernel file and a new boot image will appear in the /boot directory. (3). Set booting orderIf you execute all the steps normally, new content about the new installed kernel has been added to /boot/grub/grub.conf file. And you can edit the grub.conf file to choose to use which kernel when booting. Reboot the machine, and then you will find the new installed kernel in the booting screen. [1]: Love, Robert Love. (2003). Linux Kernel Development, 3, 40-42 [3]: Love, Robert Love. (2003). Linux Kernel Development, 3, 42-43[4]: Love, Robert Love. (2003). Linux Kernel Development, 3, 43-45","link":"/2016/03/23/Compile-your-own-linux-kernel/"},{"title":"Read and Write functions in linux","text":"Resulting from work, I have learned I/O models of the linux operating system during these days. In linux operating system, various read and write APIs are provided to user space for use. Comparasions between them are illustraed below. read()12#include &lt;unistd.h&gt;ssize_t read(int fd, void *buf, size_t count); read() is the basic read function in linux environment. It attempts to read up to count bytes from file descriptor fd into the buffer starting at buf.It will start from current file offset. And the current file offset will be increased by the number of bytes read. However, if current file offset is at or past the end of operating file, no bytes will be read into buffer.On success, the number of bytes read is returned (zero indicates end of file), and the file position is advanced by this number. It is not an error if this number is smaller than the number of bytes requested; this may happen for example because fewer bytes are actually available right now (maybe because we were close to end-of-file, or because we are reading from a pipe, or from a terminal), or because read() was interrupted by a signal. On error, -1 is returned, and errno is set appropriately. In this case it is left unspecified whether the file position (if any) changes.Referenceread() is thread safe in the sense that your program will not have undefined behavior (crash or hung) if multiple threads perform IO on the same open file using at once. But the order and atomicity of these operations could vary greatly depending on the type of the file and the implementation of program. lseek()123#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;off_t lseek(int fd, off_t offset, int whence); The lseek() function repositions the offset of the open file associated with the file descriptor fd to the argument offset according to the directive whenceThe directive whence can be as follows:SEEK_SET The offset is set to offset bytes.SEEK_CUR The offset is set to its current location plus offset bytes.SEEK_END The offset is set to the size of the file plus offset bytes.When whence is as the last one, lseek() function allows the file offset to be set beyond the size of file while the file size still keeps the same. If data is latter write at this point, subsequent reads of the data in the gap (as a “hole”) return null bytes until data is actually written to this gap. ReferenceThere are some special usage methods about lseek(): lseek(int fildes, 0, SEEK_SET):move the read or write position to the start of the file lseek(int fildes, 0, SEEK_END):move the read or write position to the end of the file lseek(int fildes, 0, SEEK_CUR):get the current read or write position of the file With lseek(), you can implement the random I/O models of read and write easily. pread()12#include &lt;unistd.h&gt;ssize_t pread(int fd, void *buf, size_t count, off_t offset); Similar to read(), pread() attempts to read count bytes from file descriptor fd at offset into buffer starting at buf. Unlike read(), the offset here will be not changed after the call of preadIn many cases pread() is the only option when you’re dealing with threads reading from a database or such.Compared with read(), pread() does more than read() on account of the time to positioning offset. From the work mechanism of pread(), we can find that it is like the combination of read() and lseek(). Nevertheless, performance of pread() is quite higher than the combination of read() and lseek().As mentioned above, read() function will be in mess when multiple threads or processes perform IO operations on the same open file because it will increase the current file offset. On the flip side, pread() do not change the position in the open file so it is more convenient to using in the scenario of multiple threads and processes. pwrite()123#include &lt;unistd.h&gt;ssize_t pwrite(int fd, const void *buf, size_t nbytes, off_t offset); Returns: number of bytes written if OK, −1 on error Calling pwrite() is equivalent to calling lseek() followed by a call to write(). Instead of calling lseesk() and write() separately, the combination of lseek() and write() is atomic operation in pwrite().","link":"/2016/04/12/Read-and-Write-functions-in-linux/"},{"title":"How leveldb log works","text":"IntroductionIn leveldb, a log file (*.log) stores a sequence of recent updates. Each update is appended to the current log file. There are several benefits of log file for leveldb: Boosting performance by converting random write into sequential write automatically in the underlying hard drives Satisfying the atomicity and durability requirements of database properties When the user issues an update operation(put or delete), a corresponding record will be appended to the log file firstly. Only when the record is persisted successfully will a successful status be returned to the user. function DBImpl::Write is the critical path for all update operations. Inside this function, statement status = log_-&gt;AddRecord(WriteBatchInternal::Contents(updates)) appends update information to the current log file. And if it fails, it will return corresponding error code to the upper layer. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172Status DBImpl::Write(const WriteOptions&amp; options, WriteBatch* my_batch) { Writer w(&amp;mutex_); w.batch = my_batch; w.sync = options.sync; w.done = false; MutexLock l(&amp;mutex_); writers_.push_back(&amp;w); while (!w.done &amp;&amp; &amp;w != writers_.front()) { w.cv.Wait(); } if (w.done) { return w.status; } // May temporarily unlock and wait. Status status = MakeRoomForWrite(my_batch == NULL); uint64_t last_sequence = versions_-&gt;LastSequence(); Writer* last_writer = &amp;w; if (status.ok() &amp;&amp; my_batch != NULL) { // NULL batch is for compactions WriteBatch* updates = BuildBatchGroup(&amp;last_writer); WriteBatchInternal::SetSequence(updates, last_sequence + 1); last_sequence += WriteBatchInternal::Count(updates); // Add to log and apply to memtable. We can release the lock // during this phase since &amp;w is currently responsible for logging // and protects against concurrent loggers and concurrent writes // into mem_. { mutex_.Unlock(); status = log_-&gt;AddRecord(WriteBatchInternal::Contents(updates)); bool sync_error = false; if (status.ok() &amp;&amp; options.sync) { status = logfile_-&gt;Sync(); if (!status.ok()) { sync_error = true; } } if (status.ok()) { status = WriteBatchInternal::InsertInto(updates, mem_); } mutex_.Lock(); if (sync_error) { // The state of the log file is indeterminate: the log record we // just added may or may not show up when the DB is re-opened. // So we force the DB into a mode where all future writes fail. RecordBackgroundError(status); } } if (updates == tmp_batch_) tmp_batch_-&gt;Clear(); versions_-&gt;SetLastSequence(last_sequence); } while (true) { Writer* ready = writers_.front(); writers_.pop_front(); if (ready != &amp;w) { ready-&gt;status = status; ready-&gt;done = true; ready-&gt;cv.Signal(); } if (ready == last_writer) break; } // Notify new head of write queue if (!writers_.empty()) { writers_.front()-&gt;cv.Signal(); } return status;} Layout of Log FileIn a log file, information is stored and retrieved with the unit of record. The record consists of two parts: header and payload. The header part is filled with CRC(4 bytes), Length(2 bytes), Type(1 byte). While the payload type is just all the content encapsulated by WriteBatch class. 12// Header is checksum (4 bytes), length (2 bytes), type (1 byte).static const int kHeaderSize = 4 + 2 + 1 A more detailed description about the header: CRC: the crc sum of the record type and the payload Len: The total length of the payload Type: The record type, candidates are kFullType, kFirstType, kMiddleType and kLastType 1234567891011121314151617181920212223242526Status Writer::EmitPhysicalRecord(RecordType t, const char* ptr, size_t n) { assert(n &lt;= 0xffff); // Must fit in two bytes assert(block_offset_ + kHeaderSize + n &lt;= kBlockSize); // Format the header char buf[kHeaderSize]; buf[4] = static_cast&lt;char&gt;(n &amp; 0xff); buf[5] = static_cast&lt;char&gt;(n &gt;&gt; 8); buf[6] = static_cast&lt;char&gt;(t); // Compute the crc of the record type and the payload. uint32_t crc = crc32c::Extend(type_crc_[t], ptr, n); crc = crc32c::Mask(crc); // Adjust for storage EncodeFixed32(buf, crc); // Write the header and the payload Status s = dest_-&gt;Append(Slice(buf, kHeaderSize)); if (s.ok()) { s = dest_-&gt;Append(Slice(ptr, n)); if (s.ok()) { s = dest_-&gt;Flush(); } } block_offset_ += kHeaderSize + n; return s;} The block size of log file is designed to 32KB intentionally. When leveldb tries to append a record to the tail of the log file, it must take block size into consideration. 1static const int kBlockSize = 32768; There are several cases to handle when appending a record: The remaining space inside a block cannot even accommodate the header(kHeaderSize = 7 bytes) The remainning space inside a block can accommodate the whole record The whole record consumes space across two continuous blocks The whole record consumes space across multiple(at least three) continuous blocks Now, let’s talk about the first case: The remaining space inside a block cannot even accommodate the header(kHeaderSize = 7 bytes). Because each record must begin with a header, so when the remaning space inside a block is not enough to handle the header, the remaning space will be just padding with dummy data and the next continuous block will be chosen as an operating block. 1234567891011const int leftover = kBlockSize - block_offset_;assert(leftover &gt;= 0);if (leftover &lt; kHeaderSize) { // Switch to a new block if (leftover &gt; 0) { // Fill the trailer (literal below relies on kHeaderSize being 7) assert(kHeaderSize == 7); dest_-&gt;Append(Slice(&quot;\\x00\\x00\\x00\\x00\\x00\\x00&quot;, leftover)); } block_offset_ = 0;} Obviously, the new block will begin to handle the record. Second case: The remaining space inside a block can accommodate the whole record. In this case, a record with type kFullType will be appended to this block and the payload field contains all the data. Third case: The whole record consumes space across two continuous blocks. The record will be splitted into two subrecords, the first subrecord is kFirstType type and the second subrecord is kLastType type. Fourth case: The whole record consumes space across multiple(at least three) continuous blocks. The record will be splitted into multiple subrecords. The first subrecord is kFirstType type and the last subrecord is kLastType type, all the other subrecords are kMiddleType type. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647Status Writer::AddRecord(const Slice&amp; slice) { const char* ptr = slice.data(); size_t left = slice.size(); // Fragment the record if necessary and emit it. Note that if slice // is empty, we still want to iterate once to emit a single // zero-length record Status s; bool begin = true; do { const int leftover = kBlockSize - block_offset_; assert(leftover &gt;= 0); if (leftover &lt; kHeaderSize) { // Switch to a new block if (leftover &gt; 0) { // Fill the trailer (literal below relies on kHeaderSize being 7) assert(kHeaderSize == 7); dest_-&gt;Append(Slice(&quot;\\x00\\x00\\x00\\x00\\x00\\x00&quot;, leftover)); } block_offset_ = 0; } // Invariant: we never leave &lt; kHeaderSize bytes in a block. assert(kBlockSize - block_offset_ - kHeaderSize &gt;= 0); const size_t avail = kBlockSize - block_offset_ - kHeaderSize; const size_t fragment_length = (left &lt; avail) ? left : avail; RecordType type; const bool end = (left == fragment_length); if (begin &amp;&amp; end) { type = kFullType; } else if (begin) { type = kFirstType; } else if (end) { type = kLastType; } else { type = kMiddleType; } s = EmitPhysicalRecord(type, ptr, fragment_length); ptr += fragment_length; left -= fragment_length; begin = false; } while (s.ok() &amp;&amp; left &gt; 0); return s;} In summary, no physical record on the disk is allowed to persist across blocks. Log RecoveryLeveldb will call DBImpl::Recover to try to recover data from the log file when a database is opened each time. Function DBImpl::Recover replays all existing log files with the help of function DBImpl::RecoverLogFile in chronological order. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// Recover from all newer log files than the ones named in the // descriptor (new log files may have been added by the previous // incarnation without registering them in the descriptor). // // Note that PrevLogNumber() is no longer used, but we pay // attention to it in case we are recovering a database // produced by an older version of leveldb. const uint64_t min_log = versions_-&gt;LogNumber(); const uint64_t prev_log = versions_-&gt;PrevLogNumber(); std::vector&lt;std::string&gt; filenames; s = env_-&gt;GetChildren(dbname_, &amp;filenames); if (!s.ok()) { return s; } std::set&lt;uint64_t&gt; expected; versions_-&gt;AddLiveFiles(&amp;expected); uint64_t number; FileType type; std::vector&lt;uint64_t&gt; logs; for (size_t i = 0; i &lt; filenames.size(); i++) { if (ParseFileName(filenames[i], &amp;number, &amp;type)) { expected.erase(number); if (type == kLogFile &amp;&amp; ((number &gt;= min_log) || (number == prev_log))) logs.push_back(number); } } if (!expected.empty()) { char buf[50]; snprintf(buf, sizeof(buf), &quot;%d missing files; e.g.&quot;, static_cast&lt;int&gt;(expected.size())); return Status::Corruption(buf, TableFileName(dbname_, *(expected.begin()))); } // Recover in the order in which the logs were generated std::sort(logs.begin(), logs.end()); for (size_t i = 0; i &lt; logs.size(); i++) { s = RecoverLogFile(logs[i], (i == logs.size() - 1), save_manifest, edit, &amp;max_sequence); if (!s.ok()) { return s; } // The previous incarnation may not have written any MANIFEST // records after allocating this log number. So we manually // update the file number allocation counter in VersionSet. versions_-&gt;MarkFileNumberUsed(logs[i]); } In function DBImpl::RecoverLogFile, it will read the log file sequentially from beginning to end. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122Status DBImpl::RecoverLogFile(uint64_t log_number, bool last_log, bool* save_manifest, VersionEdit* edit, SequenceNumber* max_sequence) { struct LogReporter : public log::Reader::Reporter { Env* env; Logger* info_log; const char* fname; Status* status; // NULL if options_.paranoid_checks==false virtual void Corruption(size_t bytes, const Status&amp; s) { Log(info_log, &quot;%s%s: dropping %d bytes; %s&quot;, (this-&gt;status == NULL ? &quot;(ignoring error) &quot; : &quot;&quot;), fname, static_cast&lt;int&gt;(bytes), s.ToString().c_str()); if (this-&gt;status != NULL &amp;&amp; this-&gt;status-&gt;ok()) *this-&gt;status = s; } }; mutex_.AssertHeld(); // Open the log file std::string fname = LogFileName(dbname_, log_number); SequentialFile* file; Status status = env_-&gt;NewSequentialFile(fname, &amp;file); if (!status.ok()) { MaybeIgnoreError(&amp;status); return status; } // Create the log reader. LogReporter reporter; reporter.env = env_; reporter.info_log = options_.info_log; reporter.fname = fname.c_str(); reporter.status = (options_.paranoid_checks ? &amp;status : NULL); // We intentionally make log::Reader do checksumming even if // paranoid_checks==false so that corruptions cause entire commits // to be skipped instead of propagating bad information (like overly // large sequence numbers). log::Reader reader(file, &amp;reporter, true/*checksum*/, 0/*initial_offset*/); Log(options_.info_log, &quot;Recovering log #%llu&quot;, (unsigned long long) log_number); // Read all the records and add to a memtable std::string scratch; Slice record; WriteBatch batch; int compactions = 0; MemTable* mem = NULL; while (reader.ReadRecord(&amp;record, &amp;scratch) &amp;&amp; status.ok()) { if (record.size() &lt; 12) { reporter.Corruption( record.size(), Status::Corruption(&quot;log record too small&quot;)); continue; } WriteBatchInternal::SetContents(&amp;batch, record); if (mem == NULL) { mem = new MemTable(internal_comparator_); mem-&gt;Ref(); } status = WriteBatchInternal::InsertInto(&amp;batch, mem); MaybeIgnoreError(&amp;status); if (!status.ok()) { break; } const SequenceNumber last_seq = WriteBatchInternal::Sequence(&amp;batch) + WriteBatchInternal::Count(&amp;batch) - 1; if (last_seq &gt; *max_sequence) { *max_sequence = last_seq; } if (mem-&gt;ApproximateMemoryUsage() &gt; options_.write_buffer_size) { compactions++; *save_manifest = true; status = WriteLevel0Table(mem, edit, NULL); mem-&gt;Unref(); mem = NULL; if (!status.ok()) { // Reflect errors immediately so that conditions like full // file-systems cause the DB::Open() to fail. break; } } } delete file; // See if we should keep reusing the last log file. if (status.ok() &amp;&amp; options_.reuse_logs &amp;&amp; last_log &amp;&amp; compactions == 0) { assert(logfile_ == NULL); assert(log_ == NULL); assert(mem_ == NULL); uint64_t lfile_size; if (env_-&gt;GetFileSize(fname, &amp;lfile_size).ok() &amp;&amp; env_-&gt;NewAppendableFile(fname, &amp;logfile_).ok()) { Log(options_.info_log, &quot;Reusing old log %s \\n&quot;, fname.c_str()); log_ = new log::Writer(logfile_, lfile_size); logfile_number_ = log_number; if (mem != NULL) { mem_ = mem; mem = NULL; } else { // mem can be NULL if lognum exists but was empty. mem_ = new MemTable(internal_comparator_); mem_-&gt;Ref(); } } } if (mem != NULL) { // mem did not get reused; compact it. if (status.ok()) { *save_manifest = true; status = WriteLevel0Table(mem, edit, NULL); } mem-&gt;Unref(); } return status;} In function Reader::ReadRecord, there are several important variables: initial_offset_: Offset at which to start looking for the first record to return. It is initialized in the Reader constructor function last_record_offset_: Offset of the last record returned by ReadRecord If last_record_offset_ is less than initial_offset_, it will call Reader::ReadRecord to go to the correct block position to read the next record: 1234567891011121314151617181920212223bool Reader::SkipToInitialBlock() { size_t offset_in_block = initial_offset_ % kBlockSize; uint64_t block_start_location = initial_offset_ - offset_in_block; // Don&apos;t search a block if we&apos;d be in the trailer if (offset_in_block &gt; kBlockSize - 6) { offset_in_block = 0; block_start_location += kBlockSize; } end_of_buffer_offset_ = block_start_location; // Skip to start of first block that can contain the initial record if (block_start_location &gt; 0) { Status skip_status = file_-&gt;Skip(block_start_location); if (!skip_status.ok()) { ReportDrop(block_start_location, skip_status); return false; } } return true;} As we see, a logical record maybe splitted into multiple subrecords. So there is another function named Reader::ReadPhysicalRecord to read the physical record on disk. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283unsigned int Reader::ReadPhysicalRecord(Slice* result) { while (true) { if (buffer_.size() &lt; kHeaderSize) { if (!eof_) { // Last read was a full read, so this is a trailer to skip buffer_.clear(); Status status = file_-&gt;Read(kBlockSize, &amp;buffer_, backing_store_); end_of_buffer_offset_ += buffer_.size(); if (!status.ok()) { buffer_.clear(); ReportDrop(kBlockSize, status); eof_ = true; return kEof; } else if (buffer_.size() &lt; kBlockSize) { eof_ = true; } continue; } else { // Note that if buffer_ is non-empty, we have a truncated header at the // end of the file, which can be caused by the writer crashing in the // middle of writing the header. Instead of considering this an error, // just report EOF. buffer_.clear(); return kEof; } } // Parse the header const char* header = buffer_.data(); const uint32_t a = static_cast&lt;uint32_t&gt;(header[4]) &amp; 0xff; const uint32_t b = static_cast&lt;uint32_t&gt;(header[5]) &amp; 0xff; const unsigned int type = header[6]; const uint32_t length = a | (b &lt;&lt; 8); if (kHeaderSize + length &gt; buffer_.size()) { size_t drop_size = buffer_.size(); buffer_.clear(); if (!eof_) { ReportCorruption(drop_size, &quot;bad record length&quot;); return kBadRecord; } // If the end of the file has been reached without reading |length| bytes // of payload, assume the writer died in the middle of writing the record. // Don&apos;t report a corruption. return kEof; } if (type == kZeroType &amp;&amp; length == 0) { // Skip zero length record without reporting any drops since // such records are produced by the mmap based writing code in // env_posix.cc that preallocates file regions. buffer_.clear(); return kBadRecord; } // Check crc if (checksum_) { uint32_t expected_crc = crc32c::Unmask(DecodeFixed32(header)); uint32_t actual_crc = crc32c::Value(header + 6, 1 + length); if (actual_crc != expected_crc) { // Drop the rest of the buffer since &quot;length&quot; itself may have // been corrupted and if we trust it, we could find some // fragment of a real log record that just happens to look // like a valid log record. size_t drop_size = buffer_.size(); buffer_.clear(); ReportCorruption(drop_size, &quot;checksum mismatch&quot;); return kBadRecord; } } buffer_.remove_prefix(kHeaderSize + length); // Skip physical record that started before initial_offset_ if (end_of_buffer_offset_ - buffer_.size() - kHeaderSize - length &lt; initial_offset_) { result-&gt;clear(); return kBadRecord; } *result = Slice(header + kHeaderSize, length); return type; }} In Reader::ReadPhysicalRecord, it will parse the record header to get crc value and length value. With this information, it can check whether the data is intact or not. And if multiple physical records belong to one logical record, the Reader::ReadRecord will combine them together and return to the upper layer: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106while (true) { const unsigned int record_type = ReadPhysicalRecord(&amp;fragment); // ReadPhysicalRecord may have only had an empty trailer remaining in its // internal buffer. Calculate the offset of the next physical record now // that it has returned, properly accounting for its header size. uint64_t physical_record_offset = end_of_buffer_offset_ - buffer_.size() - kHeaderSize - fragment.size(); if (resyncing_) { if (record_type == kMiddleType) { continue; } else if (record_type == kLastType) { resyncing_ = false; continue; } else { resyncing_ = false; } } switch (record_type) { case kFullType: if (in_fragmented_record) { // Handle bug in earlier versions of log::Writer where // it could emit an empty kFirstType record at the tail end // of a block followed by a kFullType or kFirstType record // at the beginning of the next block. if (scratch-&gt;empty()) { in_fragmented_record = false; } else { ReportCorruption(scratch-&gt;size(), &quot;partial record without end(1)&quot;); } } prospective_record_offset = physical_record_offset; scratch-&gt;clear(); *record = fragment; last_record_offset_ = prospective_record_offset; return true; case kFirstType: if (in_fragmented_record) { // Handle bug in earlier versions of log::Writer where // it could emit an empty kFirstType record at the tail end // of a block followed by a kFullType or kFirstType record // at the beginning of the next block. if (scratch-&gt;empty()) { in_fragmented_record = false; } else { ReportCorruption(scratch-&gt;size(), &quot;partial record without end(2)&quot;); } } prospective_record_offset = physical_record_offset; scratch-&gt;assign(fragment.data(), fragment.size()); in_fragmented_record = true; break; case kMiddleType: if (!in_fragmented_record) { ReportCorruption(fragment.size(), &quot;missing start of fragmented record(1)&quot;); } else { scratch-&gt;append(fragment.data(), fragment.size()); } break; case kLastType: if (!in_fragmented_record) { ReportCorruption(fragment.size(), &quot;missing start of fragmented record(2)&quot;); } else { scratch-&gt;append(fragment.data(), fragment.size()); *record = Slice(*scratch); last_record_offset_ = prospective_record_offset; return true; } break; case kEof: if (in_fragmented_record) { // This can be caused by the writer dying immediately after // writing a physical record but before completing the next; don&apos;t // treat it as a corruption, just ignore the entire logical record. scratch-&gt;clear(); } return false; case kBadRecord: if (in_fragmented_record) { ReportCorruption(scratch-&gt;size(), &quot;error in middle of record&quot;); in_fragmented_record = false; scratch-&gt;clear(); } break; default: { char buf[40]; snprintf(buf, sizeof(buf), &quot;unknown record type %u&quot;, record_type); ReportCorruption( (fragment.size() + (in_fragmented_record ? scratch-&gt;size() : 0)), buf); in_fragmented_record = false; scratch-&gt;clear(); break; } } } From the code above, we know that in normal cases only when a record type of kFullType or kLastType is met can the function return to the upper layer. And the upper layer replays the whole operation based on the logical record information returned by Reader::ReadRecord. SummaryThis blog mainly talks about how log mechanism works in leveldb. Have fun~","link":"/2020/05/31/How-leveldb-log-works/"}],"tags":[{"name":"database","slug":"database","link":"/tags/database/"},{"name":"kernel","slug":"kernel","link":"/tags/kernel/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"C","slug":"C","link":"/tags/C/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"leveldb","slug":"leveldb","link":"/tags/leveldb/"}],"categories":[{"name":"Technology","slug":"Technology","link":"/categories/Technology/"}]}